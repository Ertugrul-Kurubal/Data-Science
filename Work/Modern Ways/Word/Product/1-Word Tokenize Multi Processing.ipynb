{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f95aee8",
   "metadata": {},
   "source": [
    "### Multi Processing Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from cleantext import clean\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from openpyxl.workbook import Workbook\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec480be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1db280",
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55567d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_name = \"fr\"\n",
    "custom_char = [\"-\",\"#\",\":\",\"+\",\"~\",\"*\",\"/\",\"&quot;\",\"&\",\"'\"] # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa65384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_1():\n",
    "    part1_num = 1\n",
    "    try:\n",
    "      with open(f\"/home/ubuntu/Workspace/{text_name}{part1_num}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    except:\n",
    "      print(\"There is not such a file  or path is incorrect\")\n",
    "    \n",
    "    text_data_clean_brackets = re.sub('[\\(\\[\\{].*?[\\)\\]\\}]', '', text_data)\n",
    "    \n",
    "    #custom_char = [\"-\",\"#\",\":\",\"+\",\"~\",\"*\",\"/\",\"&quot;\",\"&\",\"'\"] # ???\n",
    "    for i in custom_char:\n",
    "        text_data_clean_brackets = text_data_clean_brackets.replace(i, '')  # must be equal each other\n",
    "    \n",
    "    text_data_clean = clean(text_data_clean_brackets,\n",
    "                            fix_unicode=True,\n",
    "                            to_ascii=False,\n",
    "                            lower=False,\n",
    "                            normalize_whitespace=True,\n",
    "                            no_line_breaks=True,\n",
    "                            strip_lines=False,\n",
    "                            keep_two_line_breaks=False,\n",
    "                            no_urls=True,\n",
    "                            no_emails=True,\n",
    "                            no_phone_numbers=True,\n",
    "                            no_numbers=True,\n",
    "                            no_digits=True,\n",
    "                            no_currency_symbols=True,\n",
    "                            no_punct=True,\n",
    "                            no_emoji=True,\n",
    "                            replace_with_url='',\n",
    "                            replace_with_email='',\n",
    "                            replace_with_phone_number='',\n",
    "                            replace_with_number='',\n",
    "                            replace_with_digit='',\n",
    "                            replace_with_currency_symbol='',\n",
    "                            replace_with_punct=''\n",
    "                            )\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text_data_clean)\n",
    "    \n",
    "    capital_word = pd.Series(word_tokens)\n",
    "    \n",
    "    capital_word = capital_word.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    df_capital = pd.DataFrame(capital_word)\n",
    "    \n",
    "    df_capital = df_capital.reset_index()\n",
    "    \n",
    "    df_capital = df_capital.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    df_capital.to_csv(f\"Not_Apply_Lower_Word{part1_num}.csv\", index=False)\n",
    "    \n",
    "    lower_list = []\n",
    "    for i in word_tokens:\n",
    "        lower_list.append(i.lower())\n",
    "    \n",
    "    tokens_without_punc = pd.Series(lower_list)\n",
    "    \n",
    "    tokens_without_punc = tokens_without_punc.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    freq_word_df = pd.DataFrame(tokens_without_punc)\n",
    "    \n",
    "    freq_word_df = freq_word_df.reset_index()\n",
    "    \n",
    "    freq_word_df = freq_word_df.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    #freq_word_df[\"ratio\"] = round((freq_word_df.frequency/(sum(freq_word_df.frequency))*100),7)\n",
    "    \n",
    "    #freq_word_df[\"cumul_ratio\"] = np.cumsum(freq_word_df[\"ratio\"])\n",
    "    \n",
    "    freq_word_df_5 = freq_word_df[freq_word_df.frequency>=5]\n",
    "    \n",
    "    freq_word_df_5.to_csv(f\"Word_Tokenize_{text_name.upper()}{part1_num}.csv\", index=False)\n",
    "    \n",
    "    freq_word_df.to_csv(f\"Word_Tokenize_Full_{text_name.upper()}{part1_num}.csv\", index=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e921115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_2():\n",
    "    part2_num = 2\n",
    "    try:\n",
    "      with open(f\"/home/ubuntu/Workspace/{text_name}{part2_num}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    except:\n",
    "      print(\"There is not such a file  or path is incorrect\")\n",
    "    \n",
    "    text_data_clean_brackets = re.sub('[\\(\\[\\{].*?[\\)\\]\\}]', '', text_data)\n",
    "    \n",
    "    #custom_char = [\"-\",\"#\",\":\",\"+\",\"~\",\"*\",\"/\",\"&quot;\",\"&\",\"'\"] # ???\n",
    "    for i in custom_char:\n",
    "        text_data_clean_brackets = text_data_clean_brackets.replace(i, '')  # must be equal each other\n",
    "    \n",
    "    text_data_clean = clean(text_data_clean_brackets,\n",
    "                            fix_unicode=True,\n",
    "                            to_ascii=False,\n",
    "                            lower=False,\n",
    "                            normalize_whitespace=True,\n",
    "                            no_line_breaks=True,\n",
    "                            strip_lines=False,\n",
    "                            keep_two_line_breaks=False,\n",
    "                            no_urls=True,\n",
    "                            no_emails=True,\n",
    "                            no_phone_numbers=True,\n",
    "                            no_numbers=True,\n",
    "                            no_digits=True,\n",
    "                            no_currency_symbols=True,\n",
    "                            no_punct=True,\n",
    "                            no_emoji=True,\n",
    "                            replace_with_url='',\n",
    "                            replace_with_email='',\n",
    "                            replace_with_phone_number='',\n",
    "                            replace_with_number='',\n",
    "                            replace_with_digit='',\n",
    "                            replace_with_currency_symbol='',\n",
    "                            replace_with_punct=''\n",
    "                            )\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text_data_clean)\n",
    "    \n",
    "    capital_word = pd.Series(word_tokens)\n",
    "    \n",
    "    capital_word = capital_word.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    df_capital = pd.DataFrame(capital_word)\n",
    "    \n",
    "    df_capital = df_capital.reset_index()\n",
    "    \n",
    "    df_capital = df_capital.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    df_capital.to_csv(\"Not_Apply_Lower_Word2.csv\", index=False)\n",
    "    \n",
    "    lower_list = []\n",
    "    for i in word_tokens:\n",
    "        lower_list.append(i.lower())\n",
    "    \n",
    "    tokens_without_punc = pd.Series(lower_list)\n",
    "    \n",
    "    tokens_without_punc = tokens_without_punc.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    freq_word_df = pd.DataFrame(tokens_without_punc)\n",
    "    \n",
    "    freq_word_df = freq_word_df.reset_index()\n",
    "    \n",
    "    freq_word_df = freq_word_df.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    #freq_word_df[\"ratio\"] = round((freq_word_df.frequency/(sum(freq_word_df.frequency))*100),7)\n",
    "    \n",
    "    #freq_word_df[\"cumul_ratio\"] = np.cumsum(freq_word_df[\"ratio\"])\n",
    "    \n",
    "    freq_word_df_5 = freq_word_df[freq_word_df.frequency>=5]\n",
    "    \n",
    "    freq_word_df_5.to_csv(\"Word_Tokenize_FR2.csv\", index=False)\n",
    "    \n",
    "    freq_word_df.to_csv(\"Word_Tokenize_Full_FR2.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_3():\n",
    "    part3_num = 3\n",
    "    try:\n",
    "      with open(f\"/home/ubuntu/Workspace/{text_name}{part3_num}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    except:\n",
    "      print(\"There is not such a file  or path is incorrect\")\n",
    "    \n",
    "    text_data_clean_brackets = re.sub('[\\(\\[\\{].*?[\\)\\]\\}]', '', text_data)\n",
    "    \n",
    "    #custom_char = [\"-\",\"#\",\":\",\"+\",\"~\",\"*\",\"/\",\"&quot;\",\"&\",\"'\"] # ???\n",
    "    for i in custom_char:\n",
    "        text_data_clean_brackets = text_data_clean_brackets.replace(i, '')  # must be equal each other\n",
    "    \n",
    "    text_data_clean = clean(text_data_clean_brackets,\n",
    "                            fix_unicode=True,\n",
    "                            to_ascii=False,\n",
    "                            lower=False,\n",
    "                            normalize_whitespace=True,\n",
    "                            no_line_breaks=True,\n",
    "                            strip_lines=False,\n",
    "                            keep_two_line_breaks=False,\n",
    "                            no_urls=True,\n",
    "                            no_emails=True,\n",
    "                            no_phone_numbers=True,\n",
    "                            no_numbers=True,\n",
    "                            no_digits=True,\n",
    "                            no_currency_symbols=True,\n",
    "                            no_punct=True,\n",
    "                            no_emoji=True,\n",
    "                            replace_with_url='',\n",
    "                            replace_with_email='',\n",
    "                            replace_with_phone_number='',\n",
    "                            replace_with_number='',\n",
    "                            replace_with_digit='',\n",
    "                            replace_with_currency_symbol='',\n",
    "                            replace_with_punct=''\n",
    "                            )\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text_data_clean)\n",
    "    \n",
    "    capital_word = pd.Series(word_tokens)\n",
    "    \n",
    "    capital_word = capital_word.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    df_capital = pd.DataFrame(capital_word)\n",
    "    \n",
    "    df_capital = df_capital.reset_index()\n",
    "    \n",
    "    df_capital = df_capital.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    df_capital.to_csv(\"Not_Apply_Lower_Word3.csv\", index=False)\n",
    "    \n",
    "    lower_list = []\n",
    "    for i in word_tokens:\n",
    "        lower_list.append(i.lower())\n",
    "    \n",
    "    tokens_without_punc = pd.Series(lower_list)\n",
    "    \n",
    "    tokens_without_punc = tokens_without_punc.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    freq_word_df = pd.DataFrame(tokens_without_punc)\n",
    "    \n",
    "    freq_word_df = freq_word_df.reset_index()\n",
    "    \n",
    "    freq_word_df = freq_word_df.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    #freq_word_df[\"ratio\"] = round((freq_word_df.frequency/(sum(freq_word_df.frequency))*100),7)\n",
    "    \n",
    "    #freq_word_df[\"cumul_ratio\"] = np.cumsum(freq_word_df[\"ratio\"])\n",
    "    \n",
    "    freq_word_df_5 = freq_word_df[freq_word_df.frequency>=5]\n",
    "    \n",
    "    freq_word_df_5.to_csv(\"Word_Tokenize_FR3.csv\", index=False)\n",
    "    \n",
    "    freq_word_df.to_csv(\"Word_Tokenize_Full_FR3.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c539d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_4():\n",
    "    part4_num = 4\n",
    "    try:\n",
    "      with open(f\"/home/ubuntu/Workspace/{text_name}{part4_num}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    except:\n",
    "      print(\"There is not such a file  or path is incorrect\")\n",
    "    \n",
    "    text_data_clean_brackets = re.sub('[\\(\\[\\{].*?[\\)\\]\\}]', '', text_data)\n",
    "    \n",
    "    #custom_char = [\"-\",\"#\",\":\",\"+\",\"~\",\"*\",\"/\",\"&quot;\",\"&\",\"'\"] # ???\n",
    "    for i in custom_char:\n",
    "        text_data_clean_brackets = text_data_clean_brackets.replace(i, '')  # must be equal each other\n",
    "    \n",
    "    text_data_clean = clean(text_data_clean_brackets,\n",
    "                            fix_unicode=True,\n",
    "                            to_ascii=False,\n",
    "                            lower=False,\n",
    "                            normalize_whitespace=True,\n",
    "                            no_line_breaks=True,\n",
    "                            strip_lines=False,\n",
    "                            keep_two_line_breaks=False,\n",
    "                            no_urls=True,\n",
    "                            no_emails=True,\n",
    "                            no_phone_numbers=True,\n",
    "                            no_numbers=True,\n",
    "                            no_digits=True,\n",
    "                            no_currency_symbols=True,\n",
    "                            no_punct=True,\n",
    "                            no_emoji=True,\n",
    "                            replace_with_url='',\n",
    "                            replace_with_email='',\n",
    "                            replace_with_phone_number='',\n",
    "                            replace_with_number='',\n",
    "                            replace_with_digit='',\n",
    "                            replace_with_currency_symbol='',\n",
    "                            replace_with_punct=''\n",
    "                            )\n",
    "    \n",
    "    word_tokens = nltk.word_tokenize(text_data_clean)\n",
    "    \n",
    "    capital_word = pd.Series(word_tokens)\n",
    "    \n",
    "    capital_word = capital_word.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    df_capital = pd.DataFrame(capital_word)\n",
    "    \n",
    "    df_capital = df_capital.reset_index()\n",
    "    \n",
    "    df_capital = df_capital.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    df_capital.to_csv(\"Not_Apply_Lower_Word4.csv\", index=False)\n",
    "    \n",
    "    lower_list = []\n",
    "    for i in word_tokens:\n",
    "        lower_list.append(i.lower())\n",
    "    \n",
    "    tokens_without_punc = pd.Series(lower_list)\n",
    "    \n",
    "    tokens_without_punc = tokens_without_punc.value_counts().sort_values(ascending=False)\n",
    "    \n",
    "    freq_word_df = pd.DataFrame(tokens_without_punc)\n",
    "    \n",
    "    freq_word_df = freq_word_df.reset_index()\n",
    "    \n",
    "    freq_word_df = freq_word_df.rename(columns={\"index\": \"word\", 0 : \"frequency\"})\n",
    "    \n",
    "    #freq_word_df[\"ratio\"] = round((freq_word_df.frequency/(sum(freq_word_df.frequency))*100),7)\n",
    "    \n",
    "    #freq_word_df[\"cumul_ratio\"] = np.cumsum(freq_word_df[\"ratio\"])\n",
    "    \n",
    "    freq_word_df_5 = freq_word_df[freq_word_df.frequency>=5]\n",
    "    \n",
    "    freq_word_df_5.to_csv(\"Word_Tokenize_FR4.csv\", index=False)\n",
    "    \n",
    "    freq_word_df.to_csv(\"Word_Tokenize_Full_FR4.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43389a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    p1 = Process(target=part_1,)\n",
    "    p2 = Process(target=part_2,)\n",
    "    p3 = Process(target=part_3,)\n",
    "    p4 = Process(target=part_4,)\n",
    "    \n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p3.start()\n",
    "    p4.start()\n",
    "    \n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    p3.join()\n",
    "    p4.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d20f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
