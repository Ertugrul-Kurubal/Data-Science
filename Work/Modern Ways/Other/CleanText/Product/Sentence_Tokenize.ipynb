{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97ee4ec8",
   "metadata": {},
   "source": [
    "### Sentence Tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24104212",
   "metadata": {},
   "source": [
    "This notebook includes sent tokenize techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b78f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unicodedata2\n",
    "#!pip install clean-text\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670d9ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package tagsets to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from cleantext import clean\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from openpyxl.workbook import Workbook\n",
    "nltk.download('tagsets')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebd1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  with open(\"tr6.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "except:\n",
    "  print(\"There is not such a file  or path is incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3320789",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean_brackets = re.sub('[\\(\\[\\{].*?[\\)\\]\\}]', '', text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0651d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_char = [\"-\",\"#\",\":\"]\n",
    "for i in custom_char:\n",
    "    text_data_clean_brackets = text_data_clean_brackets.replace(i, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93fd6651",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean_apos = re.sub(r\"\\'\", \"\", string=text_data_clean_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e27c30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean = clean(text_data_clean_apos,\n",
    "                        fix_unicode=True,\n",
    "                        to_ascii=False,\n",
    "                        lower=False,\n",
    "                        normalize_whitespace=True,\n",
    "                        no_line_breaks=True,\n",
    "                        strip_lines=False,\n",
    "                        keep_two_line_breaks=False,\n",
    "                        no_urls=True,\n",
    "                        no_emails=True,\n",
    "                        no_phone_numbers=True,\n",
    "                        no_numbers=True,\n",
    "                        no_digits=True,\n",
    "                        no_currency_symbols=True,\n",
    "                        no_punct=False,\n",
    "                        no_emoji=True,\n",
    "                        replace_with_url='',\n",
    "                        replace_with_email='',\n",
    "                        replace_with_phone_number='',\n",
    "                        replace_with_number='',\n",
    "                        replace_with_digit='',\n",
    "                        replace_with_currency_symbol='',\n",
    "                        replace_with_punct=''\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12374fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sent_tokenize(text_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe8ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lower =  []\n",
    "for i in sentence:\n",
    "    sentence_lower.append(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda9d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_df = pd.DataFrame(sentence_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c85b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_df = sent_token_df.rename(columns={0:\"sentence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca6d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc = sent_token_df.sentence.apply(lambda x: re.sub(pattern=\"[^\\w\\s]\", repl=\"\", string=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0327457",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df = pd.DataFrame(data=sent_token_no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1cf98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def space(sentence):\n",
    "    a_1 = word_tokenize(sentence)\n",
    "    a_2 = \" \".join(a_1)\n",
    "    return a_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba32072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df.sentence = sent_token_no_punc_df.sentence.apply(lambda x : space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0027021",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df.drop(sent_token_no_punc_df[sent_token_no_punc_df.sentence == \"\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc6703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df = sent_token_no_punc_df.sentence.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f394591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2 = pd.DataFrame(sent_token_no_punc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "964ec53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2 = sent_token_no_punc_df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593073b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2 = sent_token_no_punc_df2.rename(columns={\"index\":\"sentence\", \"sentence\":\"frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae3ab0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value = sent_token_no_punc_df2.frequency.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08a32aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2[\"ratio\"] = (sent_token_no_punc_df2.frequency/total_value)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "861e56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2[\"cumul_ratio\"] = np.cumsum(sent_token_no_punc_df2[\"ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7278793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_token_no_punc_df3 = sent_token_no_punc_df2.head(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cbd418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_lenght(sentence):\n",
    "    var1 = word_tokenize(sentence)\n",
    "    if len(var1) <= 10:\n",
    "        var2 = \" \".join(var1)\n",
    "        return var2\n",
    "    else:\n",
    "        return \"sentence is bigger ten word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12ea8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2.sentence = sent_token_no_punc_df2.sentence.apply(sentence_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a98f6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2 = sent_token_no_punc_df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb570cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_no_punc_df2.to_csv(\"Sentence_Tokenize_TR6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681c170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833234c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66589012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
