{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622bd6b2",
   "metadata": {},
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ba05df",
   "metadata": {},
   "source": [
    "This notebook was used word tokenize of text without multiprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c555a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package tagsets to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from cleantext import clean\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from openpyxl.workbook import Workbook\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed985279",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  with open(\"tr6.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "except:\n",
    "  print(\"There is not such a file  or path is incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36db6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean_brackets = re.sub('[\\(\\[\\{].*?[\\)\\]\\}]', '', text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57e4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_clean = clean(text_data_clean_brackets,\n",
    "                        fix_unicode=True,\n",
    "                        to_ascii=False,\n",
    "                        lower=True,\n",
    "                        normalize_whitespace=True,\n",
    "                        no_line_breaks=True,\n",
    "                        strip_lines=False,\n",
    "                        keep_two_line_breaks=False,\n",
    "                        no_urls=True,\n",
    "                        no_emails=True,\n",
    "                        no_phone_numbers=True,\n",
    "                        no_numbers=True,\n",
    "                        no_digits=True,\n",
    "                        no_currency_symbols=True,\n",
    "                        no_punct=True,\n",
    "                        no_emoji=True,\n",
    "                        replace_with_url='',\n",
    "                        replace_with_email='',\n",
    "                        replace_with_phone_number='',\n",
    "                        replace_with_number='',\n",
    "                        replace_with_digit='',\n",
    "                        replace_with_currency_symbol='',\n",
    "                        replace_with_punct=''\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7c3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(text_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759f34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc_lemma = []\n",
    "for word in word_tokens:\n",
    "    tokens_without_punc_lemma.append(WordNetLemmatizer().lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c17fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = pd.Series(tokens_without_punc_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3eb8e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = tokens_without_punc.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0782e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df = pd.DataFrame(tokens_without_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e6acd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df = freq_word_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c25ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df = freq_word_df.rename(columns={\"index\": \"word\", 0 : \"frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeba2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df[\"ratio\"] = round((freq_word_df.frequency/(sum(freq_word_df.frequency))*100),7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b82ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df[\"cumul_ratio\"] = np.cumsum(freq_word_df[\"ratio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f63285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_word_df_1b = freq_word_df.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894d3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df_5 = freq_word_df[freq_word_df.frequency>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1679ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq_word_df_1b.to_csv(\"Word_Tokenize1_TR3.csv\",  index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86edc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word_df_5.to_csv(\"Word_Tokenize_TR6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33a8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
