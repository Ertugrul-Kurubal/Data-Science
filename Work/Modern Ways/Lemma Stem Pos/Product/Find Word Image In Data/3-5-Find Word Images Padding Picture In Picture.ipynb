{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Word Images Padding Picture In Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "#lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "file_ext = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_lemma_all_data_path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "#Lemma Stem POS/Result/3-2-Word In Visual Genome Merge\"\n",
    "\n",
    "visual_genome_process_data_path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Lemma Stem POS/Result/3-0-Visual Genome Process\"\n",
    "\n",
    "path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Lemma Stem POS/Result/3-5-Find Word Images Padding Picture In Picture\"\n",
    "\n",
    "#Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_padding_aspect_ratio(data_folder_name, data_path, out_folder_name, out_path, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9):\n",
    "       '''\n",
    "       image_padding_aspect_ratio(data_folder_name, data_path, out_folder_name, out_path, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9)\\n\n",
    "       data_folder_name, data_path, out_folder_name, out_path are path and folder in string(str). others are cv2 parameter and image aspect ratio parameter.\\n\n",
    "       ex.\\n\n",
    "       output_path = f\"/home/kurubal/Documents/Modern Ways/Project/{lang_folder.capitalize()}/Image Audio Video/Data/28 Words\"\\n\n",
    "       image_path = f\"/home/kurubal/Documents/Modern Ways/Project/{lang_folder.capitalize()}/Image Audio Video/Data/28 Words\"\\n\n",
    "       image_padding_aspect_ratio(\"Images_SV_Rename\", image_path, \"Images_SV\", output_path, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9)\n",
    "       '''\n",
    "       error_file_list = []\n",
    "       result_path = f\"{out_path}/{out_folder_name}\"\n",
    "       data_folder_path = f\"{data_path}/{data_folder_name}\"\n",
    "       Path(result_path).mkdir(parents=True, exist_ok=True)\n",
    "       files=[f for f in os.listdir(data_folder_path) if isfile(join(data_folder_path,f))]   \n",
    "       for i in range(len(files)):       \n",
    "          pathfilename=data_folder_path+'/'+files[i]\n",
    "          #filename_without_ext = os.path.splitext(files[i])[0]\n",
    "          #text = f\"{text}\"\n",
    "          if isfile(f\"{result_path}/{files[i]}\"):\n",
    "                 pass\n",
    "          else:\n",
    "                 try:\n",
    "                        img = cv2.imread(f\"{pathfilename}\")\n",
    "                        old_image_height, old_image_width, channels = img.shape  # 2718  1988\n",
    "\n",
    "                        #aspect_ratio_width = 16\n",
    "                        #aspect_ratio_height = 9\n",
    "\n",
    "                        if (old_image_height/aspect_ratio_height) > (old_image_width/aspect_ratio_width):\n",
    "                               new_image_width = int((old_image_height/aspect_ratio_height)*aspect_ratio_width)\n",
    "                               new_image_height = int(old_image_height)\n",
    "                        elif (old_image_height/aspect_ratio_height) < (old_image_width/aspect_ratio_width):\n",
    "                               new_image_width = int(old_image_width)\n",
    "                               new_image_height = int((old_image_width/aspect_ratio_width)*aspect_ratio_height)\n",
    "                        elif (old_image_height/aspect_ratio_height) == (old_image_width/aspect_ratio_width):\n",
    "                               new_image_width = int((old_image_height/aspect_ratio_height)*aspect_ratio_width)\n",
    "                               new_image_height = int(old_image_height)\n",
    "                        else:\n",
    "                               new_image_width = old_image_width\n",
    "                               new_image_height = old_image_height\n",
    "\n",
    "                        color = padding_color\n",
    "                        result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "\n",
    "                        # compute center offset\n",
    "                        x_center = (new_image_width - old_image_width) // 2\n",
    "                        y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "                        # copy img image into center of result image\n",
    "                        result[y_center:y_center+old_image_height, x_center:x_center+old_image_width] = img            \n",
    "\n",
    "                        cv2.imwrite(f\"{result_path}/{files[i]}\", result)\n",
    "                 except:\n",
    "                        print(\"Oops!\", sys.exc_info()[0], \"occurred.\")\n",
    "                        error_file_list.append(files[i])\n",
    "                        \n",
    "       return error_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = False): # ??? will edit definition\n",
    "       '''\n",
    "       image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = False)\\n\n",
    "       image_path_file are path, folder and file in string(str). others are cv2 parameter and image aspect ratio parameter.\\n\n",
    "       ex.\\n\n",
    "       image_path_file = \"/home/kurubal/Downloads/2333869.jpg\"\\n\n",
    "       image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = False)\n",
    "       '''\n",
    "       error_file_list = []\n",
    "       size = (1600, 900)  # image new size (width, height)\n",
    "       # path process\n",
    "       path_split_list = os.path.split(image_path_file)\n",
    "       main_path = path_split_list[0]  # part of before last one\n",
    "       file = path_split_list[1]  # last file or folder\n",
    "       filename_without_ext = os.path.splitext(file)[0]  # file without .jpg\n",
    "       filename_ext = os.path.splitext(file)[1]  # file extention as .jpg\n",
    "       # file process\n",
    "       if isfile(f\"{main_path}/{filename_without_ext}_padded{filename_ext}\"):\n",
    "              pass\n",
    "       else:\n",
    "              try:\n",
    "                     img = cv2.imread(f\"{image_path_file}\")\n",
    "                     old_image_height, old_image_width, channels = img.shape \n",
    "\n",
    "                     if (old_image_height/aspect_ratio_height) > (old_image_width/aspect_ratio_width):\n",
    "                            new_image_width = int((old_image_height/aspect_ratio_height)*aspect_ratio_width)\n",
    "                            new_image_height = int(old_image_height)\n",
    "                     elif (old_image_height/aspect_ratio_height) < (old_image_width/aspect_ratio_width):\n",
    "                            new_image_width = int(old_image_width)\n",
    "                            new_image_height = int((old_image_width/aspect_ratio_width)*aspect_ratio_height)\n",
    "                     elif (old_image_height/aspect_ratio_height) == (old_image_width/aspect_ratio_width):\n",
    "                            new_image_width = int((old_image_height/aspect_ratio_height)*aspect_ratio_width)\n",
    "                            new_image_height = int(old_image_height)\n",
    "                     else:\n",
    "                            new_image_width = old_image_width\n",
    "                            new_image_height = old_image_height\n",
    "\n",
    "                     color = padding_color\n",
    "                     result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "\n",
    "                     # compute center offset\n",
    "                     x_center = (new_image_width - old_image_width) // 2\n",
    "                     y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "                     # copy img image into center of result image\n",
    "                     result[y_center:y_center+old_image_height, x_center:x_center+old_image_width] = img\n",
    "\n",
    "                     # image resize\n",
    "                     result_resize = cv2.resize(result, size, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "                     if overwrite:\n",
    "                            cv2.imwrite(f\"{main_path}/{filename_without_ext}{filename_ext}\", result_resize)  # for overwrite\n",
    "                     else:\n",
    "                            cv2.imwrite(f\"{main_path}/{filename_without_ext}_padded{filename_ext}\", result_resize)\n",
    "              except:\n",
    "                     print(\"Oops!\", sys.exc_info()[0], \"occurred.\")\n",
    "                     error_file_list.append(image_path_file)\n",
    "                        \n",
    "       return error_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picture_in_picture(main_image_file_path, add_image_file_path, add_pic_size=8, add_pic_pos=[\"top\",\"left\"], add_pic_margin=10, border_color=\"red\"):\n",
    "    '''picture_in_picture(main_image_file_path, add_image_file_path, add_pic_size=8, add_pic_pos=[\"top\",\"left\"], add_pic_margin=10, border_color=\"red\")\\n\n",
    "    main_image_file_path is master image file path or result of image file, add_image_file_path is slave image that use for paste on master image.\\n\n",
    "    add_pic_size is slave image size that is master_image_width(1600px)/add_pic_size(8)=200px. add_pic_pos refers to slave image position on master image.\\n\n",
    "    add_pic_margin is slave image shift from edges. border_color is slave image border color for highlight.\\n\n",
    "    ex.\\n\n",
    "    picture_in_picture(\"gibi.jpg\", \"Group 36.jpg\", add_pic_pos = [\"bottom\",\"left\"], add_pic_margin = 10, border_color = \"red\")\n",
    "    '''\n",
    "    # main image\n",
    "    try:  \n",
    "        image_main = Image.open(f\"{main_image_file_path}\")  # 1600*900 px, image file path\n",
    "    except:\n",
    "        image_main = main_image_file_path  # picture of result from RAM\n",
    "        \n",
    "    width_main, height_main = image_main.size\n",
    "\n",
    "    # additional image\n",
    "    image_add = Image.open(f\"{add_image_file_path}\")  # square image\n",
    "    height_add_edit = int(width_main/add_pic_size)  # 1600/8 = 200 px\n",
    "    width_add_edit = int(width_main/add_pic_size)\n",
    "    new_size_add = (height_add_edit ,width_add_edit)\n",
    "    image_add_resize = image_add.resize(new_size_add)\n",
    "    # border\n",
    "    border_thickness = (3, 3, 3, 3)\n",
    "    image_add_resize_border = ImageOps.expand(image_add_resize, border=border_thickness, fill=border_color)\n",
    "    width_add, height_add = image_add_resize_border.size\n",
    "\n",
    "    if (add_pic_pos[0] == \"top\") and (add_pic_pos[1] == \"left\"):\n",
    "        image_main.paste(image_add_resize_border, (add_pic_margin, add_pic_margin)) # x_koor, y_koor\n",
    "    elif (add_pic_pos[0] == \"top\") and (add_pic_pos[1] == \"right\"):\n",
    "        image_main.paste(image_add_resize_border, ((width_main-width_add-add_pic_margin), add_pic_margin))\n",
    "    elif (add_pic_pos[0] == \"bottom\") and (add_pic_pos[1] == \"left\"):\n",
    "        image_main.paste(image_add_resize_border, (add_pic_margin, (height_main-height_add-add_pic_margin)))\n",
    "    elif (add_pic_pos[0] == \"bottom\") and (add_pic_pos[1] == \"right\"):\n",
    "        image_main.paste(image_add_resize_border, ((width_main-width_add-add_pic_margin), (height_main-height_add-add_pic_margin)))\n",
    "    elif (add_pic_pos[0] == \"center\") and (add_pic_pos[1] == \"center\"):\n",
    "        image_main.paste(image_add_resize_border, ((int(width_main/2)-int(width_add/2)), (int(height_main/2)-int(height_add/2))))\n",
    "    elif (add_pic_pos[0] == \"left\") and (add_pic_pos[1] == \"center\"):\n",
    "        image_main.paste(image_add_resize_border, (add_pic_margin, (int(height_main/2)-int(height_add/2))))\n",
    "    elif (add_pic_pos[0] == \"right\") and (add_pic_pos[1] == \"center\"):\n",
    "       image_main.paste(image_add_resize_border, (width_main-width_add-add_pic_margin, (int(height_main/2)-int(height_add/2))))\n",
    "    elif (add_pic_pos[0] == \"top\") and (add_pic_pos[1] == \"center\"):\n",
    "        image_main.paste(image_add_resize_border, ((int(width_main/2)-int(width_add/2)), add_pic_margin))\n",
    "    elif (add_pic_pos[0] == \"bottom\") and (add_pic_pos[1] == \"center\"):\n",
    "        image_main.paste(image_add_resize_border, ((int(width_main/2)-int(width_add/2)), (height_main-height_add-add_pic_margin)))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return image_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pronouns Image Padding Aspect Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_output_path = f\"/home/kurubal/Downloads\"\n",
    "\n",
    "Path(pron_output_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron_image_path = f\"/home/kurubal/Downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_padding_aspect_ratio(\"Pronouns\", pron_image_path, \"Pronouns_Padding\", pron_output_path, padding_color = (255,255,255), aspect_ratio_width = 1, aspect_ratio_height = 1)  # /media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Turkish/Image Audio Video/Data/Pronouns_Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Image Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_image_path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Image Audio Video/Data/{file_ext} Words/Image_Genome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALL_OTHER_FOLDER',\n",
       " 'ALL_VERB_FOLDER',\n",
       " 'ALL_ADJ_ADV_PRON_FOLDER',\n",
       " 'Backup',\n",
       " 'ALL_NOUN_FOLDER']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_main_folder_list = os.listdir(f\"{select_image_path}\")\n",
    "image_main_folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_file_list = glob.glob(f\"{select_image_path}/ALL_OTHER_FOLDER/*/*\")  # other .jpg files\n",
    "#other_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_file_list = glob.glob(f\"{select_image_path}/ALL_VERB_FOLDER/*/*/*\")  # verb .jpg files\n",
    "#verb_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_adv_pros_file_list = glob.glob(f\"{select_image_path}/ALL_ADJ_ADV_PRON_FOLDER/*/*/*\")  # adj adv pros .jpg files\n",
    "#adj_adv_pros_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_file_list = glob.glob(f\"{select_image_path}/ALL_NOUN_FOLDER/*/*/*\")  # noun .jpg files\n",
    "#noun_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image_path_file in other_file_list:\n",
    "#    image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image_path_file in verb_file_list:\n",
    "#    image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image_path_file in adj_adv_pros_file_list:\n",
    "#    image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image_path_file in noun_file_list:\n",
    "#    image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual Genome Word Lemma All Category Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_image_path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Image Audio Video/Data/{file_ext} Words/Image_Genome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALL_OTHER_FOLDER',\n",
       " 'ALL_VERB_FOLDER',\n",
       " 'ALL_ADJ_ADV_PRON_FOLDER',\n",
       " 'Backup',\n",
       " 'ALL_NOUN_FOLDER']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_main_folder_list = os.listdir(f\"{main_image_path}\")\n",
    "image_main_folder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_image_path_folder = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Image Audio Video/Data/Pronouns_Padding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_folder = \"/home/kurubal/Downloads/Temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_word</th>\n",
       "      <th>POS1</th>\n",
       "      <th>POS2</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma.spacy</th>\n",
       "      <th>stem</th>\n",
       "      <th>word_en_translate</th>\n",
       "      <th>lemma_en_translate</th>\n",
       "      <th>frequency</th>\n",
       "      <th>prefix_suffix</th>\n",
       "      <th>tense</th>\n",
       "      <th>person</th>\n",
       "      <th>case</th>\n",
       "      <th>plural</th>\n",
       "      <th>mean</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bir</td>\n",
       "      <td>bir</td>\n",
       "      <td>bir</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>18835735</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biraz</td>\n",
       "      <td>biraz</td>\n",
       "      <td>bir</td>\n",
       "      <td>a little</td>\n",
       "      <td>a little</td>\n",
       "      <td>1269641</td>\n",
       "      <td>+az</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bir</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biri</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>somebody</td>\n",
       "      <td>somebody</td>\n",
       "      <td>837400</td>\n",
       "      <td>+i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bir</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birlikte</td>\n",
       "      <td>birlikte</td>\n",
       "      <td>bir</td>\n",
       "      <td>together</td>\n",
       "      <td>together</td>\n",
       "      <td>409940</td>\n",
       "      <td>+likte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bir</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birini</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>one</td>\n",
       "      <td>somebody</td>\n",
       "      <td>259916</td>\n",
       "      <td>+ini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>veriyor</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>veriyorum</td>\n",
       "      <td>ver</td>\n",
       "      <td>ver</td>\n",
       "      <td>i give</td>\n",
       "      <td>give</td>\n",
       "      <td>126290</td>\n",
       "      <td>+um</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.tekil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>veriyor</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>veriyor</td>\n",
       "      <td>ver</td>\n",
       "      <td>ver</td>\n",
       "      <td>giving</td>\n",
       "      <td>give</td>\n",
       "      <td>68163</td>\n",
       "      <td>+</td>\n",
       "      <td>şimdiki</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>okul</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>okul</td>\n",
       "      <td>okul</td>\n",
       "      <td>oku</td>\n",
       "      <td>school</td>\n",
       "      <td>school</td>\n",
       "      <td>68160</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>suçlu</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suçlu</td>\n",
       "      <td>suç</td>\n",
       "      <td>suç</td>\n",
       "      <td>guilty</td>\n",
       "      <td>crime</td>\n",
       "      <td>68124</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.tekil</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>mesela</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mesela</td>\n",
       "      <td>mesela</td>\n",
       "      <td>mesela</td>\n",
       "      <td>for example</td>\n",
       "      <td>for example</td>\n",
       "      <td>68096</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1624 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     search_word  POS1 POS2       word lemma.spacy    stem word_en_translate  \\\n",
       "0            bir   NUM  NaN        bir         bir     bir                 a   \n",
       "1            bir   ADV  NaN      biraz       biraz     bir          a little   \n",
       "2            bir  PRON  NaN       biri        biri     bir          somebody   \n",
       "3            bir   ADV  NaN   birlikte    birlikte     bir          together   \n",
       "4            bir  NOUN  NaN     birini        biri     bir               one   \n",
       "...          ...   ...  ...        ...         ...     ...               ...   \n",
       "1619     veriyor  VERB  NaN  veriyorum         ver     ver            i give   \n",
       "1620     veriyor  VERB  NaN    veriyor         ver     ver            giving   \n",
       "1621        okul  NOUN  NaN       okul        okul     oku            school   \n",
       "1622       suçlu  NOUN  NaN      suçlu         suç     suç            guilty   \n",
       "1623      mesela   ADV  NaN     mesela      mesela  mesela       for example   \n",
       "\n",
       "     lemma_en_translate  frequency prefix_suffix    tense   person      case  \\\n",
       "0                     a   18835735             +      NaN      NaN       NaN   \n",
       "1              a little    1269641           +az      NaN      NaN       NaN   \n",
       "2              somebody     837400            +i      NaN      NaN  belirtme   \n",
       "3              together     409940        +likte      NaN      NaN       NaN   \n",
       "4              somebody     259916          +ini      NaN      NaN  belirtme   \n",
       "...                 ...        ...           ...      ...      ...       ...   \n",
       "1619               give     126290           +um      NaN  1.tekil       NaN   \n",
       "1620               give      68163             +  şimdiki      NaN       NaN   \n",
       "1621             school      68160             +      NaN      NaN       NaN   \n",
       "1622              crime      68124             +      NaN  3.tekil  belirtme   \n",
       "1623        for example      68096             +      NaN      NaN       NaN   \n",
       "\n",
       "     plural mean other  \n",
       "0       NaN  NaN   NaN  \n",
       "1       NaN  NaN   NaN  \n",
       "2       NaN  NaN   NaN  \n",
       "3       NaN  NaN   NaN  \n",
       "4       NaN  NaN   NaN  \n",
       "...     ...  ...   ...  \n",
       "1619    NaN  NaN   NaN  \n",
       "1620    NaN  NaN   NaN  \n",
       "1621    NaN  NaN   NaN  \n",
       "1622    NaN  NaN   NaN  \n",
       "1623    NaN  NaN   NaN  \n",
       "\n",
       "[1624 rows x 16 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_prefix_suffix_file = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Lemma Stem POS/Result/2-1-Word Prefix Suffix Analysis/{lang_folder.capitalize()}_{file_ext}_Word_Person_Case_Tense_Plural_Mean.xlsx\")\n",
    "df_word_prefix_suffix_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_word</th>\n",
       "      <th>POS1</th>\n",
       "      <th>POS2</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma.spacy</th>\n",
       "      <th>stem</th>\n",
       "      <th>word_en_translate</th>\n",
       "      <th>lemma_en_translate</th>\n",
       "      <th>frequency</th>\n",
       "      <th>prefix_suffix</th>\n",
       "      <th>tense</th>\n",
       "      <th>person</th>\n",
       "      <th>case</th>\n",
       "      <th>plural</th>\n",
       "      <th>mean</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bir</td>\n",
       "      <td>bir</td>\n",
       "      <td>bir</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>18835735</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biraz</td>\n",
       "      <td>biraz</td>\n",
       "      <td>bir</td>\n",
       "      <td>a little</td>\n",
       "      <td>a little</td>\n",
       "      <td>1269641</td>\n",
       "      <td>+az</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bir</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biri</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>somebody</td>\n",
       "      <td>somebody</td>\n",
       "      <td>837400</td>\n",
       "      <td>+i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bir</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birlikte</td>\n",
       "      <td>birlikte</td>\n",
       "      <td>bir</td>\n",
       "      <td>together</td>\n",
       "      <td>together</td>\n",
       "      <td>409940</td>\n",
       "      <td>+likte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bir</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birini</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>one</td>\n",
       "      <td>somebody</td>\n",
       "      <td>259916</td>\n",
       "      <td>+ini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bir</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birisi</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>someone</td>\n",
       "      <td>somebody</td>\n",
       "      <td>182370</td>\n",
       "      <td>+isi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bir</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birinin</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>someone</td>\n",
       "      <td>somebody</td>\n",
       "      <td>135905</td>\n",
       "      <td>+inin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bir</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birine</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>to someone</td>\n",
       "      <td>somebody</td>\n",
       "      <td>111455</td>\n",
       "      <td>+ine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yönelme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bir</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birden</td>\n",
       "      <td>birden</td>\n",
       "      <td>bir</td>\n",
       "      <td>suddenly</td>\n",
       "      <td>suddenly</td>\n",
       "      <td>90124</td>\n",
       "      <td>+den</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bir</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biriyle</td>\n",
       "      <td>biri</td>\n",
       "      <td>bir</td>\n",
       "      <td>with someone</td>\n",
       "      <td>somebody</td>\n",
       "      <td>82104</td>\n",
       "      <td>+iyle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>11062659</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bunu</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>2445337</td>\n",
       "      <td>+nu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bunun</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>691351</td>\n",
       "      <td>+nun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>buna</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>this</td>\n",
       "      <td>this</td>\n",
       "      <td>487789</td>\n",
       "      <td>+na</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yönelme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bunlar</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>these</td>\n",
       "      <td>this</td>\n",
       "      <td>315388</td>\n",
       "      <td>+nlar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>çoğul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bundan</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>from this</td>\n",
       "      <td>this</td>\n",
       "      <td>292873</td>\n",
       "      <td>+ndan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ayrılma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bunları</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>these</td>\n",
       "      <td>this</td>\n",
       "      <td>204443</td>\n",
       "      <td>+nları</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belirtme</td>\n",
       "      <td>çoğul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bununla</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>with this</td>\n",
       "      <td>this</td>\n",
       "      <td>101648</td>\n",
       "      <td>+nunla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bu</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bunların</td>\n",
       "      <td>bu</td>\n",
       "      <td>bu</td>\n",
       "      <td>these</td>\n",
       "      <td>this</td>\n",
       "      <td>99699</td>\n",
       "      <td>+nların</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>çoğul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ne</td>\n",
       "      <td>PRON</td>\n",
       "      <td>Q</td>\n",
       "      <td>ne</td>\n",
       "      <td>ne</td>\n",
       "      <td>ne</td>\n",
       "      <td>what</td>\n",
       "      <td>what</td>\n",
       "      <td>8025880</td>\n",
       "      <td>+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   search_word  POS1 POS2      word lemma.spacy stem word_en_translate  \\\n",
       "0          bir   NUM  NaN       bir         bir  bir                 a   \n",
       "1          bir   ADV  NaN     biraz       biraz  bir          a little   \n",
       "2          bir  PRON  NaN      biri        biri  bir          somebody   \n",
       "3          bir   ADV  NaN  birlikte    birlikte  bir          together   \n",
       "4          bir  NOUN  NaN    birini        biri  bir               one   \n",
       "5          bir  NOUN  NaN    birisi        biri  bir           someone   \n",
       "6          bir  NOUN  NaN   birinin        biri  bir           someone   \n",
       "7          bir  NOUN  NaN    birine        biri  bir        to someone   \n",
       "8          bir   ADV  NaN    birden      birden  bir          suddenly   \n",
       "9          bir  NOUN  NaN   biriyle        biri  bir      with someone   \n",
       "10          bu  PRON  NaN        bu          bu   bu              this   \n",
       "11          bu  PRON  NaN      bunu          bu   bu              this   \n",
       "12          bu  PRON  NaN     bunun          bu   bu              this   \n",
       "13          bu  PRON  NaN      buna          bu   bu              this   \n",
       "14          bu  PRON  NaN    bunlar          bu   bu             these   \n",
       "15          bu  PRON  NaN    bundan          bu   bu         from this   \n",
       "16          bu  PRON  NaN   bunları          bu   bu             these   \n",
       "17          bu  PRON  NaN   bununla          bu   bu         with this   \n",
       "18          bu  PRON  NaN  bunların          bu   bu             these   \n",
       "19          ne  PRON    Q        ne          ne   ne              what   \n",
       "\n",
       "   lemma_en_translate  frequency prefix_suffix tense person      case plural  \\\n",
       "0                   a   18835735             +   NaN    NaN       NaN    NaN   \n",
       "1            a little    1269641           +az   NaN    NaN       NaN    NaN   \n",
       "2            somebody     837400            +i   NaN    NaN  belirtme    NaN   \n",
       "3            together     409940        +likte   NaN    NaN       NaN    NaN   \n",
       "4            somebody     259916          +ini   NaN    NaN  belirtme    NaN   \n",
       "5            somebody     182370          +isi   NaN    NaN  belirtme    NaN   \n",
       "6            somebody     135905         +inin   NaN    NaN       NaN    NaN   \n",
       "7            somebody     111455          +ine   NaN    NaN   yönelme    NaN   \n",
       "8            suddenly      90124          +den   NaN    NaN       NaN    NaN   \n",
       "9            somebody      82104         +iyle   NaN    NaN       NaN    NaN   \n",
       "10               this   11062659             +   NaN    NaN  belirtme    NaN   \n",
       "11               this    2445337           +nu   NaN    NaN  belirtme    NaN   \n",
       "12               this     691351          +nun   NaN    NaN       NaN    NaN   \n",
       "13               this     487789           +na   NaN    NaN   yönelme    NaN   \n",
       "14               this     315388         +nlar   NaN    NaN       NaN  çoğul   \n",
       "15               this     292873         +ndan   NaN    NaN   ayrılma    NaN   \n",
       "16               this     204443        +nları   NaN    NaN  belirtme  çoğul   \n",
       "17               this     101648        +nunla   NaN    NaN       NaN    NaN   \n",
       "18               this      99699       +nların   NaN    NaN       NaN  çoğul   \n",
       "19               what    8025880             +   NaN    NaN       NaN    NaN   \n",
       "\n",
       "   mean other  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  \n",
       "5   NaN   NaN  \n",
       "6   NaN   NaN  \n",
       "7   NaN   NaN  \n",
       "8   NaN   NaN  \n",
       "9   NaN   NaN  \n",
       "10  NaN   NaN  \n",
       "11  NaN   NaN  \n",
       "12  NaN   NaN  \n",
       "13  NaN   NaN  \n",
       "14  NaN   NaN  \n",
       "15  NaN   NaN  \n",
       "16  NaN   NaN  \n",
       "17  NaN   NaN  \n",
       "18  NaN   NaN  \n",
       "19  NaN   NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_prefix_suffix_file = df_word_prefix_suffix_file.head(20)\n",
    "df_word_prefix_suffix_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_prefix_suffix_file[\"word\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = df_word_prefix_suffix_file[\"word\"].unique()\n",
    "#word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_image_name = list(df_word_prefix_suffix_file[\"tense\"].dropna().unique())\n",
    "person_image_name = list(df_word_prefix_suffix_file[\"person\"].dropna().unique())\n",
    "case_image_name = list(df_word_prefix_suffix_file[\"case\"].dropna().unique())\n",
    "plural_image_name = list(df_word_prefix_suffix_file[\"plural\"].dropna().unique())\n",
    "mean_image_name = list(df_word_prefix_suffix_file[\"mean\"].dropna().unique())\n",
    "other_image_name = list(df_word_prefix_suffix_file[\"other\"].dropna().unique())\n",
    "POS2_image_name = list(df_word_prefix_suffix_file[\"POS2\"].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['belirtme', 'yönelme', 'ayrılma', 'çoğul', 'Q']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_image_list = tense_image_name + person_image_name + case_image_name + plural_image_name + mean_image_name + other_image_name + POS2_image_name\n",
    "add_image_list\n",
    "# Note: This add image list must be image file name as belirtme.jpg, yönelme.jpg etc. for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for word in word_list:\n",
    "for row_num in range(len(df_word_prefix_suffix_file)):\n",
    "    # main file result\n",
    "    df_var = df_word_prefix_suffix_file.iloc[[row_num],]\n",
    "    word = \"\".join(df_var[\"word\"])\n",
    "    try:\n",
    "        tense = \"\".join(df_var[\"tense\"])\n",
    "        tense_file = glob.glob(f\"{add_image_path_folder}/{tense}.jpg\")\n",
    "    except:\n",
    "        tense_file = []\n",
    "    try:\n",
    "        person = \"\".join(df_var[\"person\"])\n",
    "        person_file = glob.glob(f\"{add_image_path_folder}/{person}.jpg\")\n",
    "    except:\n",
    "        person_file = []\n",
    "    try:\n",
    "        case = \"\".join(df_var[\"case\"])\n",
    "        case_file = glob.glob(f\"{add_image_path_folder}/{case}.jpg\")\n",
    "    except:\n",
    "        case_file = []\n",
    "    try:\n",
    "        plural = \"\".join(df_var[\"plural\"])\n",
    "        plural_file = glob.glob(f\"{add_image_path_folder}/{plural}.jpg\")\n",
    "    except:\n",
    "        plural_file = []\n",
    "    try:\n",
    "        mean = \"\".join(df_var[\"mean\"])\n",
    "        mean_file = glob.glob(f\"{add_image_path_folder}/{mean}.jpg\")\n",
    "    except:\n",
    "        mean_file = []\n",
    "    try:\n",
    "        other = \"\".join(df_var[\"other\"])\n",
    "        other_file = glob.glob(f\"{add_image_path_folder}/{other}.jpg\")\n",
    "    except:\n",
    "        other_file = []\n",
    "    try:\n",
    "        pos = \"\".join(df_var[\"POS2\"])\n",
    "        pos_file = glob.glob(f\"{add_image_path_folder}/{pos}.jpg\")\n",
    "    except:\n",
    "        pos_file = []\n",
    "\n",
    "    add_image_file_list = tense_file + person_file + case_file + plural_file + mean_file + other_file + pos_file\n",
    "    \n",
    "    # word search in word folders\n",
    "    file_list1 = glob.glob(f\"{main_image_path}/ALL_VERB_FOLDER/*/{word}/*\")\n",
    "    file_list2 = glob.glob(f\"{main_image_path}/ALL_NOUN_FOLDER/*/{word}/*\")\n",
    "    file_list3 = glob.glob(f\"{main_image_path}/ALL_ADJ_ADV_PRON_FOLDER/*/{word}/*\")\n",
    "    file_list4 = glob.glob(f\"{main_image_path}/ALL_OTHER_FOLDER/{word}/*\")\n",
    "\n",
    "    main_image_file_list = file_list1 + file_list2 + file_list3 + file_list4\n",
    "\n",
    "    # create folder\n",
    "    folder_name = f\"{word}\"\n",
    "    Path(f\"{output_path_folder}/{folder_name}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # main image and additional image picture_in_picture\n",
    "    main_num = 1\n",
    "    for main_image_path_file in main_image_file_list:\n",
    "        main_image = Image.open(f\"{main_image_path_file}\") \n",
    "        add_num = 1\n",
    "        for add_image in add_image_file_list:\n",
    "            if add_num == 1:\n",
    "                main_image = picture_in_picture(main_image, f\"{add_image}\", add_pic_pos = [\"top\",\"left\"], add_pic_margin = 10, border_color = \"red\")\n",
    "            elif add_num == 2:\n",
    "                main_image = picture_in_picture(main_image, f\"{add_image}\", add_pic_pos = [\"top\",\"right\"], add_pic_margin = 10, border_color = \"red\")\n",
    "            elif add_num == 3:\n",
    "                main_image = picture_in_picture(main_image, f\"{add_image}\", add_pic_pos = [\"bottom\",\"left\"], add_pic_margin = 10, border_color = \"red\")\n",
    "            elif add_num == 4:\n",
    "                main_image = picture_in_picture(main_image, f\"{add_image}\", add_pic_pos = [\"bottom\",\"right\"], add_pic_margin = 10, border_color = \"red\")\n",
    "            else:\n",
    "                pass\n",
    "            add_num += 1\n",
    "\n",
    "        if isfile(f\"{output_path_folder}/{word}/{word}{main_num}.jpg\"):\n",
    "            pass\n",
    "        else:\n",
    "            main_image.save(f\"{output_path_folder}/{word}/{word}{main_num}.jpg\")\n",
    "\n",
    "        main_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pronouns Image New Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_file = \"/home/kurubal/Downloads/Pronouns/belirtme.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_padding_aspect_ratio_standalone(image_path_file, padding_color = (255,255,255), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = False): # ??? will edit definition\n",
    "       '''\n",
    "       image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = False)\\n\n",
    "       image_path_file are path, folder and file in string(str). others are cv2 parameter and image aspect ratio parameter.\\n\n",
    "       ex.\\n\n",
    "       image_path_file = \"/home/kurubal/Downloads/2333869.jpg\"\\n\n",
    "       image_padding_aspect_ratio_standalone(image_path_file, padding_color = (0,0,0), aspect_ratio_width = 16, aspect_ratio_height = 9, overwrite = False)\n",
    "       '''\n",
    "       error_file_list = []\n",
    "       size = (200, 200)  # image new size (width, height)\n",
    "       # path process\n",
    "       path_split_list = os.path.split(image_path_file)\n",
    "       main_path = path_split_list[0]  # part of before last one\n",
    "       file = path_split_list[1]  # last file or folder\n",
    "       filename_without_ext = os.path.splitext(file)[0]  # file without .jpg\n",
    "       filename_ext = os.path.splitext(file)[1]  # file extention as .jpg\n",
    "       # file process\n",
    "       if isfile(f\"{main_path}/{filename_without_ext}_padded{filename_ext}\"):\n",
    "              pass\n",
    "       else:\n",
    "              try:\n",
    "                     img = cv2.imread(f\"{image_path_file}\")\n",
    "                     old_image_height, old_image_width, channels = img.shape \n",
    "\n",
    "                     if (old_image_height/aspect_ratio_height) > (old_image_width/aspect_ratio_width):\n",
    "                            new_image_width = int((old_image_height/aspect_ratio_height)*aspect_ratio_width)\n",
    "                            new_image_height = int(old_image_height)\n",
    "                     elif (old_image_height/aspect_ratio_height) < (old_image_width/aspect_ratio_width):\n",
    "                            new_image_width = int(old_image_width)\n",
    "                            new_image_height = int((old_image_width/aspect_ratio_width)*aspect_ratio_height)\n",
    "                     elif (old_image_height/aspect_ratio_height) == (old_image_width/aspect_ratio_width):\n",
    "                            new_image_width = int((old_image_height/aspect_ratio_height)*aspect_ratio_width)\n",
    "                            new_image_height = int(old_image_height)\n",
    "                     else:\n",
    "                            new_image_width = old_image_width\n",
    "                            new_image_height = old_image_height\n",
    "\n",
    "                     color = padding_color\n",
    "                     result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "\n",
    "                     # compute center offset\n",
    "                     x_center = (new_image_width - old_image_width) // 2\n",
    "                     y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "                     # copy img image into center of result image\n",
    "                     result[y_center:y_center+old_image_height, x_center:x_center+old_image_width] = img\n",
    "\n",
    "                     # image resize\n",
    "                     result_resize = cv2.resize(result, size, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "                     if overwrite:\n",
    "                            cv2.imwrite(f\"{main_path}/{filename_without_ext}{filename_ext}\", result_resize)  # for overwrite\n",
    "                     else:\n",
    "                            cv2.imwrite(f\"{main_path}/{filename_without_ext}_padded{filename_ext}\", result_resize)\n",
    "              except:\n",
    "                     print(\"Oops!\", sys.exc_info()[0], \"occurred.\")\n",
    "                     error_file_list.append(image_path_file)\n",
    "                        \n",
    "       return error_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_padding_aspect_ratio_standalone(image_path_file, padding_color = (255,255,255), aspect_ratio_width = 1, aspect_ratio_height = 1)  # 200*200px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
