{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Group In Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Arabic\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "#lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # native word end index\n",
    "unique_word_ratio = 70  # 28,200 = 70 ; 1000,5000 = 80 ; 10000 = 85 ; 20000,40000 = 90\n",
    "include_wordgroup_ratio = 70  # 28,200 = 70 ; 1000,5000 = 80 : 10000 = 85 ; 20000,40000 = 90\n",
    "word_usage_min = 10  # 28,200,1000,5000,10000 = 10 ; 20000 = 7 ; 40000 = 5\n",
    "word_usage_max = 100\n",
    "\n",
    "# sentence ratio and time shift\n",
    "adjust_sent_word_ratio = 15  #28,200 = 15 ; 1000,5000,10000 = 25 ; 20000 = 35 ; 40000 = 45\n",
    "#adjust_text_word_ratio = 5  #28,200 = 5 ; 1000,5000,10000 = 25 ; 20000 = 35 ; 40000 = 45\n",
    "shift = 0.3  # sentence time shift\n",
    "\n",
    "# func select\n",
    "if word_end == 28:\n",
    "    sentence_word_count_number = 2\n",
    "else:\n",
    "    sentence_word_count_number = 5   \n",
    "word_group_adjust_select = False  # True, False; True for selecting word group according to sentence_word_count_number in video_id text analysis\n",
    "                                  # False for selecting word group according to max word group length \n",
    "\n",
    "# shared word frequency\n",
    "word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # True for adding prefix suffix word\n",
    "native_word = True # True for adding native word\n",
    "\n",
    "# adding output file extention\n",
    "if prefix_suffix & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Language Level/Data/1-Word Group In Youtube Sentence\"\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_usage_result(word_list, df_target, target_column, target_opt_column, word_usage_min, word_usage_max):\n",
    "    '''\n",
    "    word_usage_result(word_list, df_ngram_pair, \"threegram\", \"frequency\", 1, 5) \\n\n",
    "    word_list is a list, df_target is a dateframe, target_column is df_target dataframe target column, \\n\n",
    "    target_opt_column is df_target dataframe opt_target column, \\n\n",
    "    word_usage_min and word_usage_max word usage condition.\n",
    "    '''    \n",
    "    word_num_dict = {}\n",
    "    for i in word_list:\n",
    "        word_num_dict[f\"{i}\"] = 0\n",
    "    \n",
    "    result_list_select = []\n",
    "    var_list = []\n",
    "    for i in range(len(df_target)):\n",
    "        target_value = df_target.loc[i,f\"{target_column}\"]\n",
    "        opt_value = df_target.loc[i,f\"{target_opt_column}\"]\n",
    "        words = word_tokenize(target_value)   \n",
    "        temp_list = [word for word in words]\n",
    "        temp_list = temp_list + var_list\n",
    "        # word count for max\n",
    "        dict_list_count = Counter(temp_list)\n",
    "        count_list = list(dict_list_count.values())\n",
    "        # word count for min\n",
    "        count_list2 = list(word_num_dict.values())\n",
    "    \n",
    "        if any([True if i>word_usage_max else False for i in count_list]) or not(any([True if j<word_usage_min else False for j in count_list2])):\n",
    "            pass\n",
    "        else:\n",
    "            var_list = temp_list\n",
    "            result_list_select.append([target_value,opt_value]) \n",
    "    \n",
    "            for item2 in dict_list_count.items(): \n",
    "                word_num_dict[item2[0]] = item2[1]        \n",
    "    df_result = pd.DataFrame(result_list_select, columns=[f\"{target_column}\",f\"{target_opt_column}\"])\n",
    "    df_result.sort_values(by=f\"{target_opt_column}\", ascending=False, inplace=True)\n",
    "    df_result.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exract_word_group(text, word_list):\n",
    "    '''\n",
    "    exract_word_group(text, word_list): \\n\n",
    "    text is a string sentence, word_list occurs words \n",
    "    '''\n",
    "    words = re.findall(r\"\\w+\", text, re.UNICODE)\n",
    "    index_list = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in word_list:\n",
    "            index_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    all_index_list = []\n",
    "    var_index_list = []\n",
    "    for j in range(len(index_list)):\n",
    "        try:\n",
    "            var1 = index_list[j] + 1  \n",
    "            var2 = index_list[j+1]\n",
    "        except:\n",
    "            var1 = index_list[j] + 1  \n",
    "            var2 = index_list[-1]\n",
    "        if var1 == var2:\n",
    "            var3 = index_list[j]\n",
    "            var_index_list.append(var3)\n",
    "        else:\n",
    "            var3 = index_list[j]\n",
    "            var_index_list.append(var3)\n",
    "            var4 = list(var_index_list)\n",
    "            all_index_list.append(var4)\n",
    "            var_index_list = []\n",
    "\n",
    "    text_list = []\n",
    "    for k in max(all_index_list, key=len):  # any error convert k to i\n",
    "        word = words[k]\n",
    "        text_list.append(word)\n",
    "        if len(text_list) >= sentence_word_count_number:\n",
    "            text = \" \".join(text_list)\n",
    "        else:\n",
    "            text = f\"sentencte_word_count_less_than_{sentence_word_count_number}\"\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_exract_word_group(df, source_text_column, opt_column, word_list, sent_len=False, sent_len_num=2):\n",
    "    '''\n",
    "    df_exract_word_group(df_adjust_text_ratio, sentence, video_id, word_list, sent_len=False, sent_len_num=2): \\n\n",
    "    df_adjust_text_ratio is a dataframe and it includes sentence and video_id columns. \\n\n",
    "    sentence is a string sentence. video_id is optional column value. word_list occurs words \\n\n",
    "    sent_len is sentence length condition. sent_len_num is sentence occurs how many word at least. \n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_new = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        source_text = df.loc[i,f\"{source_text_column}\"]\n",
    "        opt_var = df.loc[i,f\"{opt_column}\"]\n",
    "        words = re.findall(r\"\\w+\", source_text, re.UNICODE)\n",
    "        \n",
    "        index_list = []\n",
    "        for j in range(len(words)):\n",
    "            if words[j] in word_list:\n",
    "                index_list.append(j)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        all_index_list = []\n",
    "        var_index_list = []\n",
    "        for k in range(len(index_list)):\n",
    "            try:\n",
    "                var1 = index_list[k] + 1  \n",
    "                var2 = index_list[k+1]\n",
    "            except:\n",
    "                var1 = index_list[k] + 1  \n",
    "                var2 = index_list[-1]\n",
    "            if var1 == var2:\n",
    "                var3 = index_list[k]\n",
    "                var_index_list.append(var3)\n",
    "            else:\n",
    "                var3 = index_list[k]\n",
    "                var_index_list.append(var3)\n",
    "                var4 = list(var_index_list)\n",
    "                all_index_list.append(var4)\n",
    "                var_index_list = []\n",
    "\n",
    "        text_all_list = []\n",
    "        for m in all_index_list:\n",
    "            text_list = [] \n",
    "            for n in m:\n",
    "                word = words[n]\n",
    "                text_list.append(word)\n",
    "                if sent_len:\n",
    "                    if len(text_list) >= sent_len_num:\n",
    "                        text = \" \".join(text_list)\n",
    "                        text_all_list.append(text)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                   text = \" \".join(text_list)\n",
    "                   text_all_list.append(text) \n",
    "            \n",
    "        for search_string in text_all_list:\n",
    "            #df_var[\"index\"] = i\n",
    "            #df_var[\"search_string\"] = search_string\n",
    "            #df_var[\"sentence\"] = source_text\n",
    "            #df_var[\"video_id\"] = opt_var\n",
    "            df_list = []\n",
    "            df_list.append([i,search_string,source_text,opt_var])\n",
    "            df_var = pd.DataFrame(df_list,columns=[\"index\",\"search_string\",f\"{source_text_column}\",f\"{opt_column}\"])\n",
    "            df_new = pd.concat([df_new, df_var], axis=0)\n",
    "\n",
    "    return df_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_video_id):\n",
    "    '''\n",
    "    word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)  # will test\n",
    "    word_time_loc_list = []\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_video_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_video_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repetation_ratio(text):\n",
    "    '''\n",
    "    repetation_ratio(text) \n",
    "    text is a like as sentence and def compute repetation word ratio\\n\n",
    "    repetation_ratio(\"yok bir şey yok bir şey yok bir şey\") \n",
    "    '''\n",
    "    word_list = word_tokenize(text)\n",
    "    word_num = len(word_list)\n",
    "    word_unique_num = len(set(word_list))\n",
    "    ratio = int((word_unique_num/word_num)*100)\n",
    "    \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select = df_word_all.iloc[word_start:word_end,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_word = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Data/{lang_folder.capitalize()}/{lang_folder.capitalize()}/{lang_folder.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "    df_word = df_word.loc[:,[\"word\",\"frequency\"]]\n",
    "    df_word = pd.concat([df_word,df_word_select], axis=0)\n",
    "    df_word.drop_duplicates(inplace=True)    \n",
    "    df_word.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    df_word = df_word_select\n",
    "\n",
    "if native_word:\n",
    "    df_word\n",
    "else:\n",
    "    df_word = df_word.head(0)\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = [\"sex\",\"seks\",\"seksi\",\"sexy\",\"sexe\",\"seksüel\",\"sexuell\",\"gey\",\"gay\",\"lezbiyen\",\"lesbienne\",\"eşcinsel\",\"mastürbasyon\",\"masturbation\",\"erotik\",\"érotique\", \\\n",
    "\"bikini\",\"penis\",\"vagina\",\"vajina\",\"fetish\",\"fetiş\",\"fetishy\",\"erotic\",\"erotik\",\"sexdom\",\"kondom\",\"condom\",\"dildo\",\"fetisj\",\"hétérosexuel\",\"féticher\",\"fétiche\",\"homosexuel\"\\\n",
    "\"ereksiyon\",\"erectie\",\"erection\",\"érection\",\"homoseksüel\",\"prezervatif\",\"préservatif\",\"ass\",\"fetisch\",\"fetiche\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_select = df_word[\"word\"].values.tolist()\n",
    "word_select_set = set(word_select)\n",
    "disable_word_set = set(disable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(word_select_set.difference(disable_word_set))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Sentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence.reset_index(inplace=True)  # adding index column\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_youtube_sentence[df_youtube_sentence[\"index\"] == 1287503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_youtube_sentence)))\n",
    "\n",
    "def sentence_word_ratio(index_num):\n",
    "    index = df_youtube_sentence.loc[index_num,\"index\"]\n",
    "    start = df_youtube_sentence.loc[index_num,\"start_time\"]\n",
    "    end = df_youtube_sentence.loc[index_num,\"end_time\"]\n",
    "    sentence = df_youtube_sentence.loc[index_num,\"sentence\"]\n",
    "    videoid = df_youtube_sentence.loc[index_num,\"video_id\"]\n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/(len(sent_word)+0.001))*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list.append([index,start,end,sentence,videoid,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs-2) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(sentence_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result = pd.DataFrame(list(result_list), columns=[\"index\",\"start_time\",\"end_time\",\"sentence\",\"video_id\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_sentence_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_sentence_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_sentence_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result[\"different_word\"] = df_sentence_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result[\"intersect_word\"] = df_sentence_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_sentence_ratio = df_sentence_ratio_result[df_sentence_ratio_result[\"word_ratio\"] >= adjust_sent_word_ratio]\n",
    "df_adjust_sentence_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_sentence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_sentence_ratio.loc[:,\"search_string\"] = df_adjust_sentence_ratio.loc[:,\"sentence\"].map(lambda x: exract_word_group(x, word_list))\n",
    "df_adjust_sentence_ratio = df_adjust_sentence_ratio[~(df_adjust_sentence_ratio[\"search_string\"]==f\"sentencte_word_count_less_than_{sentence_word_count_number}\")]\n",
    "#df_adjust_sentence_ratio.reset_index(drop=True, inplace=True)\n",
    "df_adjust_sentence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result = word_group_time_loc(df_adjust_sentence_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"start_time\"] = df_word_group_time_loc_in_sent_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_sent_result[\"end_time\"] = df_word_group_time_loc_in_sent_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"start_time\"] = df_word_group_time_loc_in_sent_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_sent_result[\"end_time\"] = df_word_group_time_loc_in_sent_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_sent_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"unique_word_ratio\"] = df_word_group_time_loc_in_sent_result[\"search_string\"].apply(lambda text: repetation_ratio(text))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result_repeat_ratio = df_word_group_time_loc_in_sent_result[df_word_group_time_loc_in_sent_result[\"unique_word_ratio\"] > unique_word_ratio]\n",
    "df_word_group_time_loc_in_sent_result_repeat_ratio.drop_duplicates(subset=[\"search_string\",\"start_time\",\"end_time\"], inplace=True)\n",
    "df_word_group_time_loc_in_sent_result_repeat_ratio.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_sent_result_repeat_ratio.drop(\"unique_word_ratio\", axis=1, inplace=True)\n",
    "df_word_group_time_loc_in_sent_result_repeat_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result_repeat_ratio[\"include_string_ratio\"] = df_word_group_time_loc_in_sent_result_repeat_ratio.apply(lambda x: int((len(str(x['search_string']))/len(str(x['sentence'])))*100) ,axis=1)\n",
    "df_word_group_time_loc_in_sent_result_repeat_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_include_string_ratio = df_word_group_time_loc_in_sent_result_repeat_ratio[df_word_group_time_loc_in_sent_result_repeat_ratio[\"include_string_ratio\"] > include_wordgroup_ratio]\n",
    "df_word_group_time_loc_include_string_ratio.drop(\"include_string_ratio\", axis=1, inplace=True)\n",
    "df_word_group_time_loc_include_string_ratio.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_include_string_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_word_usage = word_usage_result(word_list, df_word_group_time_loc_include_string_ratio, \"search_string\", \"video_id\", word_usage_min, word_usage_max)\n",
    "df_youtube_word_usage.drop_duplicates(inplace=True)\n",
    "df_youtube_word_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_word_usage_merge = pd.merge(df_word_group_time_loc_include_string_ratio,df_youtube_word_usage,how=\"inner\",on=[\"search_string\",\"video_id\"])\n",
    "df_youtube_word_usage_merge.drop_duplicates(subset=[\"search_string\",\"video_id\"],inplace=True)\n",
    "df_youtube_word_usage_merge.reset_index(drop=True, inplace=True)\n",
    "df_youtube_word_usage_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_word_usage_merge[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_youtube_word_usage_merge['video_id'].map(str)+\"&t=\"+df_youtube_word_usage_merge['start_time'].map(str)+\"s\"\n",
    "df_youtube_word_usage_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_word_usage_merge = df_youtube_word_usage_merge.head(2000)\n",
    "df_youtube_word_usage_merge.to_csv(f\"{lang_folder.capitalize()}_Word_Group_Max_In_Youtube_Sentence_{word_end}_Word{file_ext}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_word_usage_merge = df_youtube_word_usage_merge.head(700)\n",
    "df_youtube_word_usage_merge.to_excel(f\"{lang_folder.capitalize()}_Word_Group_Max_In_Youtube_Sentence_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_word_group_time_loc_in_sent_result = df_word_group_time_loc_in_sent_result.head(1000000)\n",
    "#df_word_group_time_loc_in_sent_result.to_excel(f\"{lang_folder.capitalize()}_Word_Group_Max_In_Youtube_Sentence_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_Word_Group*{file_ext}.*\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = path\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
