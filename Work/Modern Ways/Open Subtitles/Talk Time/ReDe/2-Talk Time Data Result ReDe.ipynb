{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "#lang_pair = \"Intersect\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0 native word start index\n",
    "word_end = 200  # 28 native word end index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup_simple(source_word_list, df_target, target_column, word_sample_num, simple=False):\n",
    "\n",
    "    '''word_in_wordgroup(not_in_sent_word_list, df_youtube_sent_select, \"search_string\", 5, simple=False)\\n\n",
    "       source_word_list is searching word list\\n\n",
    "       df_target is dataframe, target_column are dataframe column string name\\n\n",
    "       word_sample_num is searching sample number.\n",
    "       simple use for all column row result or only target column result \n",
    "    '''\n",
    "    if simple:\n",
    "        df_select = df_target[[f\"{target_column}\"]].dropna()\n",
    "    else:\n",
    "        df_select = df_target\n",
    "        \n",
    "    df_result = pd.DataFrame()\n",
    "    for i in source_word_list:\n",
    "        try:\n",
    "            word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(word_sample_num)    \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,\"search_string\",i)\n",
    "        df_result = pd.concat([df_result,word_in_word_cluster], axis=0)\n",
    "    df_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_youtube(df, search_list, target_column, sample_num):\n",
    "    '''\n",
    "    word_group_youtube(df_youtube_sentence, search_list, \"sentence\", 6)\n",
    "    ''' \n",
    "    df_search_result = pd.DataFrame()\n",
    "    for j in search_list:\n",
    "        try:\n",
    "            df_select = df[df[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){j}(?:\\s|$)\", na=True)].sample(sample_num)\n",
    "        except:\n",
    "            df_select = df[df[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){j}(?:\\s|$)\", na=True)].head(sample_num)\n",
    "        #df_result = df[df[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){j}(?:\\s|$)\", na=True)]  # sentence length part\n",
    "        #df_result.sort_values(f\"{target_column}\",key=lambda x:x.str.len(), inplace=True)\n",
    "        #df_select = df_result.head(sample_num)\n",
    "        df_select.insert(0,\"search_string\",j)\n",
    "        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "    df_search_result.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_video_id):\n",
    "    '''\n",
    "    word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    word_time_loc_list = []\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_video_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_video_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df, column_list, set_condition=False): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        if set_condition:\n",
    "            var_list = set(df[f\"{i}\"].dropna().tolist())\n",
    "        else:\n",
    "            var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    df_word_count.sort_values(\"word_count\", ascending=False, inplace=True)\n",
    "    df_word_count.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Talk Time/Result/1-Talk Time/{lang_folder.capitalize()}\"\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select = df_word_all.iloc[word_start:word_end,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select.to_excel(f\"{lang_folder.capitalize()}_200_Word.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = df_word_select[\"word\"].to_list()\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other option \n",
    "# mUf7VNqChac =>  black screen\n",
    "# 0_CDMstFg7M => 10sn\n",
    "# bj1JRuyYeco => 20sn\n",
    "df_link_default = pd.DataFrame(data=[[\"repeat\",0,2,\"bj1JRuyYeco\",\"https://www.youtube.com/watch?v=bj1JRuyYeco&t=0s\"]], columns=[\"search_string\",\"start_time\",\"end_time\",\"video_id\",\"video_url\"])\n",
    "df_link_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_link = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Talk Time/Data/1-Talk Time Data Prepare/{lang_folder.capitalize()}/{lang_folder.capitalize()}_200_Word_6_Youtube_0.6s_Timeshift_For_Talk_Time_Result_Manuel.xlsx\")\n",
    "df_word_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_link[df_word_link[\"search_string\"].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_link = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Talk Time/Data/1-Talk Time Data Prepare/{lang_folder.capitalize()}/{lang_folder.capitalize()}_Twogram_With_200_Word_6_Youtube_0.6s_Timeshift_For_Talk_Time_Result_Manuel.xlsx\")\n",
    "df_twogram_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threegram_link = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Talk Time/Data/1-Talk Time Data Prepare/{lang_folder.capitalize()}/{lang_folder.capitalize()}_Threegram_With_200_Word_6_Youtube_0.6s_Timeshift_For_Talk_Time_Result_Manuel.xlsx\")\n",
    "df_threegram_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_link = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Talk Time/Data/1-Talk Time Data Prepare/{lang_folder.capitalize()}/{lang_folder.capitalize()}_200_Word_Group_In_Youtube_Sentence_Sample_Selected_Manuel.xlsx\")\n",
    "df_sentence_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for multi search result\n",
    "#twogram_link_list = df_twogram_link[\"search_string\"].to_list()\n",
    "#threegram_link_list = df_threegram_link[\"search_string\"].to_list()\n",
    "df_result = pd.DataFrame()\n",
    "for sent in df_sentence_link[\"search_string\"]:\n",
    "\n",
    "    # words\n",
    "    sent_words = word_tokenize(sent)\n",
    "\n",
    "    # twogram\n",
    "    twogram_zip = ngrams(sent.split(), 2)\n",
    "    twogram_list = [\" \".join(x) for x in twogram_zip]\n",
    "    #df_twogram_var = pd.DataFrame(data=twogram, columns=[\"twogram\"])\n",
    "    \n",
    "    # threegram\n",
    "    threegram_zip = ngrams(sent.split(), 3)\n",
    "    threegram_list = [\" \".join(y) for y in threegram_zip]\n",
    "    #df_threegram_var = pd.DataFrame(data=threegram, columns=[\"threegram\"])\n",
    "    \n",
    "    # word result\n",
    "    for word in sent_words:        \n",
    "        df_word_search_var = df_word_link[df_word_link[\"search_string\"] == word]\n",
    "        df_word_search_var.reset_index(drop=True, inplace=True)\n",
    "        for i in range(len(df_word_search_var)):\n",
    "            df_link_default_var = df_link_default\n",
    "            try:\n",
    "                word_time_diff_var = df_word_search_var.loc[i,\"end_time\"] - df_word_search_var.loc[i,\"start_time\"]\n",
    "                df_link_default_var.loc[0,\"end_time\"] = df_link_default_var.loc[0,\"start_time\"] + word_time_diff_var+1.0\n",
    "                df_result = pd.concat([df_result,df_word_search_var.iloc[[i,]]], axis=0)\n",
    "                df_result = pd.concat([df_result,df_link_default_var], axis=0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # twogram result\n",
    "    df_twogram_search_var = df_twogram_link[df_twogram_link[\"search_string\"].isin(twogram_list)]\n",
    "    df_twogram_search_var.reset_index(drop=True, inplace=True)\n",
    "    for j in range(len(df_twogram_search_var)):\n",
    "        df_link_default_var = df_link_default\n",
    "        try:\n",
    "            twogram_time_diff_var = df_twogram_search_var.loc[j,\"end_time\"] - df_twogram_search_var.loc[j,\"start_time\"]\n",
    "            df_link_default_var.loc[0,\"end_time\"] = df_link_default_var.loc[0,\"start_time\"] + twogram_time_diff_var+1.0\n",
    "            df_result = pd.concat([df_result,df_twogram_search_var.iloc[[j,]]], axis=0)\n",
    "            df_result = pd.concat([df_result,df_link_default_var], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # threegram result\n",
    "    df_threegram_search_var = df_threegram_link[df_threegram_link[\"search_string\"].isin(threegram_list)]\n",
    "    df_threegram_search_var.reset_index(drop=True, inplace=True)\n",
    "    for k in range(len(df_threegram_search_var)):\n",
    "        df_link_default_var = df_link_default\n",
    "        try:\n",
    "            threegram_time_diff_var = df_threegram_search_var.loc[k,\"end_time\"] - df_threegram_search_var.loc[k,\"start_time\"]\n",
    "            df_link_default_var.loc[0,\"end_time\"] = df_link_default_var.loc[0,\"start_time\"] + threegram_time_diff_var+1.0\n",
    "            df_result = pd.concat([df_result,df_threegram_search_var.iloc[[k,]]], axis=0)\n",
    "            df_result = pd.concat([df_result,df_link_default_var], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # sentence added\n",
    "    df_sent_search_var = df_sentence_link[df_sentence_link[\"search_string\"] == sent]\n",
    "    df_sent_search_var.reset_index(drop=True, inplace=True)\n",
    "    for l in range(len(df_sent_search_var)):\n",
    "        df_link_default_var = df_link_default\n",
    "        try:\n",
    "            sent_time_diff_var = df_sent_search_var.loc[l,\"end_time\"] - df_sent_search_var.loc[l,\"start_time\"]\n",
    "            df_link_default_var.loc[0,\"end_time\"] = df_link_default_var.loc[0,\"start_time\"] + sent_time_diff_var+1.0\n",
    "            df_result = pd.concat([df_result,df_sent_search_var.iloc[[l,]]], axis=0)\n",
    "            df_result = pd.concat([df_result,df_link_default_var], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "df_result.reset_index(drop=True, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[df_result[\"search_string\"] == \"repeat\"][\"end_time\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = df_result[df_result[\"search_string\"] == \"repeat\"][\"end_time\"].count()\n",
    "sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df_result[df_result[\"search_string\"] == \"repeat\"][\"end_time\"].sum()*2)-sample_num)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_excel(\"Turkish_200_Word_Talk_Time1_Test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_select = df_result.head(84)\n",
    "df_result_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "start_list = []\n",
    "end_list = []\n",
    "for id, start, end in zip(df_result_select[\"video_id\"].to_list(),df_result_select[\"start_time\"].to_list(),df_result_select[\"end_time\"].to_list()):\n",
    "    id_list.append(str(id))\n",
    "    start_list.append(str(start))\n",
    "    end_list.append(str(end))\n",
    "\n",
    "id_join = \",\".join(id_list)\n",
    "start_join = \",\".join(start_list)\n",
    "end_join = \",\".join(end_list)\n",
    "\n",
    "df_result_for_embedded = pd.DataFrame(data=[[id_join,start_join,end_join]], columns=[\"id\",\"start_time\",\"end_time\"])\n",
    "df_result_for_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_for_embedded.to_excel(\"Turkish_200_Word_Talk_Time1_Join_Test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_*_Word_Talk_Time*.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in output_file:\n",
    "    source = y # source directory\n",
    "    destination = path\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in output_file:\n",
    "    try:\n",
    "        os.remove(z)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for one search result\n",
    "df_result = pd.DataFrame()\n",
    "for sent in df_sentence_link[\"search_string\"]:\n",
    "    sent_words = word_tokenize(sent)\n",
    "    for word in sent_words:\n",
    "        df_link_default_var = df_link_default\n",
    "        df_var = df_word_link[df_word_link[\"search_string\"] == word]\n",
    "        df_var.reset_index(drop=True, inplace=True)\n",
    "        try:\n",
    "            var_time_diff = (df_var.loc[0,\"end_time\"] - df_var.loc[0,\"start_time\"])\n",
    "            df_link_default_var.loc[0,\"end_time\"] = df_link_default_var.loc[0,\"start_time\"] + var_time_diff+1.0\n",
    "            df_result = pd.concat([df_result,df_var], axis=0)\n",
    "            df_result = pd.concat([df_result,df_link_default_var], axis=0)\n",
    "        except:\n",
    "            pass\n",
    "df_result.reset_index(drop=True, inplace=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
