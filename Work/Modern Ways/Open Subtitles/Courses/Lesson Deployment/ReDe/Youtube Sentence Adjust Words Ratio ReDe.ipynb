{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Sentence Adjust Word List Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install modin[ray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ray  # ayrı olarak çalıştırılmalı\n",
    "#ray.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"Turkish\"\n",
    "word_num = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sentence Ratio By Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_adjust_word = pd.read_excel(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/{folder_name.lower().capitalize()}/Deployment/Result/Level 1/Opus/Word_{word_num}.xlsx\")\n",
    "#df_adjust_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "df_word_prefix = pd.read_excel(\"Turkish_English_Native_And_Shared_Word_Prefix_Suffix_Custom_Concat.xlsx\")\n",
    "df_word_prefix.drop_duplicates(inplace=True)\n",
    "df_adjust_word = df_word_prefix\n",
    "df_adjust_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{folder_name.lower().capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "#df_youtube_sentence = df_youtube_sentence.head(40000)\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def sentence_word_ratio(df_word, df_sentence, word_column_name, sentence_column_name, word_num): # df_word and df_sentence are dataframe, word_column_name and sentence_column_name are string, word_num is an integer\n",
    "#    word_set = set(df_word[f\"{word_column_name}\"].to_list())\n",
    "#    for i in range(len(df_sentence)):\n",
    "#        sentence = df_sentence.loc[i,f\"{sentence_column_name}\"]\n",
    "#        sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "#        sent_word_set = set(sent_word)\n",
    "#        intersect_word = word_set.intersection(sent_word_set)\n",
    "#        different_word = sent_word_set.difference(word_set)\n",
    "#        df_sentence.loc[i,f\"{word_num}_word_ratio\"] = (len(intersect_word)/len(sent_word)+0.001)*100\n",
    "#        df_sentence.loc[i,\"different_word\"] = [different_word]\n",
    "#        df_sentence.loc[i,\"intersect_word\"] = [intersect_word]\n",
    "#\n",
    "#    df_sentence.reset_index(inplace=True)    \n",
    "#    return df_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_word_ratio(df_word, df_sentence, word_column_name, sentence_column_name, word_num): \n",
    "    '''\n",
    "    df_word and df_sentence are dataframe, word_column_name and sentence_column_name are string, word_num is an integer \\n\n",
    "    sentence_word_ratio(df_adjust_word, df_youtube_sentence, \"word\", \"sentence\", 6360)\n",
    "    '''\n",
    "    word_set = set(df_word[f\"{word_column_name}\"].to_list())\n",
    "    for i in range(len(df_sentence)):\n",
    "        sentence = df_sentence.loc[i,f\"{sentence_column_name}\"]\n",
    "        sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "        sent_word_set = set(sent_word)\n",
    "        intersect_word = list(word_set.intersection(sent_word_set))\n",
    "        different_word = list(sent_word_set.difference(word_set))\n",
    "        df_sentence.loc[i,f\"{word_num}_word_ratio\"] = (len(intersect_word)/len(sent_word)+0.001)*100\n",
    "        df_sentence.loc[i,\"different_word\"] = \", \".join(different_word)\n",
    "        df_sentence.loc[i,\"intersect_word\"] = \", \".join(intersect_word)\n",
    "\n",
    "    df_sentence.reset_index(inplace=True)    \n",
    "    return df_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result = sentence_word_ratio(df_adjust_word, df_youtube_sentence, \"word\", \"sentence\", 6360)\n",
    "df_sentence_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result[\"different_word\"] = df_sentence_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result[\"intersect_word\"] = df_sentence_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result.to_csv(f\"Youtube_Sentence_Word_Ratio_Result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentence_ratio_result.to_excel(f\"Youtube_Sentence_{word_num}_Word_Ratio_Result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Videoid Ratio By Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_adjust_word = pd.read_excel(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/{folder_name.lower().capitalize()}/Deployment/Result/Level 1/Opus/Word_{word_num}.xlsx\")\n",
    "#df_adjust_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{folder_name.lower().capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "#df_youtube_sentence = df_youtube_sentence.head(5000)\n",
    "#df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_youtube_sentence.groupby(\"video_id\")[\"sentence\"].apply(\" \".join).reset_index()\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoid_word_ratio(df_word, df_videoid_sentence, word_column_name, videoid_sentence_column_name, word_num):\n",
    "    word_set = set(df_word[f\"{word_column_name}\"].to_list())\n",
    "    for i in range(len(df_videoid_sentence)):\n",
    "        text = df_videoid_sentence.loc[i,f\"{videoid_sentence_column_name}\"]\n",
    "        text_word = re.findall(r\"\\w+\", text, re.UNICODE)\n",
    "        text_word_set = set(text_word)\n",
    "        intersect_word = list(word_set.intersection(text_word_set))\n",
    "        different_word = list(text_word_set.difference(word_set))\n",
    "        df_videoid_sentence.loc[i,f\"{word_num}_word_ratio\"] = (len(intersect_word)/len(text_word)+0.001)*100\n",
    "        df_videoid_sentence.loc[i,\"different_word\"] = \", \".join(different_word)\n",
    "        df_videoid_sentence.loc[i,\"intersect_word\"] = \", \".join(intersect_word)\n",
    "        \n",
    "    return df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result = videoid_word_ratio(df_adjust_word, df_videoid_sentence, \"word\", \"sentence\", 6360)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result.to_csv(f\"Youtube_Videoid_Text_Word_Ratio_Result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text_ratio_result.to_excel(f\"Youtube_Videoid_Text_{word_num}_Word_Ratio_Result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Adjust Sentence Ratio Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sentence_ratio_result = pd.read_excel(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/{folder_name}/Deployment/Result/Level 1/Youtube/Youtube_Sentence_28_Word_Ratio_Result.xlsx\")\n",
    "#df_sentence_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_word_ratio = 100\n",
    "df_adjust_ratio = df_sentence_ratio_result[df_sentence_ratio_result[f\"{word_num}_word_ratio\"] >= adjust_word_ratio]\n",
    "df_adjust_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_adjust_ratio.to_excel(f\"Youtube_Sentence_{word_num}_Word_{word_ratio}_Ratio_Sentences.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sequential Sentence Length Of Adjust Ratio Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_ratio[\"with_word\"] = 1\n",
    "df_adjust_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_ratio_flag = pd.merge(df_youtube_sentence, df_adjust_ratio, how=\"left\", on=[\"index\",\"sentence\",\"start_time\",\"end_time\",\"video_id\",f\"{word_num}_word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_adjust_ratio_flag[\"with_word\"].fillna(0, inplace=True)\n",
    "df_adjust_ratio_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_ratio_flag.with_word.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Running For Video_id Sentence\n",
    "zeros = []\n",
    "ones = []\n",
    "zero_list = []\n",
    "one_list = []\n",
    "for i in range(len(df_adjust_ratio_flag)):\n",
    "    videoid = df_adjust_ratio_flag.loc[i,\"video_id\"]    \n",
    "    if df_adjust_ratio_flag.loc[i,\"with_word\"] == 0:\n",
    "        zeros.append(i)\n",
    "        try:\n",
    "            one_list.append((ones[0],len(ones)))\n",
    "        except:\n",
    "            pass\n",
    "        ones = []\n",
    "    else:\n",
    "        ones.append(i)\n",
    "        try:\n",
    "            zero_list.append((zeros[0],len(zeros)))\n",
    "        except:\n",
    "            pass\n",
    "        zeros = []\n",
    "\n",
    "    try:\n",
    "        if df_adjust_ratio_flag.loc[i+1,\"video_id\"] != videoid:\n",
    "            try:\n",
    "                one_list.append((ones[0],len(ones)))\n",
    "                ones = []\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                zero_list.append((zeros[0],len(zeros)))\n",
    "                zeros = []\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    one_list.append((ones[0],len(ones)))\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    zero_list.append((zeros[0],len(zeros)))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = pd.DataFrame(one_list)\n",
    "df_one.rename(columns={0:\"index\",1:\"length\"}, inplace=True)\n",
    "df_one.drop_duplicates(inplace=True)\n",
    "df_one.sort_values(by=\"length\", ascending=False, inplace=True)\n",
    "df_one.reset_index(drop=True, inplace=True)\n",
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_all = pd.DataFrame()\n",
    "for i in range(len(df_one)):\n",
    "    index = df_one.loc[i,\"index\"]\n",
    "    length = df_one.loc[i,\"length\"]\n",
    "    start_index = index\n",
    "    end_index = index+length\n",
    "    df_video = df_adjust_ratio_flag.iloc[start_index:end_index,]\n",
    "    df_one_all = pd.concat([df_one_all, df_video],axis=0)\n",
    "df_one_all.reset_index(inplace=True, drop=True)\n",
    "df_one_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_one_all.to_excel(f\"Youtube_Sentence_{word_num}_Word_{word_ratio}_Ratio_Sequential_Sentences_And_Others.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Sequential Sentences\n",
    "df_one_sequential = df_one[df_one.length > 1]\n",
    "df_one_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_sequential_all = pd.DataFrame()\n",
    "for i in range(len(df_one_sequential)):\n",
    "    index = df_one_sequential.loc[i,\"index\"]\n",
    "    length = df_one_sequential.loc[i,\"length\"]\n",
    "    start_index = index\n",
    "    end_index = index+length\n",
    "    df_video = df_adjust_ratio_flag.iloc[start_index:end_index,]\n",
    "    df_one_sequential_all = pd.concat([df_one_sequential_all, df_video],axis=0)\n",
    "df_one_sequential_all.reset_index(inplace=True, drop=True)\n",
    "df_one_sequential_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_one_sequential_all.to_excel(f\"Youtube_Sentence_{word_num}_Word_{word_ratio}_Ratio_Only_Sequential_Sentences.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Sentence Words By Adjust Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_one_sequential_all = pd.read_excel(\"Youtube_Sentence_{word_num}_Word_{word_ratio}_Ratio_Only_Sequential_Sentences.xlsx\")\n",
    "#df_one_sequential_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list_df(df_target, target_column): # df_target include dict value columns, target_column occurs dict\n",
    "    text_value_list = []\n",
    "    for i in df_target[f\"{target_column}\"]:\n",
    "        try:\n",
    "            var = re.findall(r\"\\w+\", i)\n",
    "            for j in var:\n",
    "                text_value_list.append(j)\n",
    "        except:\n",
    "            pass\n",
    "    df_alone_value = pd.DataFrame(text_value_list)\n",
    "    df_alone_value.rename(columns={0:f\"{target_column}_alone\"}, inplace=True)\n",
    "    df_unique_count = pd.DataFrame(df_alone_value.value_counts(ascending=False))\n",
    "    df_unique_count.reset_index(inplace=True)\n",
    "    df_unique_count.rename(columns={0:\"count\"}, inplace=True)\n",
    "            \n",
    "    return df_unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_word_count = text_to_list_df(df_one_sequential_all, \"intersect_word\")\n",
    "df_unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_unique_word_count.to_excel(f\"Youtube_Sentence_{word_num}_Word_{word_ratio}_Ratio_Sequential_Sentence_Words_Count.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Adjust Sentence Ratio Result By Videoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratio_count = pd.DataFrame(df_adjust_ratio.groupby(\"video_id\")[\"28_word_ratio\"].count())\n",
    "df_ratio_count.reset_index(inplace=True)\n",
    "df_ratio_count.rename(columns={\"28_word_ratio\":\"28_word_ratio_count\"}, inplace=True)\n",
    "df_ratio_count.sort_values(by=\"28_word_ratio_count\", ascending=False, inplace=True)\n",
    "df_ratio_count.reset_index(drop=True,inplace=True)\n",
    "df_ratio_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ratio_count.to_excel(f\"Youtube_Sentence_{word_num}_Word_{word_ratio}_Ratio_Videoid_Count.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"Turkish\"\n",
    "word_num = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_word = pd.read_excel(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/{folder_name.lower().capitalize()}/Deployment/Result/Level 1/Opus/Word_{word_num}.xlsx\")\n",
    "df_adjust_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{folder_name.lower().capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence = df_youtube_sentence.head(40000)\n",
    "df_youtube_sentence.reset_index(inplace=True)\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list = manager.list()\n",
    "word_set = set(df_adjust_word[\"word\"].to_list())\n",
    "index_num = list(range(len(df_youtube_sentence)))\n",
    "\n",
    "def sentence_word_ratio(index_num):\n",
    "    index = df_youtube_sentence.loc[index_num,\"index\"]\n",
    "    start = df_youtube_sentence.loc[index_num,\"start_time\"]\n",
    "    end = df_youtube_sentence.loc[index_num,\"end_time\"]\n",
    "    sentence = df_youtube_sentence.loc[index_num,\"sentence\"]\n",
    "    videoid = df_youtube_sentence.loc[index_num,\"video_id\"]\n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = (len(intersect_word)/len(sent_word)+0.001)*100\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list.append([index,start,end,sentence,videoid,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(sentence_word_ratio, index_num) # zip_list fonksiyon için çoklu eleman gerekli olduğu için kullanıldı.\n",
    "# Not: starmap çoklu iterable değerlere ihtiyaç duyan fonksiyonlar için kullanılıyor ve bu değerler zip veya itertools şeklinde olmalı. O yüzden burada pool.map yerine pool.starmap kullanıldı. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result = pd.DataFrame(list(result_list), columns=[\"index\",\"start_time\",\"end_time\",\"sentence\",\"video_id\",f\"{word_num}_word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_sentence_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_sentence_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_sentence_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result[\"different_word\"] = df_sentence_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result[\"intersect_word\"] = df_sentence_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Word In Sentence Time Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"Turkish\"\n",
    "word_num = 206\n",
    "level = \"level 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_count_low = pd.read_excel(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/{folder_name.lower().capitalize()}/Deployment/Result/{level.lower().capitalize()}/3-Lesson Level Result/Lesson_{level.lower().capitalize()}_Low_Count_Words.xlsx\")\n",
    "df_word_count_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent = pd.read_csv(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Youtube/Result/Turkish/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent['start_time'] = pd.to_timedelta(df_sent['start_time']) # data type converted timedelta for second \n",
    "df_sent['end_time'] = pd.to_timedelta(df_sent['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent['start_time'] = df_sent['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_sent['end_time'] = df_sent['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = df_word_count_low.iloc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_result = pd.DataFrame()\n",
    "for i in word_list:\n",
    "    try:\n",
    "        word_in_video = df_sent[df_sent.sentence.str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].sample(1)\n",
    "    except:\n",
    "        word_in_video = df_sent[df_sent.sentence.str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(1) \n",
    "    word_in_video.insert(0,\"word\",i)\n",
    "    df_word_result = pd.concat([df_word_result,word_in_video], axis=0)\n",
    "df_word_result.reset_index(drop=True, inplace=True)\n",
    "df_word_result = df_word_result.sort_values(by=[\"video_id\",\"start_time\"], ascending=True)\n",
    "df_word_result.reset_index(drop=True, inplace=True)\n",
    "df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_time_loc_list = []\n",
    "def word_time_loc(df):\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,\"word\"]\n",
    "        start_time = df.loc[i,\"start_time\"]\n",
    "        end_time = df.loc[i,\"end_time\"]\n",
    "        sentence = df.loc[i,\"sentence\"]\n",
    "        video_id = df.loc[i,\"video_id\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[\"word\",\"start_time\",\"end_time\",\"sentence\",\"video_id\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_time_loc(df_word_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"güneş gökyüzünde pırıl pırıl parlıyordu. Birden gökyüzünde bulutlar belirdi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_list = []\n",
    "for i in re.finditer(fr\"(?:\\s|^)gökyüzünde(?:\\s|$)\", test_text, re.IGNORECASE|re.UNICODE):\n",
    "    loc_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_list[0].start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_list[0].end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_length_ratio = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String In Word Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_in_wordgroup(df, list_value, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, list_column and target_column are \n",
    "       dataframe column string name. list_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_search_result = pd.DataFrame()\n",
    "    #for i in df[f\"{list_column}\"].dropna():\n",
    "    for i in list_value:\n",
    "        try:\n",
    "            #word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(6)\n",
    "            string_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"{i}\", na=True)].head(6) \n",
    "        except:\n",
    "            pass        \n",
    "        string_in_word_cluster.insert(0,\"search_value\",i)\n",
    "        df_search_result = pd.concat([df_search_result,string_in_word_cluster], axis=0)\n",
    "    df_search_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opus_sent = pd.read_csv(\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Turkish/Result/Sentence/Merge/Sentence_Merge.csv\")\n",
    "df_opus_sent = df_opus_sent[[\"sentence\",\"frequency\"]]\n",
    "df_opus_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threegram = pd.read_csv(\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Turkish/Result/N Gram/Merge/Threegram_Merge.csv\")\n",
    "df_threegram = df_threegram[[\"threegram\",\"frequency\"]]\n",
    "df_threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fourgram = pd.read_csv(\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Turkish/Result/N Gram/Merge/Fourgram_Merge.csv\")\n",
    "df_fourgram = df_fourgram[[\"fourgram\",\"frequency\"]]\n",
    "df_fourgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fivegram = pd.read_csv(\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Turkish/Result/N Gram/Merge/Fivegram_Merge.csv\")\n",
    "df_fivegram = df_fivegram[[\"fivegram\",\"frequency\"]]\n",
    "df_fivegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [\"numarayı kontrol\", \"biraz kontrolden\", \"sistem kontrolü\", \"bombanın kontrolünü\", \"oteli kontrol\", \"robot kontrol\", \"biraz kontrol\", \"makineleri kontrol\", \"motoru kontrol\", \"kontrol sistemi\", \"sistemleri kontrol\", \"telefonunu kontrol\", \"sistemi kontrol\", \"fazla kontrol\", \"rutin kontrol\", \"sistemini kontrol\", \"kalite kontrol\", \"bombayı kontrol\", \"metropolis kontrol\", \"asansörü kontrol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_sent = string_in_wordgroup(df_opus_sent, word_list, \"sentence\")\n",
    "df_result_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_sent = pd.merge(df_result_sent,df_opus_sent, how=\"left\", on=\"sentence\")\n",
    "df_merge_result_sent.drop_duplicates(inplace=True)\n",
    "df_merge_result_sent.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "df_merge_result_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_sent.groupby([\"search_value\"])[[\"frequency\"]].sum().sort_values(by=\"frequency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_three = string_in_wordgroup(df_threegram, word_list, \"threegram\")\n",
    "df_result_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_three = pd.merge(df_result_three,df_threegram, how=\"left\", on=\"threegram\")\n",
    "df_merge_result_three.drop_duplicates(inplace=True)\n",
    "df_merge_result_three.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "df_merge_result_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_three.groupby([\"search_value\"])[[\"frequency\"]].sum().sort_values(by=\"frequency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_four = string_in_wordgroup(df_fourgram, word_list, \"fourgram\")\n",
    "df_result_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_four = pd.merge(df_result_four,df_fourgram, how=\"left\", on=\"fourgram\")\n",
    "df_merge_result_four.drop_duplicates(inplace=True)\n",
    "df_merge_result_four.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "df_merge_result_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_four.groupby([\"search_value\"])[[\"frequency\"]].sum().sort_values(by=\"frequency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_five = string_in_wordgroup(df_fivegram, word_list, \"fivegram\")\n",
    "df_result_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_five = pd.merge(df_result_five,df_fivegram, how=\"left\", on=\"fivegram\")\n",
    "df_merge_result_five.drop_duplicates(inplace=True)\n",
    "df_merge_result_five.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "df_merge_result_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_result_five.groupby([\"search_value\"])[[\"frequency\"]].sum().sort_values(by=\"frequency\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Search In Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sent = pd.read_csv(f\"/media/kurubal/SSD1/Data Scientist/Work/Modern Ways/Project/Youtube/Result/Turkish/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sent['start_time'] = pd.to_timedelta(df_youtube_sent['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sent['end_time'] = pd.to_timedelta(df_youtube_sent['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sent['start_time'] = df_youtube_sent['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sent['end_time'] = df_youtube_sent['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_youtube_sent[df_youtube_sent.sentence.str.contains(fr\"(?:\\s|^)bir polis(?:\\s|$)\", na=True)]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"sentence\",key=lambda x:x.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english_select = pd.read_excel(\"Turkish English Manual Selected 2 Gram Hybrids.xlsx\", sheet_name= \"2 gram target\")\n",
    "df_english_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english_select.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search_result = pd.DataFrame()\n",
    "for i in range(len(df_english_select)):\n",
    "    for j in df_english_select.columns[1:]:\n",
    "        string = df_english_select.loc[i,j]\n",
    "        df_result = df_youtube_sent[df_youtube_sent.sentence.str.contains(fr\"(?:\\s|^){string}(?:\\s|$)\", na=True)]\n",
    "        df_result.sort_values(\"sentence\",key=lambda x:x.str.len(), inplace=True)\n",
    "        df_select = df_result.head(6)\n",
    "        df_select.insert(0,\"search_string\",string)\n",
    "        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_search_result.to_excel(\"Youtube_Search_Result_Test2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_search_result[\"concat\"] = df_search_result['start_time'].map(str)+\", \"+df_search_result['end_time'].map(str)+\", \"+df_search_result['video_id'].map(str)\n",
    "df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_result = pd.DataFrame()\n",
    "for i in word_list:\n",
    "    try:\n",
    "        word_in_video = df_sent[df_sent.sentence.str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)]\n",
    "    except:\n",
    "        pass\n",
    "    word_in_video.insert(0,\"word\",i)\n",
    "    df_word_result = pd.concat([df_word_result,word_in_video], axis=0)\n",
    "df_word_result.reset_index(drop=True, inplace=True)\n",
    "df_word_result = df_word_result.sort_values(by=[\"video_id\",\"start_time\"], ascending=True)\n",
    "df_word_result.reset_index(drop=True, inplace=True)\n",
    "df_word_result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
