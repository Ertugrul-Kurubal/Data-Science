{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_folder = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language\n",
    "lang_pair = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repetition(word_group):\n",
    "    words = word_tokenize(word_group)\n",
    "    word_unique = set(words)\n",
    "    if len(word_unique) == 1:\n",
    "        return \"repetitive_word_group\"\n",
    "    else:\n",
    "        return word_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/English/English_And_Turkish (tur)_Etymologeek_Result_All.xlsx']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_pair_list = glob.glob(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()}_And_{lang_pair.lower().capitalize()}*_All.xlsx\")\n",
    "lang_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_entry_main</th>\n",
       "      <th>turkish_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>school</td>\n",
       "      <td>okul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>police</td>\n",
       "      <td>polis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cop</td>\n",
       "      <td>polis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>policeman</td>\n",
       "      <td>polis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>policewoman</td>\n",
       "      <td>polis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>skeletal</td>\n",
       "      <td>iskelet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>masa</td>\n",
       "      <td>masa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>calorie</td>\n",
       "      <td>kalori</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>kismet</td>\n",
       "      <td>kısmet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>microbe</td>\n",
       "      <td>mikrop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    dict_entry_main turkish_word\n",
       "0            school         okul\n",
       "1            police        polis\n",
       "2               cop        polis\n",
       "3         policeman        polis\n",
       "4       policewoman        polis\n",
       "..              ...          ...\n",
       "238        skeletal      iskelet\n",
       "239            masa         masa\n",
       "240         calorie       kalori\n",
       "241          kismet       kısmet\n",
       "242         microbe       mikrop\n",
       "\n",
       "[243 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pair = pd.read_excel(f\"{lang_pair_list[0]}\")\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df_pair[\"dict_entry_main\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_set = set(disable_list)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(words_set.difference(disable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 way  Not: 1 way is long and in Shared Word ReDe part \n",
    "data_kind_list = [\"Two\", \"Three\", \"Four\", \"Five\", \"Sentence\"]\n",
    "for i in data_kind_list:\n",
    "    sent_ngram_file = f\"{i}\" # Two, Three, Four, Five, Sentence\n",
    " \n",
    "    if sent_ngram_file.lower() != \"sentence\":\n",
    "        data_kind = f\"{sent_ngram_file}gram\"  # Twogram, Threegram, Fourgram, Fivegram\n",
    "        folder_path = \"N Gram\"\n",
    "    else:\n",
    "        data_kind = \"sentence\"\n",
    "        folder_path = \"Sentence\"\n",
    "\n",
    "    df = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/{folder_path.capitalize()}/Merge/{data_kind.capitalize()}_Merge.csv\")\n",
    "    d_list  = df.iloc[:,0].values.tolist()\n",
    "\n",
    "    resultlist = []\n",
    "    manager = multiprocessing.Manager()\n",
    "    resultlist = manager.list()\n",
    "    \n",
    "    def word_in_wordgroup(d_list):\n",
    "        mergelist = []\n",
    "        try:\n",
    "            word = d_list.split()\n",
    "        except:\n",
    "            pass\n",
    "        var1 = range(len(word))\n",
    "        for j in var1:\n",
    "            if word[j] in word_list:\n",
    "                mergelist.append(word[j])\n",
    "                if len(mergelist) == len(word):\n",
    "                        resultlist.append(d_list)\n",
    "                            \n",
    "    if __name__ == '__main__':\n",
    "        # with Pool(16) as p:\n",
    "        with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "            p.map(word_in_wordgroup, d_list) # string_word liste\n",
    "\n",
    "    result_list = list(resultlist)\n",
    "    df_result = pd.DataFrame(result_list, columns=[0])  # add columns parameter for empty result\n",
    "    df_result = df_result.rename(columns = {0: f\"{data_kind.lower()}\"})\n",
    "    df_merge = pd.merge(df_result, df, how=\"left\", on=f\"{data_kind.lower()}\")\n",
    "    df_merge_result = df_merge.sort_values(by=\"frequency\", ascending=False)\n",
    "    df_merge_result.drop_duplicates(inplace=True)\n",
    "    df_merge_result[f\"{data_kind.lower()}\"] = df_merge_result[f\"{data_kind.lower()}\"].apply(lambda x: remove_repetition(x))  # added remove_repetition \n",
    "    df_merge_result = df_merge_result[df_merge_result[f\"{data_kind.lower()}\"] != \"repetitive_word_group\"]  # remove_repetition part\n",
    "    df_merge_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_merge_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_{data_kind.capitalize()}.xlsx\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_twogram = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Twogram.xlsx\")\n",
    "df_shared_twogram = df_shared_twogram.loc[:,[\"twogram\",\"frequency\"]]\n",
    "df_shared_twogram.rename(columns={\"frequency\":\"freq_twogram\"}, inplace=True)\n",
    "df_shared_twogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_threegram = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Threegram.xlsx\")\n",
    "df_shared_threegram = df_shared_threegram.loc[:,[\"threegram\",\"frequency\"]]\n",
    "df_shared_threegram.rename(columns={\"frequency\":\"freq_threegram\"}, inplace=True)\n",
    "df_shared_threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_fourgram = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Fourgram.xlsx\")\n",
    "df_shared_fourgram = df_shared_fourgram.loc[:,[\"fourgram\",\"frequency\"]]\n",
    "df_shared_fourgram.rename(columns={\"frequency\":\"freq_fourgram\"}, inplace=True)\n",
    "df_shared_fourgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_fivegram = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Fivegram.xlsx\")\n",
    "df_shared_fivegram = df_shared_fivegram.loc[:,[\"fivegram\",\"frequency\"]]\n",
    "df_shared_fivegram.rename(columns={\"frequency\":\"freq_fivegram\"}, inplace=True)\n",
    "df_shared_fivegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_sentence = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Sentence.xlsx\")\n",
    "df_shared_sentence = df_shared_sentence.loc[:,[\"sentence\",\"frequency\"]]\n",
    "df_shared_sentence.rename(columns={\"frequency\":\"freq_sentence\"}, inplace=True)\n",
    "df_shared_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_all = pd.concat([df_word, df_shared_twogram, df_shared_threegram, df_shared_fourgram, df_shared_fivegram, df_shared_sentence], axis=1)\n",
    "df_shared_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_twogram(df, list_column, target_column):\n",
    "\n",
    "    '''word_in_twogram(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, list_column and target_column are \n",
    "       dataframe column string name. list_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{list_column}\"].dropna():\n",
    "        try:\n",
    "            word_in_twogram = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)] \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_twogram.insert(0,\"word\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_twogram], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_twogram(df_shared_all, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_twogram(df_shared_all, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_twogram(df_shared_all, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_twogram(df_shared_all, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_twogram(df_shared_all, \"word\", \"sentence\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"word\"])[\"sentence\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
