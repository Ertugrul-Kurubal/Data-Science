{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Word File Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Spanish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # always must be False in this part\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = True  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twogram In Threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}.xlsx\")\n",
    "#df_shared_file = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\")\n",
    "df_shared_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup(df, list_column, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, list_column and target_column are \n",
    "       dataframe column string name. list_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{list_column}\"].dropna():\n",
    "        try:\n",
    "            #word_in_twogram = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(10)  # Option\n",
    "            word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(100) \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{list_column}\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_word_cluster], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shared_count = word_count_result(df_shared_file,[\"threegram\"])\n",
    "#df_shared_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file[\"twogram\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_three = word_in_wordgroup(df_shared_file, \"twogram\", \"threegram\")\n",
    "df_two_in_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_three[\"twogram\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_select_twogram = df_shared_file.loc[:,[\"twogram\",\"freq_twogram\"]]\n",
    "df_shared_select_twogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_shared_twogram = set(df_shared_select_twogram[\"twogram\"])\n",
    "set_two_three = set(df_two_in_three[\"twogram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_in_threegram = pd.DataFrame(set_two_three, columns=[\"twogram\"])  # columns=[\"twogram_in_threegram\"]\n",
    "df_twogram_in_threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_in_threegram_freq = pd.merge(df_twogram_in_threegram, df_shared_select_twogram, how=\"left\", on=\"twogram\")\n",
    "df_twogram_in_threegram_freq.sort_values(by=\"freq_twogram\", ascending=False, inplace=True)\n",
    "df_twogram_in_threegram_freq.rename(columns={\"twogram\":\"twogram_in_threegram\",\"freq_twogram\":\"freq_two_in_three\"}, inplace=True)\n",
    "df_twogram_in_threegram_freq.reset_index(drop=True, inplace=True)\n",
    "df_twogram_in_threegram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_diff = pd.DataFrame(set_shared_twogram.difference(set_two_three), columns=[\"twogram\"])\n",
    "df_twogram_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_diff_freq = pd.merge(df_twogram_diff, df_shared_select_twogram, how=\"left\", on=\"twogram\")\n",
    "df_twogram_diff_freq.sort_values(by=\"freq_twogram\", ascending=False, inplace=True)\n",
    "df_twogram_diff_freq.reset_index(drop=True, inplace=True)\n",
    "df_twogram_diff_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_file[\"twogram\"] = df_twogram_diff_freq[\"twogram\"]\n",
    "df_shared_file[\"freq_twogram\"] = df_twogram_diff_freq[\"freq_twogram\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_twogram_process = pd.concat([df_shared_file,df_twogram_in_threegram_freq], axis=1)\n",
    "df_shared_twogram_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_twogram_process.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concat Result With Comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_wordgroup(df_shared_twogram_process, \"word\", \"sentence\")\n",
    "df_word_order_twogram_threegram = word_in_wordgroup(df_shared_twogram_process, \"word\", \"twogram_in_threegram\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"word\"])[\"sentence\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_twogram_threegram = df_word_order_twogram_threegram.groupby([\"word\"])[\"twogram_in_threegram\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence,df_word_order_join_twogram_threegram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option For Word Frequency\n",
    "if shared_word_frequency:\n",
    "    df_word_order_join_all = pd.merge(df_word_order_join_all,df_word_all, how=\"inner\", on=\"word\")\n",
    "    df_word_order_join_all.drop_duplicates(inplace=True)\n",
    "    df_word_order_join_all = df_word_order_join_all.loc[:,[\"word\",\"frequency\",\"twogram\",\"threegram\",\"fourgram\",\"fivegram\",\"sentence\",\"twogram_in_threegram\"]]\n",
    "    df_word_order_join_all.sort_values(by=\"frequency\", inplace=True, ascending=False)\n",
    "    df_word_order_join_all.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*{file_ext}2.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix Suffix Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Italian\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# word sample\n",
    "word_sample = True  # True, False\n",
    "word_sample_num = 20\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = True  # True, False  # always must be True in this part\n",
    "native_word = False # True for adding native word\n",
    "etymology_word = True  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup(df, source_column, target_column):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, source_column and target_column are \n",
    "       dataframe column string name. source_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{source_column}\"].dropna():\n",
    "        try:\n",
    "            if word_sample:\n",
    "                word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(word_sample_num)  # Option\n",
    "            else:\n",
    "                word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)] \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{source_column}\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_word_cluster], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_prefix_suffix = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "df_word_prefix_suffix = df_word_prefix_suffix[[\"search_word\",\"word\"]]\n",
    "df_word_prefix_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ety_prefix_suffix = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "df_ety_prefix_suffix = df_ety_prefix_suffix[[\"search_word\",\"word\"]]\n",
    "df_ety_prefix_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_ext == \"3\":\n",
    "    df_all_word = pd.concat([df_word_prefix_suffix,df_ety_prefix_suffix],axis=0)\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "elif file_ext == \"4\":\n",
    "    df_all_word = df_ety_prefix_suffix\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "elif file_ext == \"5\":\n",
    "    df_all_word = df_word_prefix_suffix\n",
    "    df_all_word.drop_duplicates(inplace=True)\n",
    "    df_all_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.search_word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/4-Shared Word/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency{file_ext}.xlsx\")\n",
    "#df_shared_all = pd.read_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Result_With_Frequency.xlsx\")\n",
    "df_shared_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = word_in_wordgroup(df_shared_all, \"word\", \"twogram\")\n",
    "df_word_order_threegram = word_in_wordgroup(df_shared_all, \"word\", \"threegram\") \n",
    "df_word_order_fourgram = word_in_wordgroup(df_shared_all, \"word\", \"fourgram\") \n",
    "df_word_order_fivegram = word_in_wordgroup(df_shared_all, \"word\", \"fivegram\")\n",
    "df_word_order_sentence = word_in_wordgroup(df_shared_all, \"word\", \"sentence\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_twogram = pd.merge(df_word_order_twogram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_threegram = pd.merge(df_word_order_threegram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_fourgram = pd.merge(df_word_order_fourgram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_fivegram = pd.merge(df_word_order_fivegram,df_all_word, how=\"inner\", on=\"word\")\n",
    "df_word_order_sentence = pd.merge(df_word_order_sentence,df_all_word, how=\"inner\", on=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_twogram = df_word_order_twogram.groupby([\"search_word\"])[\"twogram\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_threegram = df_word_order_threegram.groupby([\"search_word\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fourgram = df_word_order_fourgram.groupby([\"search_word\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_fivegram = df_word_order_fivegram.groupby([\"search_word\"])[\"fivegram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_sentence = df_word_order_sentence.groupby([\"search_word\"])[\"sentence\"].apply(\", \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_word_order_join_twogram,df_word_order_join_threegram,df_word_order_join_fourgram,df_word_order_join_fivegram,df_word_order_join_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all = reduce(lambda  left,right: pd.merge(left,right, on=['search_word'], how='outer'), dfs)  # left,right make left to right merge\n",
    "#df_word_order_join_all = reduce(lambda  right,left: pd.merge(left,right, on=['word'], how='outer'), dfs)  # right,left make right to left merge\n",
    "df_word_order_join_all.rename(columns={\"search_word\":\"word\"}, inplace=True)\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option For Word Frequency\n",
    "if shared_word_frequency:\n",
    "    df_word_order_join_all = pd.merge(df_word_order_join_all,df_word_all, how=\"left\", on=\"word\")\n",
    "    df_word_order_join_all.drop_duplicates(inplace=True)\n",
    "    df_word_order_join_all = df_word_order_join_all.loc[:,[\"word\",\"frequency\",\"twogram\",\"threegram\",\"fourgram\",\"fivegram\",\"sentence\"]]\n",
    "    df_word_order_join_all.sort_values(by=\"frequency\", inplace=True, ascending=False)\n",
    "    df_word_order_join_all.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "df_word_order_join_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.word.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}3.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file2 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*{file_ext}3.xlsx\")\n",
    "output_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file2:\n",
    "    source = l # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file2:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix Suffix Shared File Word Result Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## language pair (same previous part parameter)\n",
    "#lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "#lang_pair = \"French\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# word sample\n",
    "word_sample_num = 20\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_strip_func(x):\n",
    "    try:\n",
    "        var_low = x.lower()\n",
    "        var_out = var_low.strip()\n",
    "    except:\n",
    "        var_out = x\n",
    "    return var_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_process_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Result_Without_Frequency{file_ext}3.xlsx\")\n",
    "df_shared_process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all = df_word_all.loc[:,[\"word\",\"frequency\"]]\n",
    "df_word_all[\"word\"] = df_word_all[\"word\"].apply(lambda x: lower_strip_func(x))\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Twogram_Merge.csv\")\n",
    "df_twogram_all = df_twogram_all.loc[:,[\"twogram\",\"frequency\"]]\n",
    "df_twogram_all[\"twogram\"] = df_twogram_all[\"twogram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_twogram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threegram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Threegram_Merge.csv\")\n",
    "df_threegram_all = df_threegram_all.loc[:,[\"threegram\",\"frequency\"]]\n",
    "df_threegram_all[\"threegram\"] = df_threegram_all[\"threegram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_threegram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fourgram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fourgram_Merge.csv\")\n",
    "df_fourgram_all = df_fourgram_all.loc[:,[\"fourgram\",\"frequency\"]]\n",
    "df_fourgram_all[\"fourgram\"] = df_fourgram_all[\"fourgram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_fourgram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fivegram_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fivegram_Merge.csv\")\n",
    "df_fivegram_all = df_fivegram_all.loc[:,[\"fivegram\",\"frequency\"]]\n",
    "df_fivegram_all[\"fivegram\"] = df_fivegram_all[\"fivegram\"].apply(lambda x: lower_strip_func(x))\n",
    "df_fivegram_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_all = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/Sentence/Merge/Sentence_Merge.csv\")\n",
    "df_sentence_all = df_sentence_all.loc[:,[\"sentence\",\"frequency\"]]\n",
    "df_sentence_all[\"sentence\"] = df_sentence_all[\"sentence\"].apply(lambda x: lower_strip_func(x))\n",
    "df_sentence_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_shared_process_all)):\n",
    "    # column result\n",
    "    try:\n",
    "        # column result\n",
    "        df_two_var = pd.DataFrame(df_shared_process_all.loc[i,\"twogram\"].split(\", \"), columns=[\"twogram\"])\n",
    "        # merge with all\n",
    "        df_two_var_merge = pd.merge(df_two_var, df_twogram_all, how=\"left\", on=\"twogram\")\n",
    "        df_two_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_two_var_merge_select = df_two_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_two_var_list = df_two_var_merge_select[\"twogram\"].to_list()\n",
    "        # list join\n",
    "        df_two_var_list_join = \", \".join(df_two_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"twogram\"] = df_two_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_three_var = pd.DataFrame(df_shared_process_all.loc[i,\"threegram\"].split(\", \"), columns=[\"threegram\"])\n",
    "        # merge with all\n",
    "        df_three_var_merge = pd.merge(df_three_var, df_threegram_all, how=\"left\", on=\"threegram\")\n",
    "        df_three_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_three_var_merge_select = df_three_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_three_var_list = df_three_var_merge_select[\"threegram\"].to_list()\n",
    "        # list join\n",
    "        df_three_var_list_join = \", \".join(df_three_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"threegram\"] = df_three_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_four_var = pd.DataFrame(df_shared_process_all.loc[i,\"fourgram\"].split(\", \"), columns=[\"fourgram\"])\n",
    "        # merge with all\n",
    "        df_four_var_merge = pd.merge(df_four_var, df_fourgram_all, how=\"left\", on=\"fourgram\")\n",
    "        df_four_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_four_var_merge_select = df_four_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_four_var_list = df_four_var_merge_select[\"fourgram\"].to_list()\n",
    "        # list join\n",
    "        df_four_var_list_join = \", \".join(df_four_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"fourgram\"] = df_four_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_five_var = pd.DataFrame(df_shared_process_all.loc[i,\"fivegram\"].split(\", \"), columns=[\"fivegram\"])\n",
    "        # merge with all\n",
    "        df_five_var_merge = pd.merge(df_five_var, df_fivegram_all, how=\"left\", on=\"fivegram\")\n",
    "        df_five_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_five_var_merge_select = df_five_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_five_var_list = df_five_var_merge_select[\"fivegram\"].to_list()\n",
    "        # list join\n",
    "        df_five_var_list_join = \", \".join(df_five_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"fivegram\"] = df_five_var_list_join\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        # column result\n",
    "        df_sentence_var = pd.DataFrame(df_shared_process_all.loc[i,\"sentence\"].split(\", \"), columns=[\"sentence\"])\n",
    "        # merge with all\n",
    "        df_sentence_var_merge = pd.merge(df_sentence_var, df_sentence_all, how=\"left\", on=\"sentence\")\n",
    "        df_sentence_var_merge.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "        df_sentence_var_merge_select = df_sentence_var_merge.head(word_sample_num)\n",
    "        # to list value\n",
    "        df_sentence_var_list = df_sentence_var_merge_select[\"sentence\"].to_list()\n",
    "        # list join\n",
    "        df_sentence_var_list_join = \", \".join(df_sentence_var_list)\n",
    "        # change value\n",
    "        df_shared_process_all.loc[i,\"sentence\"] = df_sentence_var_list_join\n",
    "    except:\n",
    "        pass      \n",
    "\n",
    "df_shared_process_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_shared_process_all.sort_values(by=\"frequency\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shared_process_all.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Join_Select_Result_Without_Frequency{file_ext}4.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file3 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared*_Select_*{file_ext}4.xlsx\")\n",
    "output_file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file3:\n",
    "    source = l # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file3:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/Deploy Result Manuel\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, column_list is list value\n",
    "    '''\n",
    "    word_count_bool(df, column_list): df columns word count for word frequency\\n\n",
    "    df is dataframe, column_list is list value\\n\n",
    "    word_count_bool(df, [\"word\",\"twogram\"]):\n",
    "    '''\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list, columns=[\"word\"])\n",
    "    #df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid = pd.read_excel(\"Turkish English manual selected 2 gram hybrids 3.xlsx\", sheet_name=\"2 gram hybrid\")\n",
    "df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count = word_count_result(df_hybrid, [\"twogram_pair1\",\"twogram_pair2\",\"twogram_pair3\",\"twogram_pair4\"])\n",
    "df_hybrid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge = pd.merge(df_hybrid,df_hybrid_count,how=\"left\",on=\"word\")\n",
    "df_hybrid_count_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge2 = pd.merge(df_hybrid,df_hybrid_count,how=\"outer\",on=\"word\")\n",
    "df_hybrid_count_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_Hybrid_Word_Count.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count_merge.to_excel(writer, sheet_name='28_Hybrid_Word_Count', index=False)\n",
    "df_hybrid_count_merge2.to_excel(writer, sheet_name='All_Hybrid_Word_Count', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_excel(\"Turkish English manual selected 2 gram hybrids 3.xlsx\", sheet_name=\"2 gram target\")\n",
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count = word_count_result(df_target, [\"twogram_1\",\"twogram_2\",\"twogram_3\",\"twogram_4\"])\n",
    "df_target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge = pd.merge(df_target,df_target_count,how=\"left\",on=\"word\")\n",
    "df_target_count_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge2 = pd.merge(df_target,df_target_count,how=\"outer\",on=\"word\")\n",
    "df_target_count_merge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2 = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_Target_Word_Count.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count_merge.to_excel(writer2, sheet_name='28_Target_Word_Count', index=False)\n",
    "df_target_count_merge2.to_excel(writer2, sheet_name='All_Target_Word_Count', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Target Hybrid Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word = pd.concat([df_target_count, df_hybrid_count], axis=0)\n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.groupby(\"word\")[[\"word_count\"]].sum().reset_index(inplace=True)\n",
    "df_all_word.sort_values(by=\"word_count\", ascending=False, inplace=True)\n",
    "df_all_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_word.to_excel(f\"{lang_folder}_{lang_pair}_Target_Hybrid_Word_Count.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file4 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_*_Word_Count.xlsx\")\n",
    "output_file4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in output_file4:\n",
    "    source = o # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/5-Shared Word File/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in output_file4:\n",
    "    try:\n",
    "        os.remove(p)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Youtube Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# parameter\n",
    "sheets = \"2 gram target\"  # 2 gram target, 2 gram hybrids\n",
    "time_shift = 0.3\n",
    "sample_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:02:51.948</td>\n",
       "      <td>00:02:58.829</td>\n",
       "      <td>özgür bunlar normalde kamyon daha büyük araçla...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:03:00.956</td>\n",
       "      <td>00:03:04.236</td>\n",
       "      <td>burcu arka tarafı bağlamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:03:13.434</td>\n",
       "      <td>00:03:16.327</td>\n",
       "      <td>özgür arabaya yarım tur attıracağım</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:03:17.235</td>\n",
       "      <td>00:03:21.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:03:27.806</td>\n",
       "      <td>00:03:33.383</td>\n",
       "      <td>burcu şimdilik iki tekere takacağız ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036932</th>\n",
       "      <td>00:07:51.970</td>\n",
       "      <td>00:07:52.470</td>\n",
       "      <td>umarız ki bu büyük ve güçlü teknoloji yanlış e...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036933</th>\n",
       "      <td>00:07:52.470</td>\n",
       "      <td>00:08:02.304</td>\n",
       "      <td>daha faydalı ve özgün kurumlarda herkesin eşit...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036934</th>\n",
       "      <td>00:08:02.498</td>\n",
       "      <td>00:08:04.178</td>\n",
       "      <td>i zlediğiniz için teşekkürler</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036935</th>\n",
       "      <td>00:08:04.178</td>\n",
       "      <td>00:08:08.089</td>\n",
       "      <td>yararlandığım kaynakları açıklamada link olara...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036936</th>\n",
       "      <td>00:08:08.089</td>\n",
       "      <td>00:08:11.770</td>\n",
       "      <td>i leri düzey okuma ve araştırma yapmak isteyen...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036937 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time      end_time  \\\n",
       "0        00:02:51.948  00:02:58.829   \n",
       "1        00:03:00.956  00:03:04.236   \n",
       "2        00:03:13.434  00:03:16.327   \n",
       "3        00:03:17.235  00:03:21.338   \n",
       "4        00:03:27.806  00:03:33.383   \n",
       "...               ...           ...   \n",
       "3036932  00:07:51.970  00:07:52.470   \n",
       "3036933  00:07:52.470  00:08:02.304   \n",
       "3036934  00:08:02.498  00:08:04.178   \n",
       "3036935  00:08:04.178  00:08:08.089   \n",
       "3036936  00:08:08.089  00:08:11.770   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        özgür bunlar normalde kamyon daha büyük araçla...  8V9tq1pe8eI  \n",
       "1               burcu arka tarafı bağlamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      özgür arabaya yarım tur attıracağım  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu şimdilik iki tekere takacağız ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3036932  umarız ki bu büyük ve güçlü teknoloji yanlış e...  YFFJ5FyZj4Q  \n",
       "3036933  daha faydalı ve özgün kurumlarda herkesin eşit...  YFFJ5FyZj4Q  \n",
       "3036934                      i zlediğiniz için teşekkürler  YFFJ5FyZj4Q  \n",
       "3036935  yararlandığım kaynakları açıklamada link olara...  YFFJ5FyZj4Q  \n",
       "3036936  i leri düzey okuma ve araştırma yapmak isteyen...  YFFJ5FyZj4Q  \n",
       "\n",
       "[3036937 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.948</td>\n",
       "      <td>178.829</td>\n",
       "      <td>özgür bunlar normalde kamyon daha büyük araçla...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180.956</td>\n",
       "      <td>184.236</td>\n",
       "      <td>burcu arka tarafı bağlamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193.434</td>\n",
       "      <td>196.327</td>\n",
       "      <td>özgür arabaya yarım tur attıracağım</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>197.235</td>\n",
       "      <td>201.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207.806</td>\n",
       "      <td>213.383</td>\n",
       "      <td>burcu şimdilik iki tekere takacağız ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036932</th>\n",
       "      <td>471.970</td>\n",
       "      <td>472.470</td>\n",
       "      <td>umarız ki bu büyük ve güçlü teknoloji yanlış e...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036933</th>\n",
       "      <td>472.470</td>\n",
       "      <td>482.304</td>\n",
       "      <td>daha faydalı ve özgün kurumlarda herkesin eşit...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036934</th>\n",
       "      <td>482.498</td>\n",
       "      <td>484.178</td>\n",
       "      <td>i zlediğiniz için teşekkürler</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036935</th>\n",
       "      <td>484.178</td>\n",
       "      <td>488.089</td>\n",
       "      <td>yararlandığım kaynakları açıklamada link olara...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036936</th>\n",
       "      <td>488.089</td>\n",
       "      <td>491.770</td>\n",
       "      <td>i leri düzey okuma ve araştırma yapmak isteyen...</td>\n",
       "      <td>YFFJ5FyZj4Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3036937 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         start_time  end_time  \\\n",
       "0           171.948   178.829   \n",
       "1           180.956   184.236   \n",
       "2           193.434   196.327   \n",
       "3           197.235   201.338   \n",
       "4           207.806   213.383   \n",
       "...             ...       ...   \n",
       "3036932     471.970   472.470   \n",
       "3036933     472.470   482.304   \n",
       "3036934     482.498   484.178   \n",
       "3036935     484.178   488.089   \n",
       "3036936     488.089   491.770   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        özgür bunlar normalde kamyon daha büyük araçla...  8V9tq1pe8eI  \n",
       "1               burcu arka tarafı bağlamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      özgür arabaya yarım tur attıracağım  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu şimdilik iki tekere takacağız ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3036932  umarız ki bu büyük ve güçlü teknoloji yanlış e...  YFFJ5FyZj4Q  \n",
       "3036933  daha faydalı ve özgün kurumlarda herkesin eşit...  YFFJ5FyZj4Q  \n",
       "3036934                      i zlediğiniz için teşekkürler  YFFJ5FyZj4Q  \n",
       "3036935  yararlandığım kaynakları açıklamada link olara...  YFFJ5FyZj4Q  \n",
       "3036936  i leri düzey okuma ve araştırma yapmak isteyen...  YFFJ5FyZj4Q  \n",
       "\n",
       "[3036937 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>twogram_1</th>\n",
       "      <th>twogram_2</th>\n",
       "      <th>twogram_3</th>\n",
       "      <th>twogram_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>bir şey</td>\n",
       "      <td>bir daha</td>\n",
       "      <td>bir de</td>\n",
       "      <td>bir çok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bu</td>\n",
       "      <td>bu kadar</td>\n",
       "      <td>bu ne</td>\n",
       "      <td>bu çok</td>\n",
       "      <td>bu değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ne</td>\n",
       "      <td>ne kadar</td>\n",
       "      <td>ne için</td>\n",
       "      <td>bana ne</td>\n",
       "      <td>ne gibi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ve</td>\n",
       "      <td>ve de</td>\n",
       "      <td>ve evet</td>\n",
       "      <td>ve ne</td>\n",
       "      <td>ve sen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>için</td>\n",
       "      <td>ne için</td>\n",
       "      <td>benim için</td>\n",
       "      <td>senin için</td>\n",
       "      <td>onun için</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mi</td>\n",
       "      <td>değil mi</td>\n",
       "      <td>ben mi</td>\n",
       "      <td>sen mi</td>\n",
       "      <td>evet mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>o</td>\n",
       "      <td>o kadar</td>\n",
       "      <td>o da</td>\n",
       "      <td>o değil</td>\n",
       "      <td>evet o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ben</td>\n",
       "      <td>ben de</td>\n",
       "      <td>ben değil</td>\n",
       "      <td>hayır ben</td>\n",
       "      <td>ben mi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>de</td>\n",
       "      <td>sen de</td>\n",
       "      <td>bir de</td>\n",
       "      <td>beni de</td>\n",
       "      <td>beni de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>çok</td>\n",
       "      <td>bu çok</td>\n",
       "      <td>daha çok</td>\n",
       "      <td>çok var</td>\n",
       "      <td>çok şey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ama</td>\n",
       "      <td>ama bu</td>\n",
       "      <td>evet ama</td>\n",
       "      <td>hayır ama</td>\n",
       "      <td>var ama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>var</td>\n",
       "      <td>var mı</td>\n",
       "      <td>ne var</td>\n",
       "      <td>daha var</td>\n",
       "      <td>evet var</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>evet</td>\n",
       "      <td>evet ama</td>\n",
       "      <td>evet bu</td>\n",
       "      <td>evet ben</td>\n",
       "      <td>evet o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mı</td>\n",
       "      <td>var mı</td>\n",
       "      <td>hayır mı</td>\n",
       "      <td>bana mı</td>\n",
       "      <td>daha mı</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>değil</td>\n",
       "      <td>değil mi</td>\n",
       "      <td>bu değil</td>\n",
       "      <td>hayır değil</td>\n",
       "      <td>o değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>da</td>\n",
       "      <td>bu da</td>\n",
       "      <td>o da</td>\n",
       "      <td>daha da</td>\n",
       "      <td>bana da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>şey</td>\n",
       "      <td>bir şey</td>\n",
       "      <td>bu şey</td>\n",
       "      <td>o şey</td>\n",
       "      <td>şey gibi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hayır</td>\n",
       "      <td>hayır ben</td>\n",
       "      <td>hayır mı</td>\n",
       "      <td>hayır değil</td>\n",
       "      <td>hayır yok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>daha</td>\n",
       "      <td>bir daha</td>\n",
       "      <td>daha çok</td>\n",
       "      <td>daha var</td>\n",
       "      <td>daha değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sen</td>\n",
       "      <td>sen de</td>\n",
       "      <td>sen mi</td>\n",
       "      <td>hayır sen</td>\n",
       "      <td>sen değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kadar</td>\n",
       "      <td>ne kadar</td>\n",
       "      <td>bu kadar</td>\n",
       "      <td>o kadar</td>\n",
       "      <td>bana kadar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bana</td>\n",
       "      <td>bana ne</td>\n",
       "      <td>bana da</td>\n",
       "      <td>bana mı</td>\n",
       "      <td>bu bana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yok</td>\n",
       "      <td>hayır yok</td>\n",
       "      <td>yok gibi</td>\n",
       "      <td>evet yok</td>\n",
       "      <td>o yok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>onu</td>\n",
       "      <td>ve onu</td>\n",
       "      <td>onu da</td>\n",
       "      <td>evet onu</td>\n",
       "      <td>hayır onu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>seni</td>\n",
       "      <td>seni de</td>\n",
       "      <td>evet seni</td>\n",
       "      <td>hayır seni</td>\n",
       "      <td>seni değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beni</td>\n",
       "      <td>beni mi</td>\n",
       "      <td>beni de</td>\n",
       "      <td>beni değil</td>\n",
       "      <td>evet beni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bunu</td>\n",
       "      <td>hayır bunu</td>\n",
       "      <td>evet bunu</td>\n",
       "      <td>bunu değil</td>\n",
       "      <td>bunun için</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gibi</td>\n",
       "      <td>ne gibi</td>\n",
       "      <td>şey gibi</td>\n",
       "      <td>var gibi</td>\n",
       "      <td>yok gibi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   twogram_1   twogram_2    twogram_3   twogram_4\n",
       "0     bir     bir şey    bir daha       bir de     bir çok\n",
       "1      bu    bu kadar       bu ne       bu çok    bu değil\n",
       "2      ne    ne kadar     ne için      bana ne     ne gibi\n",
       "3      ve       ve de     ve evet        ve ne      ve sen\n",
       "4    için     ne için  benim için   senin için   onun için\n",
       "5      mi    değil mi      ben mi       sen mi     evet mi\n",
       "6       o     o kadar        o da      o değil      evet o\n",
       "7     ben      ben de   ben değil    hayır ben      ben mi\n",
       "8      de      sen de      bir de      beni de     beni de\n",
       "9     çok      bu çok    daha çok      çok var     çok şey\n",
       "10    ama      ama bu    evet ama    hayır ama     var ama\n",
       "11    var      var mı      ne var     daha var    evet var\n",
       "12   evet    evet ama     evet bu     evet ben      evet o\n",
       "13     mı      var mı    hayır mı      bana mı     daha mı\n",
       "14  değil    değil mi    bu değil  hayır değil     o değil\n",
       "15     da       bu da        o da      daha da     bana da\n",
       "16    şey     bir şey      bu şey        o şey    şey gibi\n",
       "17  hayır   hayır ben    hayır mı  hayır değil   hayır yok\n",
       "18   daha    bir daha    daha çok     daha var  daha değil\n",
       "19    sen      sen de      sen mi    hayır sen   sen değil\n",
       "20  kadar    ne kadar    bu kadar      o kadar  bana kadar\n",
       "21   bana     bana ne     bana da      bana mı     bu bana\n",
       "22    yok   hayır yok    yok gibi     evet yok       o yok\n",
       "23    onu      ve onu      onu da     evet onu   hayır onu\n",
       "24   seni     seni de   evet seni   hayır seni  seni değil\n",
       "25   beni     beni mi     beni de   beni değil   evet beni\n",
       "26   bunu  hayır bunu   evet bunu   bunu değil  bunun için\n",
       "27   gibi     ne gibi    şey gibi     var gibi    yok gibi"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_group_select = pd.read_excel(\"Turkish English Manual Selected 2 Gram Hybrids.xlsx\", sheet_name= \"2 gram target\")\n",
    "df_word_group_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bir şey',\n",
       " 'bir daha',\n",
       " 'bir de',\n",
       " 'bir çok',\n",
       " 'bu kadar',\n",
       " 'bu ne',\n",
       " 'bu çok',\n",
       " 'bu değil',\n",
       " 'ne kadar',\n",
       " 'ne için',\n",
       " 'bana ne',\n",
       " 'ne gibi',\n",
       " 've de',\n",
       " 've evet',\n",
       " 've ne',\n",
       " 've sen',\n",
       " 'ne için',\n",
       " 'benim için',\n",
       " 'senin için',\n",
       " 'onun için',\n",
       " 'değil mi',\n",
       " 'ben mi',\n",
       " 'sen mi',\n",
       " 'evet mi',\n",
       " 'o kadar',\n",
       " 'o da',\n",
       " 'o değil',\n",
       " 'evet o',\n",
       " 'ben de',\n",
       " 'ben değil',\n",
       " 'hayır ben',\n",
       " 'ben mi',\n",
       " 'sen de',\n",
       " 'bir de',\n",
       " 'beni de',\n",
       " 'beni de',\n",
       " 'bu çok',\n",
       " 'daha çok',\n",
       " 'çok var',\n",
       " 'çok şey',\n",
       " 'ama bu',\n",
       " 'evet ama',\n",
       " 'hayır ama',\n",
       " 'var ama',\n",
       " 'var mı',\n",
       " 'ne var',\n",
       " 'daha var',\n",
       " 'evet var',\n",
       " 'evet ama',\n",
       " 'evet bu',\n",
       " 'evet ben',\n",
       " 'evet o',\n",
       " 'var mı',\n",
       " 'hayır mı',\n",
       " 'bana mı',\n",
       " 'daha mı',\n",
       " 'değil mi',\n",
       " 'bu değil',\n",
       " 'hayır değil',\n",
       " 'o değil',\n",
       " 'bu da',\n",
       " 'o da',\n",
       " 'daha da',\n",
       " 'bana da',\n",
       " 'bir şey',\n",
       " 'bu şey',\n",
       " 'o şey',\n",
       " 'şey gibi',\n",
       " 'hayır ben',\n",
       " 'hayır mı',\n",
       " 'hayır değil',\n",
       " 'hayır yok',\n",
       " 'bir daha',\n",
       " 'daha çok',\n",
       " 'daha var',\n",
       " 'daha değil',\n",
       " 'sen de',\n",
       " 'sen mi',\n",
       " 'hayır sen',\n",
       " 'sen değil',\n",
       " 'ne kadar',\n",
       " 'bu kadar',\n",
       " 'o kadar',\n",
       " 'bana kadar',\n",
       " 'bana ne',\n",
       " 'bana da',\n",
       " 'bana mı',\n",
       " 'bu bana',\n",
       " 'hayır yok',\n",
       " 'yok gibi',\n",
       " 'evet yok',\n",
       " 'o yok',\n",
       " 've onu',\n",
       " 'onu da',\n",
       " 'evet onu',\n",
       " 'hayır onu',\n",
       " 'seni de',\n",
       " 'evet seni',\n",
       " 'hayır seni',\n",
       " 'seni değil',\n",
       " 'beni mi',\n",
       " 'beni de',\n",
       " 'beni değil',\n",
       " 'evet beni',\n",
       " 'hayır bunu',\n",
       " 'evet bunu',\n",
       " 'bunu değil',\n",
       " 'bunun için',\n",
       " 'ne gibi',\n",
       " 'şey gibi',\n",
       " 'var gibi',\n",
       " 'yok gibi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_list = []\n",
    "for i in range(len(df_word_group_select)):\n",
    "    for j in df_word_group_select.columns[1:]:\n",
    "        string = df_word_group_select.loc[i,j]\n",
    "        search_list.append(string.strip())\n",
    "#search_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_youtube(df,search_list,target_column, sample_num): \n",
    "    df_search_result = pd.DataFrame()\n",
    "    for j in search_list:\n",
    "        df_result = df[df[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){j}(?:\\s|$)\", na=True)]\n",
    "        df_result.sort_values(\"sentence\",key=lambda x:x.str.len(), inplace=True)\n",
    "        df_select = df_result.head(sample_num)\n",
    "        df_select.insert(0,\"search_string\",j)\n",
    "        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "    df_search_result.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_search_result = pd.DataFrame()\n",
    "#for i in range(len(df_english_select)):\n",
    "#    for j in df_english_select.columns[1:]:\n",
    "#        string = df_english_select.loc[i,j]\n",
    "#        df_result = df_youtube_sent[df_youtube_sent.sentence.str.contains(fr\"(?:\\s|^){string}(?:\\s|$)\", na=True)]\n",
    "#        df_result.sort_values(\"sentence\",key=lambda x:x.str.len(), inplace=True)\n",
    "#        df_select = df_result.head(6)\n",
    "#        df_select.insert(0,\"search_string\",string)\n",
    "#        df_search_result = pd.concat([df_search_result,df_select], axis=0)\n",
    "#df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurubal/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df_search_result = word_group_youtube(df_youtube_sentence,search_list,\"sentence\",6)\n",
    "df_search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_time_loc_list = []\n",
    "#def word_group_time_loc(df):\n",
    "#    for i in range(len(df)):\n",
    "#        word = df.loc[i,\"search_string\"]\n",
    "#        start_time = df.loc[i,\"{start_time\"]\n",
    "#        end_time = df.loc[i,\"end_time\"]\n",
    "#        sentence = df.loc[i,\"sentence\"]\n",
    "#        video_id = df.loc[i,\"video_id\"]\n",
    "#        time_length = end_time-start_time\n",
    "#        sentence_length = len(sentence)\n",
    "#        time_length_ratio = time_length/sentence_length\n",
    "#        loc_list = []\n",
    "#        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "#            loc_list.append(j)\n",
    "#            start = loc_list[0].start()\n",
    "#            end = loc_list[0].end()\n",
    "#            start_loc = start_time+(start*time_length_ratio)\n",
    "#            end_loc = start_time+(end*time_length_ratio)\n",
    "#        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "#    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[\"search_string\",\"start_time\",\"end_time\",\"sentence\",\"video_id\"])\n",
    "#\n",
    "#    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_time_loc_list = []\n",
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_id):\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_result = word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_shift = 0.3\n",
    "df_word_group_time_loc_result.start_time = df_word_group_time_loc_result.start_time.apply(lambda x: (x-time_shift))\n",
    "df_word_group_time_loc_result.end_time = df_word_group_time_loc_result.end_time.apply(lambda x: (x+time_shift))\n",
    "df_word_group_time_loc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_result.start_time = df_word_group_time_loc_result.start_time.apply(lambda x: round(x))\n",
    "df_word_group_time_loc_result.end_time = df_word_group_time_loc_result.end_time.apply(lambda x: round(x))\n",
    "df_word_group_time_loc_result "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
