{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language\n",
    "lang_pair = \"Italian\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/3-Adjust Word Level/{lang_folder.capitalize()} {lang_pair.capitalize()}\").mkdir(parents=True, exist_ok=True)  # create path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_twogram_sent = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Twogram_Merge.csv\")\n",
    "df_twogram_sent = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/N Gram And Sentence/Two_Gram_And_Sentence.csv\")  # ext. sentence and ngram\n",
    "df_twogram_sent.rename(columns={\"two_gram\":\"twogram\"}, inplace=True)  # ext. Not: Two_Gram_And_Sentence.csv convert to Two_Gram_And_Sentence_All.csv\n",
    "df_twogram_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pair_list = glob.glob(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()}_And_{lang_pair.lower().capitalize()}*_All.xlsx\")\n",
    "lang_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair_all = pd.read_excel(f\"{lang_pair_list[0]}\")\n",
    "df_pair_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = [\"sex\",\"seks\",\"seksi\",\"sexy\",\"sexe\",\"seksüel\",\"sexuell\",\"gey\",\"gay\",\"lezbiyen\",\"lesbienne\",\"eşcinsel\",\"mastürbasyon\",\"masturbation\",\"erotik\",\"érotique\", \\\n",
    "\"bikini\",\"penis\",\"vagina\",\"vajina\",\"fetish\",\"fetiş\",\"fetishy\",\"erotic\",\"erotik\",\"sexdom\",\"kondom\",\"condom\",\"dildo\",\"fetisj\",\"hétérosexuel\",\"féticher\",\"fétiche\",\"homosexuel\"\\\n",
    "\"ereksiyon\",\"erectie\",\"erection\",\"érection\",\"homoseksüel\",\"prezervatif\",\"préservatif\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df_pair_all[\"dict_entry_main\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_set = set(disable_list)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair = pd.DataFrame(list(words_set.difference(disable_set)), columns=[\"dict_entry_main\"])\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repetition(word_group):\n",
    "    words = word_tokenize(word_group)\n",
    "    word_unique = set(words)\n",
    "    if len(word_unique) == 1:\n",
    "        return \"repetitive_word_group\"\n",
    "    else:\n",
    "        return word_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_result(df,column_list): # df is dataframe, bigger_than is integer\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list)\n",
    "    df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"word_count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_bool(df, word_thresh_num, column_list): # df is a dataframe, bigger_than is an integer\n",
    "    list_all = []\n",
    "    for i in df.loc[:,[x for x in column_list]].columns:\n",
    "        var_list = df[f\"{i}\"].dropna().tolist()\n",
    "        for j in var_list:\n",
    "            list_all.append(j)\n",
    "    text = \" \".join(list_all)\n",
    "    word_list = re.findall(r\"\\w+\",text, re.UNICODE)\n",
    "    df_word_list = pd.DataFrame(word_list)\n",
    "    df_word_list.rename(columns={0:\"word\"}, inplace=True)\n",
    "    df_word_count = pd.DataFrame(df_word_list.value_counts())\n",
    "    df_word_count.reset_index(inplace=True)\n",
    "    df_word_count.rename(columns={0:\"count\"}, inplace=True)\n",
    "    \n",
    "    return  df_word_count[\"count\"][df_word_count.loc[:,\"count\"] > word_thresh_num].any()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while loop code block word and twogram pair\n",
    "twogram_num = 100  # 2*step_num minimum: for each word takes two twogram\n",
    "twogram_pair_num = 100  # 2*step_num minimum: for each word takes two twogram pair\n",
    "word_thresh_num = 2\n",
    "\n",
    "word_start = 0  # 0\n",
    "word_end = 7  # 10\n",
    "step_num = word_end  # 10\n",
    "word_limit = 28  # 200\n",
    "part_num = 1\n",
    "while word_end <= word_limit:\n",
    "    df_word = df_word_all.iloc[word_start:word_end,]\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # language pair twogram\n",
    "    ngram_list = []\n",
    "    for i in df_pair[\"dict_entry_main\"]:\n",
    "        for j in word_tokenize(i):\n",
    "            for k in df_word[\"word\"]:\n",
    "                twogram_1_2 = f\"{j} {k}\"\n",
    "                ngram_list.append(twogram_1_2)\n",
    "                twogram_2_1 = f\"{k} {j}\"\n",
    "                ngram_list.append(twogram_2_1)\n",
    "    df_pair_ngram = pd.DataFrame(ngram_list)\n",
    "    df_pair_ngram.rename(columns={0:\"twogram\"}, inplace=True)\n",
    "    df_pair_ngram.iloc[:,0] = df_pair_ngram.iloc[:,0].apply(lambda x: remove_repetition(x))\n",
    "    df_pair_ngram.drop_duplicates(inplace=True)\n",
    "    df_pair_ngram.reset_index(drop=True, inplace=True)\n",
    "    df_lang_pair_twogram = pd.merge(df_twogram_sent, df_pair_ngram, how=\"inner\", on=\"twogram\")\n",
    "    df_lang_pair_twogram.rename(columns={\"twogram\":f\"twogram_pair_{lang_pair.lower()}\"}, inplace=True)\n",
    "    df_lang_pair_twogram.drop_duplicates(inplace=True)\n",
    "    #df_lang_pair_twogram = df_lang_pair_twogram.head(100)\n",
    "\n",
    "    # output\n",
    "    df_output_result = pd.concat([df_word, df_lang_pair_twogram], axis=1)\n",
    "\n",
    "    df_lesson_result = pd.DataFrame(columns=[\"word\",\"freq_word\",f\"twogram_pair_{lang_pair.lower()}\",f\"freq_twogram_pair_{lang_pair.lower()}\"])\n",
    "    a = 0\n",
    "\n",
    "    for i in range(0,110):\n",
    "        # Insert words and their count \n",
    "        try:\n",
    "            word = df_output_result.iloc[i,0]  # word \n",
    "            freq_word = df_output_result.iloc[i,1]  # word freq\n",
    "            df_lesson_result.loc[i,\"word\"] = word\n",
    "            df_lesson_result.loc[i,\"freq_word\"] = freq_word\n",
    "        except:\n",
    "            pass\n",
    "         \n",
    "        # Insert twogram pair\n",
    "        try:\n",
    "            var2 = df_output_result.loc[a,f\"twogram_pair_{lang_pair.lower()}\"]\n",
    "            freq_var2 = df_output_result.iloc[a,3]  # twogram_pair frequency\n",
    "            if (len(df_lesson_result[f\"twogram_pair_{lang_pair.lower()}\"]) < twogram_pair_num): # and (not(word_count(df_lesson_result, word_thresh_num))):\n",
    "                df_lesson_result.loc[i,f\"twogram_pair_{lang_pair.lower()}\"] = var2\n",
    "                df_lesson_result.loc[i,f\"freq_twogram_pair_{lang_pair.lower()}\"] = freq_var2\n",
    "                try:\n",
    "                    while word_count_bool(df_lesson_result, word_thresh_num, [f\"twogram_pair_{lang_pair.lower()}\"]): # word count result                \n",
    "                        a += 1\n",
    "                        var2 = df_output_result.loc[a,f\"twogram_pair_{lang_pair.lower()}\"]\n",
    "                        freq_var2 = df_output_result.iloc[a,3]  # twogram_pair frequency\n",
    "                        df_lesson_result.loc[i,f\"twogram_pair_{lang_pair.lower()}\"] = var2\n",
    "                        df_lesson_result.loc[i,f\"freq_twogram_pair_{lang_pair.lower()}\"] = freq_var2\n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    df_lesson_result.loc[i,f\"twogram_pair_{lang_pair.lower()}\"] = np.nan\n",
    "                    df_lesson_result.loc[i,f\"freq_twogram_pair_{lang_pair.lower()}\"] = np.nan\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "        a += 1\n",
    "\n",
    "    df_lesson_word_count = word_count_result(df_lesson_result, [f\"twogram_pair_{lang_pair.lower()}\"])\n",
    "    df_lesson_result = pd.merge(df_lesson_result, df_lesson_word_count, how=\"left\", on=\"word\")\n",
    "    df_lesson_result = df_lesson_result.drop_duplicates()\n",
    "    df_lesson_result.to_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_{step_num}_Result{part_num}.xlsx\", index=False)\n",
    "\n",
    "    word_start += step_num\n",
    "    word_end += step_num\n",
    "    part_num += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_result_file = glob.glob(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_{step_num}_Result*.xlsx\")\n",
    "part_result_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read previous part result\n",
    "df_part_all = pd.DataFrame()\n",
    "for i in part_result_file:\n",
    "    df_var = pd.read_excel(f\"{i}\")\n",
    "    df_part_all = pd.concat([df_part_all,df_var], axis=0)\n",
    "df_part_twogram_pair = df_part_all.loc[:,[f\"twogram_pair_{lang_pair.lower()}\"]]\n",
    "df_part_twogram_pair.reset_index(drop=True, inplace=True)\n",
    "set_part_twogram_pair = set(df_part_twogram_pair[f\"twogram_pair_{lang_pair.lower()}\"])\n",
    "\n",
    "# while loop code block\n",
    "twogram_num = 100  # 2*step_num minimum: for each word takes two twogram\n",
    "twogram_pair_num = 100  # 2*step_num minimum: for each word takes two twogram pair\n",
    "word_thresh_num = 7\n",
    "\n",
    "word_start = 0  # 0\n",
    "word_end = 28  # 10\n",
    "step_num = word_end  # 10\n",
    "word_limit = 28  # 200\n",
    "part_num = 1\n",
    "while word_end <= word_limit:\n",
    "    df_word = df_word_all.iloc[word_start:word_end,]\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # language pair twogram\n",
    "    ngram_list = []\n",
    "    for i in df_pair[\"dict_entry_main\"]:\n",
    "        for j in word_tokenize(i):\n",
    "            for k in df_word[\"word\"]:\n",
    "                twogram_1_2 = f\"{j} {k}\"\n",
    "                ngram_list.append(twogram_1_2)\n",
    "                twogram_2_1 = f\"{k} {j}\"\n",
    "                ngram_list.append(twogram_2_1)\n",
    "    df_pair_ngram = pd.DataFrame(ngram_list)\n",
    "    df_pair_ngram.rename(columns={0:\"twogram\"}, inplace=True)\n",
    "    df_pair_ngram.iloc[:,0] = df_pair_ngram.iloc[:,0].apply(lambda x: remove_repetition(x))\n",
    "    df_pair_ngram.drop_duplicates(inplace=True)\n",
    "    df_pair_ngram.reset_index(drop=True, inplace=True)\n",
    "    df_lang_pair_twogram = pd.merge(df_twogram_sent, df_pair_ngram, how=\"inner\", on=\"twogram\")\n",
    "    df_lang_pair_twogram.rename(columns={\"twogram\":f\"twogram_pair_{lang_pair.lower()}\"}, inplace=True)\n",
    "    df_lang_pair_twogram.drop_duplicates(inplace=True)\n",
    "    #df_lang_pair_twogram = df_lang_pair_twogram.head(100)\n",
    "    set_lang_pair_twogram = set(df_lang_pair_twogram[f\"twogram_pair_{lang_pair.lower()}\"])\n",
    "    df_set_result = pd.DataFrame(set_lang_pair_twogram.difference(set_part_twogram_pair))\n",
    "    df_set_result.rename(columns={0:f\"twogram_pair_{lang_pair.lower()}\"}, inplace=True)\n",
    "    df_set_pair_twogram = pd.merge(df_lang_pair_twogram, df_set_result, how=\"inner\", on=f\"twogram_pair_{lang_pair.lower()}\")\n",
    "\n",
    "    # twogram\n",
    "    word_list = df_word[\"word\"].values.tolist()\n",
    "    data_kind = \"twogram\"\n",
    "    twogram_list  = df_twogram_sent.iloc[:,0].values.tolist()\n",
    "    \n",
    "    resultlist2 = []\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    resultlist2 = manager.list()\n",
    "    \n",
    "    def word_in_wordgroup2(list_var2):\n",
    "        mergelist = []\n",
    "        try:\n",
    "            word = list_var2.split()\n",
    "        except:\n",
    "            pass\n",
    "        var1 = range(len(word))\n",
    "        for j in var1:\n",
    "            if word[j] in word_list:\n",
    "                mergelist.append(word[j])\n",
    "                if len(mergelist) == len(word):\n",
    "                        resultlist2.append(list_var2)\n",
    "                            \n",
    "    if __name__ == '__main__':\n",
    "        # with Pool(16) as p:\n",
    "        with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "            p.map(word_in_wordgroup2, twogram_list) # string_word liste \n",
    "\n",
    "    result_list2 = list(resultlist2)\n",
    "    df_result2 = pd.DataFrame(result_list2)\n",
    "    df_result2 = df_result2.rename(columns = {0: f\"{data_kind}\"})\n",
    "    df_result2.iloc[:,0] = df_result2.iloc[:,0].apply(lambda x: remove_repetition(x)) # **\n",
    "    df_merge2 = pd.merge(df_result2, df_twogram_sent, how=\"inner\", on=f\"{data_kind}\")\n",
    "    df_merge_result2 = df_merge2.sort_values(by=\"frequency\", ascending=False)\n",
    "    df_merge_result2.drop_duplicates(inplace=True)\n",
    "    df_merge_result2.reset_index(drop=True, inplace=True)\n",
    "    df_twogram_result = df_merge_result2\n",
    "    #df_twogram_result = df_twogram_result.head(100)\n",
    "\n",
    "    # output\n",
    "    df_output_result = pd.concat([df_word, df_twogram_result, df_set_pair_twogram], axis=1)\n",
    "\n",
    "    df_lesson_result = pd.DataFrame(columns=[\"word\",\"freq_word\",\"twogram\",\"freq_twogram\",f\"twogram_pair_{lang_pair.lower()}\",f\"freq_twogram_pair_{lang_pair.lower()}\"])\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    for i in range(0,110):\n",
    "        # Insert words and their count \n",
    "        try:\n",
    "            word = df_output_result.iloc[i,0]  # word\n",
    "            freq_word = df_output_result.iloc[i,1]  # word freq\n",
    "            df_lesson_result.loc[i,\"word\"] = word\n",
    "            df_lesson_result.loc[i,\"freq_word\"] = freq_word\n",
    "        except:\n",
    "            pass\n",
    "         \n",
    "        # Insert n grams\n",
    "        try:\n",
    "            var1 = df_output_result.iloc[a,2]\n",
    "            freq_var1 = df_output_result.iloc[a,3]\n",
    "            if (len(df_lesson_result[\"twogram\"]) < twogram_num): # and (not(word_count(df_lesson_result, word_thresh_num))):\n",
    "                df_lesson_result.loc[i,\"twogram\"] = var1\n",
    "                df_lesson_result.loc[i,\"freq_twogram\"] = freq_var1\n",
    "                try:\n",
    "                    while word_count_bool(df_lesson_result, (word_thresh_num-5), [\"twogram\"]): # word count result                \n",
    "                        a += 1\n",
    "                        var1 = df_output_result.iloc[a,2]\n",
    "                        freq_var1 = df_output_result.iloc[a,3]\n",
    "                        df_lesson_result.loc[i,\"twogram\"] = var1\n",
    "                        df_lesson_result.loc[i,\"freq_twogram\"] = freq_var1\n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    df_lesson_result.loc[i,\"twogram\"] = np.nan\n",
    "                    df_lesson_result.loc[i,\"freq_twogram\"] = np.nan\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "        a += 1\n",
    "\n",
    "        try:\n",
    "            var2 = df_output_result.iloc[b,4]\n",
    "            freq_var2 = df_output_result.iloc[b,5]\n",
    "            if (len(df_lesson_result[f\"twogram_pair_{lang_pair.lower()}\"]) < twogram_pair_num): # and (not(word_count(df_lesson_result, word_thresh_num))):\n",
    "                df_lesson_result.loc[i,f\"twogram_pair_{lang_pair.lower()}\"] = var2\n",
    "                df_lesson_result.loc[i,f\"freq_twogram_pair_{lang_pair.lower()}\"] = freq_var2\n",
    "                try:\n",
    "                    while word_count_bool(df_lesson_result, (word_thresh_num-5), [f\"twogram_pair_{lang_pair.lower()}\"]): # word count result                \n",
    "                        b += 1\n",
    "                        var2 = df_output_result.iloc[b,4]\n",
    "                        freq_var2 = df_output_result.iloc[b,5]\n",
    "                        df_lesson_result.loc[i,f\"twogram_pair_{lang_pair.lower()}\"] = var2\n",
    "                        df_lesson_result.loc[i,f\"freq_twogram_pair_{lang_pair.lower()}\"] = freq_var2\n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    df_lesson_result.loc[i,f\"twogram_pair_{lang_pair.lower()}\"] = np.nan\n",
    "                    df_lesson_result.loc[i,f\"freq_twogram_pair_{lang_pair.lower()}\"] = np.nan\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "        b += 1\n",
    "\n",
    "    df_lesson_word_count = word_count_result(df_lesson_result, [\"twogram\",f\"twogram_pair_{lang_pair.lower()}\"])\n",
    "    df_lesson_result = pd.merge(df_lesson_result, df_lesson_word_count, how=\"left\", on=\"word\")\n",
    "    df_lesson_result = df_lesson_result.drop_duplicates()\n",
    "    df_lesson_result.to_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_{step_num}_Result{part_num}.xlsx\", index=False)\n",
    "\n",
    "    word_start += step_num\n",
    "    word_end += step_num\n",
    "    part_num += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lesson_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_count_result(df_lesson_result, [\"twogram\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_count_result(df_lesson_result, [f\"twogram_pair_{lang_pair.lower()}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Output File And Multi Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_11 = pd.read_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_7_Result1.xlsx\")\n",
    "df_part_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_12 = pd.read_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_7_Result2.xlsx\")\n",
    "df_part_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_13 = pd.read_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_7_Result3.xlsx\")\n",
    "df_part_13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_14 = pd.read_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_7_Result4.xlsx\")\n",
    "df_part_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_21 = pd.read_excel(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Step_28_Result1.xlsx\")\n",
    "df_part_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Result_With_Frequency.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_11.to_excel(writer, sheet_name='Word_Part11', index=False)\n",
    "df_part_12.to_excel(writer, sheet_name='Word_Part12', index=False)\n",
    "df_part_13.to_excel(writer, sheet_name='Word_Part13', index=False)\n",
    "df_part_14.to_excel(writer, sheet_name='Word_Part14', index=False)\n",
    "df_part_21.to_excel(writer, sheet_name='Word_Part21', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output File Word Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_twogram(df, list_column, target_column):\n",
    "\n",
    "    '''word_in_twogram(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, list_column and target_column are \n",
    "       dataframe column string name. list_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_word_result = pd.DataFrame()\n",
    "    for i in df[f\"{list_column}\"].dropna():\n",
    "        try:\n",
    "            word_in_twogram = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)] \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_twogram.insert(0,\"word\",i)\n",
    "        df_word_result = pd.concat([df_word_result,word_in_twogram], axis=0)\n",
    "    df_word_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_in_twogram(df_part_11, \"word\", f\"twogram_pair_{lang_pair.lower()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_11 = word_in_twogram(df_part_11, \"word\", f\"twogram_pair_{lang_pair.lower()}\")\n",
    "df_word_order_12 = word_in_twogram(df_part_12, \"word\", f\"twogram_pair_{lang_pair.lower()}\") \n",
    "df_word_order_13 = word_in_twogram(df_part_13, \"word\", f\"twogram_pair_{lang_pair.lower()}\") \n",
    "df_word_order_14 = word_in_twogram(df_part_14, \"word\", f\"twogram_pair_{lang_pair.lower()}\")\n",
    "df_word_order_21 = word_in_twogram(df_part_21, \"word\", f\"twogram\") \n",
    "df_word_order_212 = word_in_twogram(df_part_21, \"word\", f\"twogram_pair_{lang_pair.lower()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_11 = df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].apply(\", \".join).reset_index()   # df_word_order_11.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].transform(lambda x: ','.join(x))\n",
    "df_word_order_join_12 = df_word_order_12.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_13 = df_word_order_13.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_14 = df_word_order_14.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_211 = df_word_order_21.groupby([\"word\"])[\"twogram\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_212 = df_word_order_212.groupby([\"word\"])[f\"twogram_pair_{lang_pair.lower()}\"].apply(\", \".join).reset_index()\n",
    "df_word_order_join_21 = pd.merge(df_word_order_join_211, df_word_order_join_212, how=\"outer\", on=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2 = pd.ExcelWriter(f\"{lang_folder}_{lang_pair}_{word_limit}_Word_Join_Result_Without_Frequency.xlsx\", engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_order_join_11.to_excel(writer2, sheet_name='Word_Part11', index=False)\n",
    "df_word_order_join_12.to_excel(writer2, sheet_name='Word_Part12', index=False)\n",
    "df_word_order_join_13.to_excel(writer2, sheet_name='Word_Part13', index=False)\n",
    "df_word_order_join_14.to_excel(writer2, sheet_name='Word_Part14', index=False)\n",
    "df_word_order_join_21.to_excel(writer2, sheet_name='Word_Part21', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_*.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Deployment/Result/3-Adjust Word Level/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
