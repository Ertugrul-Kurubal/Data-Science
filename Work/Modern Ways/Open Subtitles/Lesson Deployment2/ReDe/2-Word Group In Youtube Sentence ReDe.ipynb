{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Group In Youtube Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 16\n"
     ]
    }
   ],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 20000  # 28  # native word end index\n",
    "\n",
    "# sentence ratio and time shift\n",
    "adjust_sent_word_ratio = 50\n",
    "adjust_text_word_ratio = 5\n",
    "shift = 0.3  # sentence time shift\n",
    "\n",
    "# func select\n",
    "word_group_adjust_select = False  # True, False; True for selecting word group according to sentence_word_count_number\n",
    "sentence_word_count_number = 10   # False for selecting word group according to max word group length\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # True for adding prefix suffix word\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = False  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Deployment2/Result/2-Word Group In Youtube Sentence/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exract_word_group(text, word_list):\n",
    "    '''\n",
    "    exract_word_group(text, word_list): \\n\n",
    "    text is a string sentence, word_list occurs words \n",
    "    '''\n",
    "    words = re.findall(r\"\\w+\", text, re.UNICODE)\n",
    "    index_list = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in word_list:\n",
    "            index_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    all_index_list = []\n",
    "    var_index_list = []\n",
    "    for j in range(len(index_list)):\n",
    "        try:\n",
    "            var1 = index_list[j] + 1  \n",
    "            var2 = index_list[j+1]\n",
    "        except:\n",
    "            var1 = index_list[j] + 1  \n",
    "            var2 = index_list[-1]\n",
    "        if var1 == var2:\n",
    "            var3 = index_list[j]\n",
    "            var_index_list.append(var3)\n",
    "        else:\n",
    "            var3 = index_list[j]\n",
    "            var_index_list.append(var3)\n",
    "            var4 = list(var_index_list)\n",
    "            all_index_list.append(var4)\n",
    "            var_index_list = []\n",
    "\n",
    "    text_list = []\n",
    "    for k in max(all_index_list, key=len):  # any error convert k to i\n",
    "        word = words[k]\n",
    "        text_list.append(word)\n",
    "        if len(text_list) >= sentence_word_count_number:\n",
    "            text = \" \".join(text_list)\n",
    "        else:\n",
    "            text = f\"sentencte_word_count_less_than_{sentence_word_count_number}\"\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_exract_word_group(df, source_text_column, opt_column, word_list, sent_len=False, sent_len_num=2):\n",
    "    '''\n",
    "    df_exract_word_group(df_adjust_text_ratio, sentence, video_id, word_list, sent_len=False, sent_len_num=2): \\n\n",
    "    df_adjust_text_ratio is a dataframe and it includes sentence and video_id columns. \\n\n",
    "    sentence is a string sentence. video_id is optional column value. word_list occurs words \\n\n",
    "    sent_len is sentence length condition. sent_len_num is sentence occurs how many word at least. \n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_new = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        source_text = df.loc[i,f\"{source_text_column}\"]\n",
    "        opt_var = df.loc[i,f\"{opt_column}\"]\n",
    "        words = re.findall(r\"\\w+\", source_text, re.UNICODE)\n",
    "        \n",
    "        index_list = []\n",
    "        for j in range(len(words)):\n",
    "            if words[j] in word_list:\n",
    "                index_list.append(j)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        all_index_list = []\n",
    "        var_index_list = []\n",
    "        for k in range(len(index_list)):\n",
    "            try:\n",
    "                var1 = index_list[k] + 1  \n",
    "                var2 = index_list[k+1]\n",
    "            except:\n",
    "                var1 = index_list[k] + 1  \n",
    "                var2 = index_list[-1]\n",
    "            if var1 == var2:\n",
    "                var3 = index_list[k]\n",
    "                var_index_list.append(var3)\n",
    "            else:\n",
    "                var3 = index_list[k]\n",
    "                var_index_list.append(var3)\n",
    "                var4 = list(var_index_list)\n",
    "                all_index_list.append(var4)\n",
    "                var_index_list = []\n",
    "\n",
    "        text_all_list = []\n",
    "        for m in all_index_list:\n",
    "            text_list = [] \n",
    "            for n in m:\n",
    "                word = words[n]\n",
    "                text_list.append(word)\n",
    "                if sent_len:\n",
    "                    if len(text_list) >= sent_len_num:\n",
    "                        text = \" \".join(text_list)\n",
    "                        text_all_list.append(text)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                   text = \" \".join(text_list)\n",
    "                   text_all_list.append(text) \n",
    "            \n",
    "        for search_string in text_all_list:\n",
    "            #df_var[\"index\"] = i\n",
    "            #df_var[\"search_string\"] = search_string\n",
    "            #df_var[\"sentence\"] = source_text\n",
    "            #df_var[\"video_id\"] = opt_var\n",
    "            df_list = []\n",
    "            df_list.append([i,search_string,source_text,opt_var])\n",
    "            df_var = pd.DataFrame(df_list,columns=[\"index\",\"search_string\",f\"{source_text_column}\",f\"{opt_column}\"])\n",
    "            df_new = pd.concat([df_new, df_var], axis=0)\n",
    "\n",
    "    return df_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_video_id):\n",
    "    '''\n",
    "    word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)  # will test\n",
    "    word_time_loc_list = []\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_video_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_video_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>18835735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bu</td>\n",
       "      <td>11062659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ne</td>\n",
       "      <td>8025880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ve</td>\n",
       "      <td>7766036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>için</td>\n",
       "      <td>5484109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988210</th>\n",
       "      <td>karneleme</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988211</th>\n",
       "      <td>karnaya</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988212</th>\n",
       "      <td>dörtlümüzün</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988213</th>\n",
       "      <td>karnavalınız</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988214</th>\n",
       "      <td>hurmanın</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988215 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  frequency\n",
       "0                bir   18835735\n",
       "1                 bu   11062659\n",
       "2                 ne    8025880\n",
       "3                 ve    7766036\n",
       "4               için    5484109\n",
       "...              ...        ...\n",
       "988210     karneleme          5\n",
       "988211       karnaya          5\n",
       "988212   dörtlümüzün          5\n",
       "988213  karnavalınız          5\n",
       "988214      hurmanın          5\n",
       "\n",
       "[988215 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select = df_word_all.iloc[word_start:word_end,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bir</td>\n",
       "      <td>18835735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bu</td>\n",
       "      <td>11062659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ne</td>\n",
       "      <td>8025880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ve</td>\n",
       "      <td>7766036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>için</td>\n",
       "      <td>5484109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>bianca</td>\n",
       "      <td>2477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>gülmeyi</td>\n",
       "      <td>2476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>yorgunsun</td>\n",
       "      <td>2476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>kriket</td>\n",
       "      <td>2476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>askerlerini</td>\n",
       "      <td>2476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  frequency\n",
       "0              bir   18835735\n",
       "1               bu   11062659\n",
       "2               ne    8025880\n",
       "3               ve    7766036\n",
       "4             için    5484109\n",
       "...            ...        ...\n",
       "19995       bianca       2477\n",
       "19996      gülmeyi       2476\n",
       "19997    yorgunsun       2476\n",
       "19998       kriket       2476\n",
       "19999  askerlerini       2476\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_word = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "    df_word = df_word.loc[:,[\"word\",\"frequency\"]]\n",
    "    df_word = pd.concat([df_word,df_word_select], axis=0)\n",
    "    df_word.drop_duplicates(inplace=True)    \n",
    "    df_word.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    df_word = df_word_select\n",
    "\n",
    "if native_word:\n",
    "    df_word\n",
    "else:\n",
    "    df_word = df_word.head(0)\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_entry_main</th>\n",
       "      <th>english_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abaküs</td>\n",
       "      <td>abacus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abluka</td>\n",
       "      <td>blockade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absorbe</td>\n",
       "      <td>absorb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>absürt</td>\n",
       "      <td>absurd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>açelya</td>\n",
       "      <td>azalea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>zebra</td>\n",
       "      <td>zebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>zikzak</td>\n",
       "      <td>zigzag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>zombi</td>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>zooloji</td>\n",
       "      <td>zoology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>zum</td>\n",
       "      <td>zoom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1778 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dict_entry_main english_word\n",
       "0             abaküs       abacus\n",
       "1             abluka     blockade\n",
       "2            absorbe       absorb\n",
       "3             absürt       absurd\n",
       "4             açelya       azalea\n",
       "...              ...          ...\n",
       "1773           zebra        zebra\n",
       "1774          zikzak       zigzag\n",
       "1775           zombi       zombie\n",
       "1776         zooloji      zoology\n",
       "1777             zum         zoom\n",
       "\n",
       "[1778 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pair = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.lower().capitalize()}/{lang_folder.capitalize()}_{lang_pair.lower().capitalize()}_Shared_Vocabulary.xlsx\")\n",
    "#df_pair = df_pair.head()\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict_entry_main</th>\n",
       "      <th>english_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dict_entry_main, english_word]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_prefix_suffix_select = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "    df_prefix_suffix_select = df_prefix_suffix_select.loc[:,[\"search_word\",\"word\"]]\n",
    "    df_prefix_suffix_select.rename(columns={\"search_word\":\"dict_entry_main\"}, inplace=True)\n",
    "    df_pair = pd.merge(df_pair,df_prefix_suffix_select, how=\"inner\", on=\"dict_entry_main\")\n",
    "    df_pair.drop_duplicates(inplace=True)\n",
    "    df_pair.reset_index(drop=True, inplace=True)\n",
    "    df_pair = df_pair.loc[:,[\"word\",f\"{lang_pair.lower()}_word\"]]\n",
    "    df_pair.rename(columns={\"word\":\"dict_entry_main\"}, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if etymology_word:\n",
    "    df_pair\n",
    "else:\n",
    "    df_pair = df_pair.head(0)\n",
    "    \n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = [\"sex\",\"seks\",\"seksi\",\"sexy\",\"sexe\",\"seksüel\",\"sexuell\",\"gey\",\"gay\",\"lezbiyen\",\"lesbienne\",\"eşcinsel\",\"mastürbasyon\",\"masturbation\",\"erotik\",\"érotique\", \\\n",
    "\"bikini\",\"penis\",\"vagina\",\"vajina\",\"fetish\",\"fetiş\",\"fetishy\",\"erotic\",\"erotik\",\"sexdom\",\"kondom\",\"condom\",\"dildo\",\"fetisj\",\"hétérosexuel\",\"féticher\",\"fétiche\",\"homosexuel\"\\\n",
    "\"ereksiyon\",\"erectie\",\"erection\",\"érection\",\"homoseksüel\",\"prezervatif\",\"préservatif\",\"ass\",\"fetisch\",\"fetiche\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_select = df_word[\"word\"].values.tolist()\n",
    "words = df_pair[\"dict_entry_main\"].values.tolist()\n",
    "word_select_set = set(word_select)\n",
    "disable_word_set = set(disable_list)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(word_select_set.union(words_set.difference(disable_word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Sentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:02:51.948</td>\n",
       "      <td>00:02:58.829</td>\n",
       "      <td>özgür bunlar normalde kamyon daha büyük araçla...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:03:00.956</td>\n",
       "      <td>00:03:04.236</td>\n",
       "      <td>burcu arka tarafı bağlamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:03:13.434</td>\n",
       "      <td>00:03:16.327</td>\n",
       "      <td>özgür arabaya yarım tur attıracağım</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:03:17.235</td>\n",
       "      <td>00:03:21.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:03:27.806</td>\n",
       "      <td>00:03:33.383</td>\n",
       "      <td>burcu şimdilik iki tekere takacağız ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063204</th>\n",
       "      <td>00:11:07.990</td>\n",
       "      <td>00:11:14.560</td>\n",
       "      <td>oynatma listesi linkini videonun sağ üst köşes...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063205</th>\n",
       "      <td>00:11:14.560</td>\n",
       "      <td>00:11:21.040</td>\n",
       "      <td>kısmına ekledim dilediğiniz kenar deseni uyarl...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063206</th>\n",
       "      <td>00:11:21.040</td>\n",
       "      <td>00:11:27.880</td>\n",
       "      <td>ibaretti bir sonraki farklı bir elle örgü eğit...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063207</th>\n",
       "      <td>00:11:27.880</td>\n",
       "      <td>00:11:32.290</td>\n",
       "      <td>kanalıma abone olarak videomu beğenmeyi unutma...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063208</th>\n",
       "      <td>00:00:03.280</td>\n",
       "      <td>00:00:12.200</td>\n",
       "      <td>merhaba arkadaşlar sizler için tren yaptım vid...</td>\n",
       "      <td>L_ERx2ZNheU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3063209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           start_time      end_time  \\\n",
       "0        00:02:51.948  00:02:58.829   \n",
       "1        00:03:00.956  00:03:04.236   \n",
       "2        00:03:13.434  00:03:16.327   \n",
       "3        00:03:17.235  00:03:21.338   \n",
       "4        00:03:27.806  00:03:33.383   \n",
       "...               ...           ...   \n",
       "3063204  00:11:07.990  00:11:14.560   \n",
       "3063205  00:11:14.560  00:11:21.040   \n",
       "3063206  00:11:21.040  00:11:27.880   \n",
       "3063207  00:11:27.880  00:11:32.290   \n",
       "3063208  00:00:03.280  00:00:12.200   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        özgür bunlar normalde kamyon daha büyük araçla...  8V9tq1pe8eI  \n",
       "1               burcu arka tarafı bağlamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      özgür arabaya yarım tur attıracağım  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu şimdilik iki tekere takacağız ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3063204  oynatma listesi linkini videonun sağ üst köşes...  MvWp9pWLihA  \n",
       "3063205  kısmına ekledim dilediğiniz kenar deseni uyarl...  MvWp9pWLihA  \n",
       "3063206  ibaretti bir sonraki farklı bir elle örgü eğit...  MvWp9pWLihA  \n",
       "3063207  kanalıma abone olarak videomu beğenmeyi unutma...  MvWp9pWLihA  \n",
       "3063208  merhaba arkadaşlar sizler için tren yaptım vid...  L_ERx2ZNheU  \n",
       "\n",
       "[3063209 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>sentence</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>171.948</td>\n",
       "      <td>178.829</td>\n",
       "      <td>özgür bunlar normalde kamyon daha büyük araçla...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>180.956</td>\n",
       "      <td>184.236</td>\n",
       "      <td>burcu arka tarafı bağlamak kolay olmayacak</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>193.434</td>\n",
       "      <td>196.327</td>\n",
       "      <td>özgür arabaya yarım tur attıracağım</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>197.235</td>\n",
       "      <td>201.338</td>\n",
       "      <td>burcu biraz daha devam et devam et tamam oldu</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>207.806</td>\n",
       "      <td>213.383</td>\n",
       "      <td>burcu şimdilik iki tekere takacağız ama kar ka...</td>\n",
       "      <td>8V9tq1pe8eI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063204</th>\n",
       "      <td>3063204</td>\n",
       "      <td>667.990</td>\n",
       "      <td>674.560</td>\n",
       "      <td>oynatma listesi linkini videonun sağ üst köşes...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063205</th>\n",
       "      <td>3063205</td>\n",
       "      <td>674.560</td>\n",
       "      <td>681.040</td>\n",
       "      <td>kısmına ekledim dilediğiniz kenar deseni uyarl...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063206</th>\n",
       "      <td>3063206</td>\n",
       "      <td>681.040</td>\n",
       "      <td>687.880</td>\n",
       "      <td>ibaretti bir sonraki farklı bir elle örgü eğit...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063207</th>\n",
       "      <td>3063207</td>\n",
       "      <td>687.880</td>\n",
       "      <td>692.290</td>\n",
       "      <td>kanalıma abone olarak videomu beğenmeyi unutma...</td>\n",
       "      <td>MvWp9pWLihA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063208</th>\n",
       "      <td>3063208</td>\n",
       "      <td>3.280</td>\n",
       "      <td>12.200</td>\n",
       "      <td>merhaba arkadaşlar sizler için tren yaptım vid...</td>\n",
       "      <td>L_ERx2ZNheU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3063209 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index  start_time  end_time  \\\n",
       "0              0     171.948   178.829   \n",
       "1              1     180.956   184.236   \n",
       "2              2     193.434   196.327   \n",
       "3              3     197.235   201.338   \n",
       "4              4     207.806   213.383   \n",
       "...          ...         ...       ...   \n",
       "3063204  3063204     667.990   674.560   \n",
       "3063205  3063205     674.560   681.040   \n",
       "3063206  3063206     681.040   687.880   \n",
       "3063207  3063207     687.880   692.290   \n",
       "3063208  3063208       3.280    12.200   \n",
       "\n",
       "                                                  sentence     video_id  \n",
       "0        özgür bunlar normalde kamyon daha büyük araçla...  8V9tq1pe8eI  \n",
       "1               burcu arka tarafı bağlamak kolay olmayacak  8V9tq1pe8eI  \n",
       "2                      özgür arabaya yarım tur attıracağım  8V9tq1pe8eI  \n",
       "3            burcu biraz daha devam et devam et tamam oldu  8V9tq1pe8eI  \n",
       "4        burcu şimdilik iki tekere takacağız ama kar ka...  8V9tq1pe8eI  \n",
       "...                                                    ...          ...  \n",
       "3063204  oynatma listesi linkini videonun sağ üst köşes...  MvWp9pWLihA  \n",
       "3063205  kısmına ekledim dilediğiniz kenar deseni uyarl...  MvWp9pWLihA  \n",
       "3063206  ibaretti bir sonraki farklı bir elle örgü eğit...  MvWp9pWLihA  \n",
       "3063207  kanalıma abone olarak videomu beğenmeyi unutma...  MvWp9pWLihA  \n",
       "3063208  merhaba arkadaşlar sizler için tren yaptım vid...  L_ERx2ZNheU  \n",
       "\n",
       "[3063209 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence.reset_index(inplace=True)  # adding index column\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-13:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-26b0f19c2487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# with Pool(16) as p:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnprocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_word_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_youtube_sentence)))\n",
    "\n",
    "def sentence_word_ratio(index_num):\n",
    "    index = df_youtube_sentence.loc[index_num,\"index\"]\n",
    "    start = df_youtube_sentence.loc[index_num,\"start_time\"]\n",
    "    end = df_youtube_sentence.loc[index_num,\"end_time\"]\n",
    "    sentence = df_youtube_sentence.loc[index_num,\"sentence\"]\n",
    "    videoid = df_youtube_sentence.loc[index_num,\"video_id\"]\n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/len(sent_word)+0.001)*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list.append([index,start,end,sentence,videoid,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(sentence_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result = pd.DataFrame(list(result_list), columns=[\"index\",\"start_time\",\"end_time\",\"sentence\",\"video_id\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_sentence_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_sentence_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_sentence_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result[\"different_word\"] = df_sentence_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result[\"intersect_word\"] = df_sentence_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_sentence_ratio = df_sentence_ratio_result[df_sentence_ratio_result[\"word_ratio\"] >= adjust_sent_word_ratio]\n",
    "df_adjust_sentence_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_sentence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_sentence_ratio.loc[:,\"search_string\"] = df_adjust_sentence_ratio.loc[:,\"sentence\"].map(lambda x: exract_word_group(x, word_list))\n",
    "df_adjust_sentence_ratio = df_adjust_sentence_ratio[~(df_adjust_sentence_ratio[\"search_string\"]==f\"sentencte_word_count_less_than_{sentence_word_count_number}\")]\n",
    "#df_adjust_sentence_ratio.reset_index(drop=True, inplace=True)\n",
    "df_adjust_sentence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result = word_group_time_loc(df_adjust_sentence_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"start_time\"] = df_word_group_time_loc_in_sent_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_sent_result[\"end_time\"] = df_word_group_time_loc_in_sent_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"start_time\"] = df_word_group_time_loc_in_sent_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_sent_result[\"end_time\"] = df_word_group_time_loc_in_sent_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_sent_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_word_group_time_loc_in_sent_result['video_id'].map(str)+\"&t=\"+df_word_group_time_loc_in_sent_result['start_time'].map(str)+\"s\"\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result = df_word_group_time_loc_in_sent_result.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Max_In_Youtube_Sentence_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Videoid Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_youtube_sentence.groupby(\"video_id\")[\"sentence\"].apply(\" \".join).reset_index()\n",
    "df_videoid_sentence.reset_index(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_start_time = df_youtube_sentence.groupby(\"video_id\")[[\"start_time\"]].min()\n",
    "df_videoid_start_time.reset_index(inplace=True)\n",
    "df_videoid_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_end_time = df_youtube_sentence.groupby(\"video_id\")[[\"end_time\"]].max()\n",
    "df_videoid_end_time.reset_index(inplace=True)\n",
    "df_videoid_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_start_end_time = pd.merge(df_videoid_start_time, df_videoid_end_time, how=\"inner\", on=\"video_id\")\n",
    "df_merge_start_end_time.drop_duplicates(inplace=True)\n",
    "df_merge_start_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = pd.merge(df_videoid_sentence, df_merge_start_end_time,how=\"inner\", on=\"video_id\")\n",
    "df_videoid_sentence.drop_duplicates(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list2 = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_videoid_sentence)))\n",
    "\n",
    "def videoid_word_ratio(index_num):\n",
    "    index = df_videoid_sentence.loc[index_num,\"index\"]\n",
    "    videoid = df_videoid_sentence.loc[index_num,\"video_id\"]\n",
    "    sentence = df_videoid_sentence.loc[index_num,\"sentence\"]\n",
    "    \n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/len(sent_word)+0.001)*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list2.append([index,videoid,sentence,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(videoid_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result = pd.DataFrame(list(result_list2), columns=[\"index\",\"video_id\",\"sentence\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_text_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_text_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_text_ratio_result.drop([\"index\"], axis=1, inplace=True)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result[\"different_word\"] = df_text_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result[\"intersect_word\"] = df_text_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_text_ratio_result[df_text_ratio_result[\"word_ratio\"] >= adjust_text_word_ratio]\n",
    "df_adjust_text_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_group_adjust_select:\n",
    "    df_adjust_text_ratio = df_exract_word_group(df_adjust_text_ratio, \"sentence\", \"video_id\", word_list, sent_len=word_group_adjust_select, sent_len_num=sentence_word_count_number)\n",
    "else:\n",
    "    df_adjust_text_ratio.loc[:,\"search_string\"] = df_adjust_text_ratio.loc[:,\"sentence\"].map(lambda x: exract_word_group(x, word_list))\n",
    "    df_adjust_text_ratio = df_adjust_text_ratio[~(df_adjust_text_ratio[\"search_string\"]==f\"sentencte_word_count_less_than_{sentence_word_count_number}\")]\n",
    "    \n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_adjust_text_ratio[[\"search_string\",\"video_id\",\"sentence\"]]\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_videoid_sentence[[\"video_id\",\"start_time\",\"end_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = pd.merge(df_adjust_text_ratio, df_videoid_sentence, how=\"inner\", on=\"video_id\")\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = word_group_time_loc(df_adjust_text_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_text_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_word_group_time_loc_in_text_result['video_id'].map(str)+\"&t=\"+df_word_group_time_loc_in_text_result['start_time'].map(str)+\"s\"\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = df_word_group_time_loc_in_text_result.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_word_group_time_loc_in_text_result.to_csv(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Adjust_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_group_adjust_select:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Adjust_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)\n",
    "else:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Max_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Videoid Text Analysis Multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_youtube_sentence.groupby(\"video_id\")[\"sentence\"].apply(\" \".join).reset_index()\n",
    "df_videoid_sentence.reset_index(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_start_time = df_youtube_sentence.groupby(\"video_id\")[[\"start_time\"]].min()\n",
    "df_videoid_start_time.reset_index(inplace=True)\n",
    "df_videoid_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_end_time = df_youtube_sentence.groupby(\"video_id\")[[\"end_time\"]].max()\n",
    "df_videoid_end_time.reset_index(inplace=True)\n",
    "df_videoid_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_start_end_time = pd.merge(df_videoid_start_time, df_videoid_end_time, how=\"inner\", on=\"video_id\")\n",
    "df_merge_start_end_time.drop_duplicates(inplace=True)\n",
    "df_merge_start_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = pd.merge(df_videoid_sentence, df_merge_start_end_time,how=\"inner\", on=\"video_id\")\n",
    "df_videoid_sentence.drop_duplicates(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list3 = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_videoid_sentence)))\n",
    "\n",
    "def videoid_word_ratio(index_num):\n",
    "    index = df_videoid_sentence.loc[index_num,\"index\"]\n",
    "    videoid = df_videoid_sentence.loc[index_num,\"video_id\"]\n",
    "    sentence = df_videoid_sentence.loc[index_num,\"sentence\"]\n",
    "    \n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/len(sent_word)+0.001)*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list3.append([index,videoid,sentence,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(videoid_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result = pd.DataFrame(list(result_list3), columns=[\"index\",\"video_id\",\"sentence\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_text_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_text_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_text_ratio_result.drop([\"index\"], axis=1, inplace=True)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result[\"different_word\"] = df_text_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result[\"intersect_word\"] = df_text_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_text_ratio_result[df_text_ratio_result[\"word_ratio\"] >= adjust_text_word_ratio]\n",
    "df_adjust_text_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list4 = manager.list()\n",
    "index_list = manager.list()\n",
    "all_index_list = manager.list()\n",
    "var_index_list = manager.list()\n",
    "text_all_list = manager.list()  \n",
    "df = df_adjust_text_ratio\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "index_num = list(range(len(df)))\n",
    "sent_len = word_group_adjust_select\n",
    "sent_len_num = sentence_word_count_number \n",
    "\n",
    "def df_exract_word_group(index_num):    \n",
    "    source_text = df.loc[index_num,\"sentence\"]\n",
    "    opt_var = df.loc[index_num,\"video_id\"]\n",
    "    words = re.findall(r\"\\w+\", source_text, re.UNICODE)\n",
    "    \n",
    "    index_list = []\n",
    "    for j in range(len(words)):\n",
    "        if words[j] in word_list:\n",
    "            index_list.append(j)\n",
    "        else:\n",
    "            pass\n",
    "    all_index_list = []\n",
    "    var_index_list = []\n",
    "    for k in range(len(index_list)):\n",
    "        try:\n",
    "            var1 = index_list[k] + 1  \n",
    "            var2 = index_list[k+1]\n",
    "        except:\n",
    "            var1 = index_list[k] + 1  \n",
    "            var2 = index_list[-1]\n",
    "        if var1 == var2:\n",
    "            var3 = index_list[k]\n",
    "            var_index_list.append(var3)\n",
    "        else:\n",
    "            var3 = index_list[k]\n",
    "            var_index_list.append(var3)\n",
    "            var4 = list(var_index_list)\n",
    "            all_index_list.append(var4)\n",
    "            var_index_list = []\n",
    "    text_all_list = []\n",
    "    for m in all_index_list:\n",
    "        text_list = [] \n",
    "        for n in m:\n",
    "            word = words[n]\n",
    "            text_list.append(word)\n",
    "            if sent_len:\n",
    "                if len(text_list) >= sent_len_num:\n",
    "                    text = \" \".join(text_list)\n",
    "                    text_all_list.append(text)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "               text = \" \".join(text_list)\n",
    "               text_all_list.append(text) \n",
    "        \n",
    "    for search_string in text_all_list:\n",
    "        #df_var[\"index\"] = i\n",
    "        #df_var[\"search_string\"] = search_string\n",
    "        #df_var[\"sentence\"] = source_text\n",
    "        #df_var[\"video_id\"] = opt_var\n",
    "        result_list4.append([index_num,search_string,source_text,opt_var])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(df_exract_word_group, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = pd.DataFrame(list(result_list4), columns=[\"index\",\"search_string\",\"sentence\",\"video_id\"])\n",
    "df_adjust_text_ratio.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_adjust_text_ratio.reset_index(drop=True, inplace=True)\n",
    "df_adjust_text_ratio.drop([\"index\"], axis=1, inplace=True)\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_adjust_text_ratio[[\"search_string\",\"video_id\",\"sentence\"]]\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_videoid_sentence[[\"video_id\",\"start_time\",\"end_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = pd.merge(df_adjust_text_ratio, df_videoid_sentence, how=\"inner\", on=\"video_id\")\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = word_group_time_loc(df_adjust_text_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_text_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_word_group_time_loc_in_text_result['video_id'].map(str)+\"&t=\"+df_word_group_time_loc_in_text_result['start_time'].map(str)+\"s\"\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = df_word_group_time_loc_in_text_result.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_group_adjust_select:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Adjust_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)\n",
    "else:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Max_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_word_group_time_loc_in_text_result.loc[13,\"sentence\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", text, re.UNICODE)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for i in range(len(words)):\n",
    "    if words[i] in word_list:\n",
    "        index_list.append(i)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_list = []\n",
    "var_index_list = []\n",
    "for j in range(len(index_list)):\n",
    "    try:\n",
    "        var1 = index_list[j] + 1  \n",
    "        var2 = index_list[j+1]\n",
    "    except:\n",
    "        var1 = index_list[j] + 1  \n",
    "        var2 = index_list[-1]\n",
    "    if var1 == var2:\n",
    "        var3 = index_list[j]\n",
    "        var_index_list.append(var3)\n",
    "    else:\n",
    "        var3 = index_list[j]\n",
    "        var_index_list.append(var3)\n",
    "        var4 = list(var_index_list)\n",
    "        all_index_list.append(var4)\n",
    "        var_index_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(all_index_list, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(all_index_list, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for i in max(all_index_list, key=len):\n",
    "    word = words[i]\n",
    "    text_list.append(word)\n",
    "text = \" \".join(text_list)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_adjust_text_ratio.iloc[4:5,:]\n",
    "#df_test.reset_index(drop=True, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ablan burada nerede i çeriye kadar gelemedi ama benimle buraya kadar geldi sen şimdi git baba sonra beni almaya gel ama hemen gel tamam mı tamam defol defol defol her şeyi mahvettin defol ne işin var burada hayır hayır hayır bırakın bırak beni bırak beni bırak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", text, re.UNICODE)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[1] in word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for i in range(len(words)):\n",
    "    if words[i] in word_list:\n",
    "        index_list.append(i)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_list = []\n",
    "var_index_list = []\n",
    "for j in range(len(index_list)):\n",
    "    try:\n",
    "        var1 = index_list[j] + 1  \n",
    "        var2 = index_list[j+1]\n",
    "    except:\n",
    "        var1 = index_list[j] + 1  \n",
    "        var2 = index_list[-1]\n",
    "    if var1 == var2:\n",
    "        var3 = index_list[j]\n",
    "        var_index_list.append(var3)\n",
    "    else:\n",
    "        var3 = index_list[j]\n",
    "        var_index_list.append(var3)\n",
    "        var4 = list(var_index_list)\n",
    "        all_index_list.append(var4)\n",
    "        var_index_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_index_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(all_index_list, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for i in max(all_index_list, key=len):\n",
    "    word = words[i]\n",
    "    text_list.append(word)\n",
    "text = \" \".join(text_list)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
