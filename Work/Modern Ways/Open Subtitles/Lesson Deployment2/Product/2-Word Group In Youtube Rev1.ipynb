{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Group In Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 200  # 28  # native word end index\n",
    "\n",
    "# sentence ratio and time shift\n",
    "adjust_sent_word_ratio = 50\n",
    "adjust_text_word_ratio = 5\n",
    "shift = 0.3  # sentence time shift\n",
    "\n",
    "# func select\n",
    "word_group_adjust_select = False  # True, False; True for selecting word group according to sentence_word_count_number\n",
    "sentence_word_count_number = 4   # False for selecting word group according to max word group length\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # True for adding prefix suffix word\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = False  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Deployment2/Result/2-Word Group In Youtube Sentence/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exract_word_group(text, word_list):\n",
    "    '''\n",
    "    exract_word_group(text, word_list): \\n\n",
    "    text is a string sentence, word_list occurs words \n",
    "    '''\n",
    "    words = re.findall(r\"\\w+\", text, re.UNICODE)\n",
    "    index_list = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in word_list:\n",
    "            index_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    all_index_list = []\n",
    "    var_index_list = []\n",
    "    for j in range(len(index_list)):\n",
    "        try:\n",
    "            var1 = index_list[j] + 1  \n",
    "            var2 = index_list[j+1]\n",
    "        except:\n",
    "            var1 = index_list[j] + 1  \n",
    "            var2 = index_list[-1]\n",
    "        if var1 == var2:\n",
    "            var3 = index_list[j]\n",
    "            var_index_list.append(var3)\n",
    "        else:\n",
    "            var3 = index_list[j]\n",
    "            var_index_list.append(var3)\n",
    "            var4 = list(var_index_list)\n",
    "            all_index_list.append(var4)\n",
    "            var_index_list = []\n",
    "\n",
    "    text_list = []\n",
    "    for k in max(all_index_list, key=len):  # any error convert k to i\n",
    "        word = words[k]\n",
    "        text_list.append(word)\n",
    "        if len(text_list) >= sentence_word_count_number:\n",
    "            text = \" \".join(text_list)\n",
    "        else:\n",
    "            text = f\"sentencte_word_count_less_than_{sentence_word_count_number}\"\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_exract_word_group(df, source_text_column, opt_column, word_list, sent_len=False, sent_len_num=2):\n",
    "    '''\n",
    "    df_exract_word_group(df_adjust_text_ratio, sentence, video_id, word_list, sent_len=False, sent_len_num=2): \\n\n",
    "    df_adjust_text_ratio is a dataframe and it includes sentence and video_id columns. \\n\n",
    "    sentence is a string sentence. video_id is optional column value. word_list occurs words \\n\n",
    "    sent_len is sentence length condition. sent_len_num is sentence occurs how many word at least. \n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df_new = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        source_text = df.loc[i,f\"{source_text_column}\"]\n",
    "        opt_var = df.loc[i,f\"{opt_column}\"]\n",
    "        words = re.findall(r\"\\w+\", source_text, re.UNICODE)\n",
    "        \n",
    "        index_list = []\n",
    "        for j in range(len(words)):\n",
    "            if words[j] in word_list:\n",
    "                index_list.append(j)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        all_index_list = []\n",
    "        var_index_list = []\n",
    "        for k in range(len(index_list)):\n",
    "            try:\n",
    "                var1 = index_list[k] + 1  \n",
    "                var2 = index_list[k+1]\n",
    "            except:\n",
    "                var1 = index_list[k] + 1  \n",
    "                var2 = index_list[-1]\n",
    "            if var1 == var2:\n",
    "                var3 = index_list[k]\n",
    "                var_index_list.append(var3)\n",
    "            else:\n",
    "                var3 = index_list[k]\n",
    "                var_index_list.append(var3)\n",
    "                var4 = list(var_index_list)\n",
    "                all_index_list.append(var4)\n",
    "                var_index_list = []\n",
    "\n",
    "        text_all_list = []\n",
    "        for m in all_index_list:\n",
    "            text_list = [] \n",
    "            for n in m:\n",
    "                word = words[n]\n",
    "                text_list.append(word)\n",
    "                if sent_len:\n",
    "                    if len(text_list) >= sent_len_num:\n",
    "                        text = \" \".join(text_list)\n",
    "                        text_all_list.append(text)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                   text = \" \".join(text_list)\n",
    "                   text_all_list.append(text) \n",
    "            \n",
    "        for search_string in text_all_list:\n",
    "            #df_var[\"index\"] = i\n",
    "            #df_var[\"search_string\"] = search_string\n",
    "            #df_var[\"sentence\"] = source_text\n",
    "            #df_var[\"video_id\"] = opt_var\n",
    "            df_list = []\n",
    "            df_list.append([i,search_string,source_text,opt_var])\n",
    "            df_var = pd.DataFrame(df_list,columns=[\"index\",\"search_string\",f\"{source_text_column}\",f\"{opt_column}\"])\n",
    "            df_new = pd.concat([df_new, df_var], axis=0)\n",
    "\n",
    "    return df_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_video_id):\n",
    "    '''\n",
    "    word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)  # will test\n",
    "    word_time_loc_list = []\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_video_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_video_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select = df_word_all.iloc[word_start:word_end,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_word = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "    df_word = df_word.loc[:,[\"word\",\"frequency\"]]\n",
    "    df_word = pd.concat([df_word,df_word_select], axis=0)\n",
    "    df_word.drop_duplicates(inplace=True)    \n",
    "    df_word.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    df_word = df_word_select\n",
    "\n",
    "if native_word:\n",
    "    df_word\n",
    "else:\n",
    "    df_word = df_word.head(0)\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.lower().capitalize()}/{lang_folder.capitalize()}_{lang_pair.lower().capitalize()}_Shared_Vocabulary.xlsx\")\n",
    "#df_pair = df_pair.head()\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_prefix_suffix_select = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "    df_prefix_suffix_select = df_prefix_suffix_select.loc[:,[\"search_word\",\"word\"]]\n",
    "    df_prefix_suffix_select.rename(columns={\"search_word\":\"dict_entry_main\"}, inplace=True)\n",
    "    df_pair = pd.merge(df_pair,df_prefix_suffix_select, how=\"inner\", on=\"dict_entry_main\")\n",
    "    df_pair.drop_duplicates(inplace=True)\n",
    "    df_pair.reset_index(drop=True, inplace=True)\n",
    "    df_pair = df_pair.loc[:,[\"word\",f\"{lang_pair.lower()}_word\"]]\n",
    "    df_pair.rename(columns={\"word\":\"dict_entry_main\"}, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if etymology_word:\n",
    "    df_pair\n",
    "else:\n",
    "    df_pair = df_pair.head(0)\n",
    "    \n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = [\"sex\",\"seks\",\"seksi\",\"sexy\",\"sexe\",\"seksüel\",\"sexuell\",\"gey\",\"gay\",\"lezbiyen\",\"lesbienne\",\"eşcinsel\",\"mastürbasyon\",\"masturbation\",\"erotik\",\"érotique\", \\\n",
    "\"bikini\",\"penis\",\"vagina\",\"vajina\",\"fetish\",\"fetiş\",\"fetishy\",\"erotic\",\"erotik\",\"sexdom\",\"kondom\",\"condom\",\"dildo\",\"fetisj\",\"hétérosexuel\",\"féticher\",\"fétiche\",\"homosexuel\"\\\n",
    "\"ereksiyon\",\"erectie\",\"erection\",\"érection\",\"homoseksüel\",\"prezervatif\",\"préservatif\",\"ass\",\"fetisch\",\"fetiche\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_select = df_word[\"word\"].values.tolist()\n",
    "words = df_pair[\"dict_entry_main\"].values.tolist()\n",
    "word_select_set = set(word_select)\n",
    "disable_word_set = set(disable_list)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list((word_select_set.union(words_set)).difference(disable_word_set))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Sentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence.reset_index(inplace=True)  # adding index column\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_youtube_sentence)))\n",
    "\n",
    "def sentence_word_ratio(index_num):\n",
    "    index = df_youtube_sentence.loc[index_num,\"index\"]\n",
    "    start = df_youtube_sentence.loc[index_num,\"start_time\"]\n",
    "    end = df_youtube_sentence.loc[index_num,\"end_time\"]\n",
    "    sentence = df_youtube_sentence.loc[index_num,\"sentence\"]\n",
    "    videoid = df_youtube_sentence.loc[index_num,\"video_id\"]\n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/len(sent_word)+0.001)*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list.append([index,start,end,sentence,videoid,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(sentence_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result = pd.DataFrame(list(result_list), columns=[\"index\",\"start_time\",\"end_time\",\"sentence\",\"video_id\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_sentence_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_sentence_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_sentence_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentence_ratio_result[\"different_word\"] = df_sentence_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result[\"intersect_word\"] = df_sentence_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_sentence_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_sentence_ratio = df_sentence_ratio_result[df_sentence_ratio_result[\"word_ratio\"] >= adjust_sent_word_ratio]\n",
    "df_adjust_sentence_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_sentence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_sentence_ratio.loc[:,\"search_string\"] = df_adjust_sentence_ratio.loc[:,\"sentence\"].map(lambda x: exract_word_group(x, word_list))\n",
    "df_adjust_sentence_ratio = df_adjust_sentence_ratio[~(df_adjust_sentence_ratio[\"search_string\"]==f\"sentencte_word_count_less_than_{sentence_word_count_number}\")]\n",
    "#df_adjust_sentence_ratio.reset_index(drop=True, inplace=True)\n",
    "df_adjust_sentence_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result = word_group_time_loc(df_adjust_sentence_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"start_time\"] = df_word_group_time_loc_in_sent_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_sent_result[\"end_time\"] = df_word_group_time_loc_in_sent_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"start_time\"] = df_word_group_time_loc_in_sent_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_sent_result[\"end_time\"] = df_word_group_time_loc_in_sent_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_sent_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_word_group_time_loc_in_sent_result['video_id'].map(str)+\"&t=\"+df_word_group_time_loc_in_sent_result['start_time'].map(str)+\"s\"\n",
    "df_word_group_time_loc_in_sent_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result = df_word_group_time_loc_in_sent_result.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_sent_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Max_In_Youtube_Sentence_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Videoid Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_youtube_sentence.groupby(\"video_id\")[\"sentence\"].apply(\" \".join).reset_index()\n",
    "df_videoid_sentence.reset_index(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_start_time = df_youtube_sentence.groupby(\"video_id\")[[\"start_time\"]].min()\n",
    "df_videoid_start_time.reset_index(inplace=True)\n",
    "df_videoid_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_end_time = df_youtube_sentence.groupby(\"video_id\")[[\"end_time\"]].max()\n",
    "df_videoid_end_time.reset_index(inplace=True)\n",
    "df_videoid_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_start_end_time = pd.merge(df_videoid_start_time, df_videoid_end_time, how=\"inner\", on=\"video_id\")\n",
    "df_merge_start_end_time.drop_duplicates(inplace=True)\n",
    "df_merge_start_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = pd.merge(df_videoid_sentence, df_merge_start_end_time,how=\"inner\", on=\"video_id\")\n",
    "df_videoid_sentence.drop_duplicates(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list2 = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_videoid_sentence)))\n",
    "\n",
    "def videoid_word_ratio(index_num):\n",
    "    index = df_videoid_sentence.loc[index_num,\"index\"]\n",
    "    videoid = df_videoid_sentence.loc[index_num,\"video_id\"]\n",
    "    sentence = df_videoid_sentence.loc[index_num,\"sentence\"]\n",
    "    \n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/len(sent_word)+0.001)*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list2.append([index,videoid,sentence,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(videoid_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result = pd.DataFrame(list(result_list2), columns=[\"index\",\"video_id\",\"sentence\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_text_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_text_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_text_ratio_result.drop([\"index\"], axis=1, inplace=True)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result[\"different_word\"] = df_text_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result[\"intersect_word\"] = df_text_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_text_ratio_result[df_text_ratio_result[\"word_ratio\"] >= adjust_text_word_ratio]\n",
    "df_adjust_text_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_group_adjust_select:\n",
    "    df_adjust_text_ratio = df_exract_word_group(df_adjust_text_ratio, \"sentence\", \"video_id\", word_list, sent_len=word_group_adjust_select, sent_len_num=sentence_word_count_number)\n",
    "else:\n",
    "    df_adjust_text_ratio.loc[:,\"search_string\"] = df_adjust_text_ratio.loc[:,\"sentence\"].map(lambda x: exract_word_group(x, word_list))\n",
    "    df_adjust_text_ratio = df_adjust_text_ratio[~(df_adjust_text_ratio[\"search_string\"]==f\"sentencte_word_count_less_than_{sentence_word_count_number}\")]\n",
    "    \n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_adjust_text_ratio[[\"search_string\",\"video_id\",\"sentence\"]]\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_videoid_sentence[[\"video_id\",\"start_time\",\"end_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = pd.merge(df_adjust_text_ratio, df_videoid_sentence, how=\"inner\", on=\"video_id\")\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = word_group_time_loc(df_adjust_text_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_text_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_word_group_time_loc_in_text_result['video_id'].map(str)+\"&t=\"+df_word_group_time_loc_in_text_result['start_time'].map(str)+\"s\"\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = df_word_group_time_loc_in_text_result.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_word_group_time_loc_in_text_result.to_csv(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Adjust_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_group_adjust_select:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Adjust_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)\n",
    "else:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Max_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group*{file_ext}.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = path\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Youtube Videoid Text Adjust Result Analysis Multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"English\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 28  # 28  # native word end index\n",
    "\n",
    "# sentence ratio and time shift\n",
    "adjust_sent_word_ratio = 50\n",
    "adjust_text_word_ratio = 5\n",
    "shift = 0.3  # sentence time shift\n",
    "\n",
    "# func select\n",
    "word_group_adjust_select = True  # True, False; True for selecting word group according to sentence_word_count_number\n",
    "sentence_word_count_number = 5   # False for selecting word group according to all word group length\n",
    "\n",
    "# shared word frequency\n",
    "shared_word_frequency = True  # True, False\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = True  # True, False  # True for adding prefix suffix word\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = False  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_group_time_loc(df, search, start_sent, end_sent, sent, sent_video_id):\n",
    "    '''\n",
    "    word_group_time_loc(df_search_result, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "    '''\n",
    "    df.reset_index(drop=True, inplace=True)  # will test\n",
    "    word_time_loc_list = []\n",
    "    for i in range(len(df)):\n",
    "        word = df.loc[i,f\"{search}\"]\n",
    "        start_time = df.loc[i,f\"{start_sent}\"]\n",
    "        end_time = df.loc[i,f\"{end_sent}\"]\n",
    "        sentence = df.loc[i,f\"{sent}\"]\n",
    "        video_id = df.loc[i,f\"{sent_video_id}\"]\n",
    "        time_length = end_time-start_time\n",
    "        sentence_length = len(sentence)\n",
    "        time_length_ratio = time_length/sentence_length\n",
    "        loc_list = []\n",
    "        for j in re.finditer(fr\"(?:\\s|^){word}(?:\\s|$)\", sentence, re.IGNORECASE|re.UNICODE):\n",
    "            loc_list.append(j)\n",
    "            start = loc_list[0].start()\n",
    "            end = loc_list[0].end()\n",
    "            start_loc = start_time+(start*time_length_ratio)\n",
    "            end_loc = start_time+(end*time_length_ratio)\n",
    "        word_time_loc_list.append([word,start_loc,end_loc,sentence,video_id])\n",
    "    df_word_time_loc = pd.DataFrame(word_time_loc_list, columns=[f\"{search}\",f\"{start_sent}\",f\"{end_sent}\",f\"{sent}\",f\"{sent_video_id}\"])\n",
    "\n",
    "    return df_word_time_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select = df_word_all.iloc[word_start:word_end,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_word = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "    df_word = df_word.loc[:,[\"word\",\"frequency\"]]\n",
    "    df_word = pd.concat([df_word,df_word_select], axis=0)\n",
    "    df_word.drop_duplicates(inplace=True)    \n",
    "    df_word.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    df_word = df_word_select\n",
    "\n",
    "if native_word:\n",
    "    df_word\n",
    "else:\n",
    "    df_word = df_word.head(0)\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.lower().capitalize()}/{lang_folder.capitalize()}_{lang_pair.lower().capitalize()}_Shared_Vocabulary.xlsx\")\n",
    "#df_pair = df_pair.head()\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_prefix_suffix_select = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "    df_prefix_suffix_select = df_prefix_suffix_select.loc[:,[\"search_word\",\"word\"]]\n",
    "    df_prefix_suffix_select.rename(columns={\"search_word\":\"dict_entry_main\"}, inplace=True)\n",
    "    df_pair = pd.merge(df_pair,df_prefix_suffix_select, how=\"inner\", on=\"dict_entry_main\")\n",
    "    df_pair.drop_duplicates(inplace=True)\n",
    "    df_pair.reset_index(drop=True, inplace=True)\n",
    "    df_pair = df_pair.loc[:,[\"word\",f\"{lang_pair.lower()}_word\"]]\n",
    "    df_pair.rename(columns={\"word\":\"dict_entry_main\"}, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if etymology_word:\n",
    "    df_pair\n",
    "else:\n",
    "    df_pair = df_pair.head(0)\n",
    "    \n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = [\"sex\",\"seks\",\"seksi\",\"sexy\",\"sexe\",\"seksüel\",\"sexuell\",\"gey\",\"gay\",\"lezbiyen\",\"lesbienne\",\"eşcinsel\",\"mastürbasyon\",\"masturbation\",\"erotik\",\"érotique\", \\\n",
    "\"bikini\",\"penis\",\"vagina\",\"vajina\",\"fetish\",\"fetiş\",\"fetishy\",\"erotic\",\"erotik\",\"sexdom\",\"kondom\",\"condom\",\"dildo\",\"fetisj\",\"hétérosexuel\",\"féticher\",\"fétiche\",\"homosexuel\"\\\n",
    "\"ereksiyon\",\"erectie\",\"erection\",\"érection\",\"homoseksüel\",\"prezervatif\",\"préservatif\",\"ass\",\"fetisch\",\"fetiche\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_select = df_word[\"word\"].values.tolist()\n",
    "words = df_pair[\"dict_entry_main\"].values.tolist()\n",
    "word_select_set = set(word_select)\n",
    "disable_word_set = set(disable_list)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(word_select_set.union(words_set.difference(disable_word_set)))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Youtube/Result/{lang_folder.capitalize()}/Sentence Clean Merge/Clean_Youtube_Sentence_Merge_Result.csv\")\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = pd.to_timedelta(df_youtube_sentence['start_time']) # data type converted timedelta for second \n",
    "df_youtube_sentence['end_time'] = pd.to_timedelta(df_youtube_sentence['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_youtube_sentence['start_time'] = df_youtube_sentence['start_time'].apply(lambda x: x.total_seconds()) # convert seconds\n",
    "df_youtube_sentence['end_time'] = df_youtube_sentence['end_time'].apply(lambda x: x.total_seconds())\n",
    "df_youtube_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_youtube_sentence.groupby(\"video_id\")[\"sentence\"].apply(\" \".join).reset_index()\n",
    "df_videoid_sentence.reset_index(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_start_time = df_youtube_sentence.groupby(\"video_id\")[[\"start_time\"]].min()\n",
    "df_videoid_start_time.reset_index(inplace=True)\n",
    "df_videoid_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_end_time = df_youtube_sentence.groupby(\"video_id\")[[\"end_time\"]].max()\n",
    "df_videoid_end_time.reset_index(inplace=True)\n",
    "df_videoid_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_start_end_time = pd.merge(df_videoid_start_time, df_videoid_end_time, how=\"inner\", on=\"video_id\")\n",
    "df_merge_start_end_time.drop_duplicates(inplace=True)\n",
    "df_merge_start_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = pd.merge(df_videoid_sentence, df_merge_start_end_time,how=\"inner\", on=\"video_id\")\n",
    "df_videoid_sentence.drop_duplicates(inplace=True)\n",
    "df_videoid_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list3 = manager.list()\n",
    "word_set = set(word_list)\n",
    "index_num = list(range(len(df_videoid_sentence)))\n",
    "\n",
    "def videoid_word_ratio(index_num):\n",
    "    index = df_videoid_sentence.loc[index_num,\"index\"]\n",
    "    videoid = df_videoid_sentence.loc[index_num,\"video_id\"]\n",
    "    sentence = df_videoid_sentence.loc[index_num,\"sentence\"]\n",
    "    \n",
    "\n",
    "    sent_word = re.findall(r\"\\w+\", sentence, re.UNICODE)\n",
    "    sent_word_set = set(sent_word)\n",
    "    intersect_word = list(word_set.intersection(sent_word_set))\n",
    "    different_word = list(sent_word_set.difference(word_set))\n",
    "    word_ratio = round(((len(intersect_word)/len(sent_word)+0.001)*100),1)\n",
    "    different = \", \".join(different_word)\n",
    "    intersect = \", \".join(intersect_word)\n",
    "\n",
    "    result_list3.append([index,videoid,sentence,word_ratio,different,intersect])  \n",
    "    \n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(videoid_word_ratio, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result = pd.DataFrame(list(result_list3), columns=[\"index\",\"video_id\",\"sentence\",\"word_ratio\",\"different_word\",\"intersect_word\"])\n",
    "df_text_ratio_result.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_text_ratio_result.reset_index(drop=True, inplace=True)\n",
    "df_text_ratio_result.drop([\"index\"], axis=1, inplace=True)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_ratio_result[\"different_word\"] = df_text_ratio_result[\"different_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result[\"intersect_word\"] = df_text_ratio_result[\"intersect_word\"].apply(lambda x: np.nan if x == \"\" else x)\n",
    "df_text_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_text_ratio_result[df_text_ratio_result[\"word_ratio\"] >= adjust_text_word_ratio]\n",
    "df_adjust_text_ratio.reset_index(inplace=True, drop=True)\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Process, Manager, Pool, Queue\n",
    "manager = multiprocessing.Manager()\n",
    "result_list4 = manager.list()\n",
    "index_list = manager.list()\n",
    "all_index_list = manager.list()\n",
    "var_index_list = manager.list()\n",
    "text_all_list = manager.list()  \n",
    "df = df_adjust_text_ratio\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "index_num = list(range(len(df)))\n",
    "sent_len = word_group_adjust_select\n",
    "sent_len_num = sentence_word_count_number \n",
    "\n",
    "def df_exract_word_group(index_num):    \n",
    "    source_text = df.loc[index_num,\"sentence\"]\n",
    "    opt_var = df.loc[index_num,\"video_id\"]\n",
    "    words = re.findall(r\"\\w+\", source_text, re.UNICODE)\n",
    "    \n",
    "    index_list = []\n",
    "    for j in range(len(words)):\n",
    "        if words[j] in word_list:\n",
    "            index_list.append(j)\n",
    "        else:\n",
    "            pass\n",
    "    all_index_list = []\n",
    "    var_index_list = []\n",
    "    for k in range(len(index_list)):\n",
    "        try:\n",
    "            var1 = index_list[k] + 1  \n",
    "            var2 = index_list[k+1]\n",
    "        except:\n",
    "            var1 = index_list[k] + 1  \n",
    "            var2 = index_list[-1]\n",
    "        if var1 == var2:\n",
    "            var3 = index_list[k]\n",
    "            var_index_list.append(var3)\n",
    "        else:\n",
    "            var3 = index_list[k]\n",
    "            var_index_list.append(var3)\n",
    "            var4 = list(var_index_list)\n",
    "            all_index_list.append(var4)\n",
    "            var_index_list = []\n",
    "    text_all_list = []\n",
    "    for m in all_index_list:\n",
    "        text_list = [] \n",
    "        for n in m:\n",
    "            word = words[n]\n",
    "            text_list.append(word)\n",
    "            if sent_len:\n",
    "                if len(text_list) >= sent_len_num:\n",
    "                    text = \" \".join(text_list)\n",
    "                    text_all_list.append(text)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "               text = \" \".join(text_list)\n",
    "               text_all_list.append(text) \n",
    "        \n",
    "    for search_string in text_all_list:\n",
    "        #df_var[\"index\"] = i\n",
    "        #df_var[\"search_string\"] = search_string\n",
    "        #df_var[\"sentence\"] = source_text\n",
    "        #df_var[\"video_id\"] = opt_var\n",
    "        result_list4.append([index_num,search_string,source_text,opt_var])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(df_exract_word_group, index_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = pd.DataFrame(list(result_list4), columns=[\"index\",\"search_string\",\"sentence\",\"video_id\"])\n",
    "df_adjust_text_ratio.sort_values(by=\"index\", ascending=True, inplace=True)\n",
    "df_adjust_text_ratio.reset_index(drop=True, inplace=True)\n",
    "df_adjust_text_ratio.drop([\"index\"], axis=1, inplace=True)\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = df_adjust_text_ratio[[\"search_string\",\"video_id\",\"sentence\"]]\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videoid_sentence = df_videoid_sentence[[\"video_id\",\"start_time\",\"end_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjust_text_ratio = pd.merge(df_adjust_text_ratio, df_videoid_sentence, how=\"inner\", on=\"video_id\")\n",
    "df_adjust_text_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = word_group_time_loc(df_adjust_text_ratio, \"search_string\", \"start_time\", \"end_time\", \"sentence\", \"video_id\")\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: (x-shift))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: (x+shift))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"start_time\"] = df_word_group_time_loc_in_text_result[\"start_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result[\"end_time\"] = df_word_group_time_loc_in_text_result[\"end_time\"].apply(lambda x: round(x))\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result.sort_values(\"search_string\",key=lambda x:x.str.len(), inplace=True, ascending=False)\n",
    "df_word_group_time_loc_in_text_result.reset_index(drop=True, inplace=True)\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result[\"video_url\"] = \"https://www.youtube.com/watch?v=\"+df_word_group_time_loc_in_text_result['video_id'].map(str)+\"&t=\"+df_word_group_time_loc_in_text_result['start_time'].map(str)+\"s\"\n",
    "df_word_group_time_loc_in_text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_group_time_loc_in_text_result = df_word_group_time_loc_in_text_result.head(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if word_group_adjust_select:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_Adjust_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)\n",
    "else:\n",
    "    df_word_group_time_loc_in_text_result.to_excel(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group_All_In_Youtube_Sentence_Text_{word_end}_Word{file_ext}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file2 = glob.glob(f\"{lang_folder.capitalize()}_{lang_pair.capitalize()}_Word_Group*{file_ext}.xlsx\")\n",
    "output_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in output_file2:\n",
    "    source = l # source directory\n",
    "    destination = path\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in output_file2:\n",
    "    try:\n",
    "        os.remove(j)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
