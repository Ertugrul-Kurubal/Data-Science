{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Gram In N Gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "#import multiprocessing as mp\n",
    "from multiprocessing import Process, Manager, Pool, Queue\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {nprocs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language pair\n",
    "lang_folder = \"Turkish\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> target language for learner\n",
    "lang_pair = \"Intersect\"  # Arabic, English, French, German, Turkish, Spanish, Portuguese, Dutch, Italian ==> native language\n",
    "\n",
    "# adding native word to shared word\n",
    "word_start = 0  # 0  # native word start index\n",
    "word_end = 200  # 28  # native word end index\n",
    "\n",
    "# adding native word to shared word\n",
    "twogram_start = 0  # 0  # native word start index\n",
    "twogram_end = 200  # 28  # native word end index\n",
    "\n",
    "# sentence check\n",
    "twogram_sentence_check = False  # True, False\n",
    "threegram_sentence_check = False\n",
    "fourgram_sentence_check = False\n",
    "fivegram_sentence_check = False\n",
    "\n",
    "# n gram sample\n",
    "threegram_sample = 6\n",
    "fourgram_sample = 4\n",
    "fivegram_sample = 2\n",
    "\n",
    "# n gram select\n",
    "twogram_select_start = 0\n",
    "twogram_select_end = 10000\n",
    "\n",
    "threegram_select_start = 0\n",
    "threegram_select_end = 10000\n",
    "\n",
    "fourgram_select_start = 0\n",
    "fourgram_select_end = 10000\n",
    "\n",
    "fivegram_select_start = 0\n",
    "fivegram_select_end = 10000\n",
    "\n",
    "# prefix suffix file\n",
    "prefix_suffix = False  # True, False  # True for adding prefix suffix word\n",
    "native_word = True # True for adding native word\n",
    "etymology_word = False  # True for adding etymology word\n",
    "\n",
    "# adding output file extention\n",
    "if (not prefix_suffix) & etymology_word & native_word:\n",
    "    file_ext = \"1\"\n",
    "elif (not prefix_suffix) & etymology_word & (not native_word):\n",
    "    file_ext = \"2\"\n",
    "elif prefix_suffix & etymology_word & native_word:\n",
    "    file_ext = \"3\"\n",
    "elif prefix_suffix & etymology_word & (not native_word):\n",
    "    file_ext = \"4\"\n",
    "elif prefix_suffix & (not etymology_word) & native_word:\n",
    "    file_ext = \"5\"\n",
    "elif (not prefix_suffix) & (not etymology_word) & native_word:\n",
    "    file_ext = \"6\"\n",
    "else:\n",
    "    file_ext = \"7\"              \n",
    "# 1 => for native word and etymology word without prefix suffix. \n",
    "# 2 => for only etymology word without prefix suffix. \n",
    "# 3 => for native word and etymology word with prefix suffix. prefix_suffix, native_word and etymology_word must be True. \n",
    "# 4 => for only etymology word with prefix suffix.\n",
    "# 5 => for only native word with prefix suffix.\n",
    "# 6 => for only native word without prefix suffix.\n",
    "\n",
    "print(f\"{file_ext}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/\\\n",
    "Deployment2/Result/5-N Gram In N Gram Analysis/{lang_folder.capitalize()} {lang_pair.capitalize()}\"\n",
    "\n",
    "Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(df):\n",
    "    for i in df.columns:\n",
    "        new_name = i.strip()\n",
    "        df.rename(columns={f\"{i}\":f\"{new_name}\"}, inplace=True)\n",
    "        df[f\"{new_name}\"] = df[f\"{new_name}\"].apply(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_in_wordgroup_simple(df, source_column, target_column, word_sample_num):\n",
    "\n",
    "    '''word_in_wordgroup(df, \"word\", \"twogram\"):\n",
    "       df is dataframe, source_column and target_column are \n",
    "       dataframe column string name. source_column convert list\n",
    "       values that are in target column.\n",
    "    '''\n",
    "    \n",
    "    df_select = df[[f\"{target_column}\"]].dropna()\n",
    "    df_result = pd.DataFrame()\n",
    "    for i in df[f\"{source_column}\"].dropna():\n",
    "        try:\n",
    "            word_in_word_cluster = df_select[df_select[f\"{target_column}\"].str.contains(fr\"(?:\\s|^){i}(?:\\s|$)\", na=True)].head(word_sample_num)    \n",
    "        except:\n",
    "            pass        \n",
    "        word_in_word_cluster.insert(0,f\"{source_column}\",i)\n",
    "        df_result = pd.concat([df_result,word_in_word_cluster], axis=0)\n",
    "    df_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_all = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.lower().capitalize()}/Deployment/Data/Word/Word_Merge_Preprocess.xlsx\")\n",
    "df_word_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_select = df_word_all.iloc[word_start:word_end,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_word = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_{word_end}_Word_Prefix_Suffix_Custom_Result_Manuel.xlsx\")\n",
    "    df_word = df_word.loc[:,[\"word\",\"frequency\"]]\n",
    "    df_word = pd.concat([df_word,df_word_select], axis=0)\n",
    "    df_word.drop_duplicates(inplace=True)    \n",
    "    df_word.sort_values(by=\"frequency\", ascending=False, inplace=True)\n",
    "    df_word.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    df_word = df_word_select\n",
    "\n",
    "if native_word:\n",
    "    df_word\n",
    "else:\n",
    "    df_word = df_word.head(0)\n",
    "\n",
    "df_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.lower().capitalize()}/{lang_folder.capitalize()}_{lang_pair.lower().capitalize()}_Shared_Vocabulary.xlsx\")\n",
    "#df_pair = df_pair.head()\n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option\n",
    "if prefix_suffix:\n",
    "    df_prefix_suffix_select = pd.read_excel(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Result/{lang_folder.capitalize()}/{lang_folder.capitalize()} {lang_pair.capitalize()}/{lang_folder.capitalize()}_{lang_pair.capitalize()}_Shared_Word_Prefix_Suffix_Custom_Result.xlsx\")\n",
    "    df_prefix_suffix_select = df_prefix_suffix_select.loc[:,[\"search_word\",\"word\"]]\n",
    "    df_prefix_suffix_select.rename(columns={\"search_word\":\"dict_entry_main\"}, inplace=True)\n",
    "    df_pair = pd.merge(df_pair,df_prefix_suffix_select, how=\"inner\", on=\"dict_entry_main\")\n",
    "    df_pair.drop_duplicates(inplace=True)\n",
    "    df_pair.reset_index(drop=True, inplace=True)\n",
    "    df_pair = df_pair.loc[:,[\"word\",f\"{lang_pair.lower()}_word\"]]\n",
    "    df_pair.rename(columns={\"word\":\"dict_entry_main\"}, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if etymology_word:\n",
    "    df_pair\n",
    "else:\n",
    "    df_pair = df_pair.head(0)\n",
    "    \n",
    "df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_list = [\"sex\",\"seks\",\"seksi\",\"sexy\",\"sexe\",\"seksüel\",\"sexuell\",\"gey\",\"gay\",\"lezbiyen\",\"lesbienne\",\"eşcinsel\",\"mastürbasyon\",\"masturbation\",\"erotik\",\"érotique\", \\\n",
    "\"bikini\",\"penis\",\"vagina\",\"vajina\",\"fetish\",\"fetiş\",\"fetishy\",\"erotic\",\"erotik\",\"sexdom\",\"kondom\",\"condom\",\"dildo\",\"fetisj\",\"hétérosexuel\",\"féticher\",\"fétiche\",\"homosexuel\"\\\n",
    "\"ereksiyon\",\"erectie\",\"erection\",\"érection\",\"homoseksüel\",\"prezervatif\",\"préservatif\",\"ass\",\"fetisch\",\"fetiche\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_select = df_word[\"word\"].values.tolist()\n",
    "words = df_pair[\"dict_entry_main\"].values.tolist()\n",
    "word_select_set = set(word_select)\n",
    "disable_word_set = set(disable_list)\n",
    "words_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list((word_select_set.union(words_set)).difference(disable_word_set))\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twogram_sentence_check:\n",
    "    df_twogram_sent = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/N Gram And Sentence/Two_Gram_And_Sentence.csv\")  # ext. sentence and ngram\n",
    "    df_twogram_sent.rename(columns={\"two_gram\":\"twogram\"}, inplace=True)  # ext. Not: Two_Gram_And_Sentence.csv convert to Two_Gram_And_Sentence_All.csv\n",
    "    df_twogram = df_twogram_sent.loc[:,[\"twogram\",\"frequency\"]]\n",
    "    #df_twogram_select = df_twogram.iloc[twogram_start:twogram_end,]\n",
    "else:\n",
    "    df_twogram = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Twogram_Merge.csv\")  \n",
    "    df_twogram = df_twogram.loc[:,[\"twogram\",\"frequency\"]]\n",
    "    #df_twogram_select = df_twogram.iloc[twogram_start:twogram_end,]\n",
    "\n",
    "df_twogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list2  = df_twogram.iloc[:,0].values.tolist()\n",
    "\n",
    "resultlist2 = []\n",
    "manager = multiprocessing.Manager()\n",
    "resultlist2 = manager.list()\n",
    "\n",
    "def word_in_wordgroup(d_list2):\n",
    "    mergelist = []\n",
    "    try:\n",
    "        word = d_list2.split()\n",
    "    except:\n",
    "        word = []\n",
    "        #pass  disabled for non split value\n",
    "    var1 = range(len(word))\n",
    "    for j in var1:\n",
    "        if word[j] in word_list:\n",
    "            mergelist.append(word[j])\n",
    "            if len(mergelist) == len(word):\n",
    "                    resultlist2.append(d_list2)\n",
    "                        \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(word_in_wordgroup, d_list2) # string_word liste\n",
    "\n",
    "result_list2 = list(resultlist2)\n",
    "df_result2 = pd.DataFrame(result_list2, columns=[0])  # add columns parameter for empty result\n",
    "df_result2 = df_result2.rename(columns = {0: \"twogram\"})\n",
    "df_merge2 = pd.merge(df_result2, df_twogram, how=\"left\", on=\"twogram\")\n",
    "df_merge_result2 = df_merge2.sort_values(by=\"frequency\", ascending=False)\n",
    "df_merge_result2.drop_duplicates(inplace=True)\n",
    "df_merge_result2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# all word should be used at least once\n",
    "#df_twogram_select = df_merge_result2.iloc[twogram_start:twogram_end,]\n",
    "\n",
    "df_merge_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be test for max min using word number\n",
    "word_num_dict = {}\n",
    "for i in word_list:\n",
    "    word_num_dict[f\"{i}\"] = 0\n",
    "\n",
    "result_list_select = []\n",
    "var_list = []\n",
    "for i in range(len(df_merge_result2)):\n",
    "    temp_list = []\n",
    "    word_num_dict_temp = word_num_dict\n",
    "    twogram = df_merge_result2.loc[i,\"twogram\"]\n",
    "    frequency=df_merge_result2.loc[i,\"frequency\"]\n",
    "    words = word_tokenize(twogram)   \n",
    "    temp_list = [word for word in words]\n",
    "    temp_list = temp_list + var_list\n",
    "    # word count for max\n",
    "    dict_list_count = Counter(temp_list)\n",
    "    count_list = list(dict_list_count.values())\n",
    "    # word count for min\n",
    "    for item in dict_list_count.items():\n",
    "        word_num_dict_temp[item[0]] = item[1]\n",
    "    count_list2 = list(word_num_dict_temp.values())\n",
    "\n",
    "    if any([True if i>4 else False for i in count_list]) or not(any([True if j<2 else False for j in count_list2])):\n",
    "        var_list = var_list\n",
    "        word_num_dict = word_num_dict\n",
    "    else:\n",
    "        var_list = temp_list\n",
    "        word_num_dict = word_num_dict_temp \n",
    "        result_list_select.append([twogram,frequency])\n",
    "\n",
    "print(f\"Max condition: {any([True if i>5 else False for i in count_list])} \\nMin condition: {any([True if j<2 else False for j in count_list2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_select = pd.DataFrame(result_list_select, columns=[\"twogram\",\"frequency\"])\n",
    "df_twogram_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if threegram_sentence_check:\n",
    "    df_threegram_sent = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/N Gram And Sentence/Three_Gram_And_Sentence.csv\")  # ext. sentence and ngram\n",
    "    df_threegram_sent.rename(columns={\"three_gram\":\"threegram\"}, inplace=True)  # ext. Not: Two_Gram_And_Sentence.csv convert to Two_Gram_And_Sentence_All.csv\n",
    "    df_threegram = df_threegram_sent.loc[:,[\"threegram\",\"frequency\"]]\n",
    "else:\n",
    "    df_threegram = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Threegram_Merge.csv\")  \n",
    "    df_threegram = df_threegram.loc[:,[\"threegram\",\"frequency\"]]\n",
    "\n",
    "df_threegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list3  = df_threegram.iloc[:,0].values.tolist()\n",
    "\n",
    "resultlist3 = []\n",
    "manager = multiprocessing.Manager()\n",
    "resultlist3 = manager.list()\n",
    "\n",
    "def word_in_wordgroup(d_list3):\n",
    "    mergelist = []\n",
    "    try:\n",
    "        word = d_list3.split()\n",
    "    except:\n",
    "        word = []\n",
    "        #pass  disabled for non split value\n",
    "    var1 = range(len(word))\n",
    "    for j in var1:\n",
    "        if word[j] in word_list:\n",
    "            mergelist.append(word[j])\n",
    "            if len(mergelist) == len(word):\n",
    "                    resultlist3.append(d_list3)\n",
    "                        \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(word_in_wordgroup, d_list3) # string_word liste\n",
    "\n",
    "result_list3 = list(resultlist3)\n",
    "df_result3 = pd.DataFrame(result_list3, columns=[0])  # add columns parameter for empty result\n",
    "df_result3 = df_result3.rename(columns = {0: \"threegram\"})\n",
    "df_merge3 = pd.merge(df_result3, df_threegram, how=\"left\", on=\"threegram\")\n",
    "df_merge_result3 = df_merge3.sort_values(by=\"frequency\", ascending=False)\n",
    "df_merge_result3.drop_duplicates(inplace=True)\n",
    "df_merge_result3.reset_index(drop=True, inplace=True)\n",
    "df_threegram_select = df_merge_result3\n",
    "df_threegram_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fourgram_sentence_check:\n",
    "    df_fourgram_sent = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/N Gram And Sentence/Four_Gram_And_Sentence.csv\")  # ext. sentence and ngram\n",
    "    df_fourgram_sent.rename(columns={\"four_gram\":\"fourgram\"}, inplace=True)  # ext. Not: Two_Gram_And_Sentence.csv convert to Two_Gram_And_Sentence_All.csv\n",
    "    df_fourgram = df_fourgram_sent.loc[:,[\"fourgram\",\"frequency\"]]\n",
    "else:\n",
    "    df_fourgram = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fourgram_Merge.csv\")  \n",
    "    df_fourgram = df_fourgram.loc[:,[\"fourgram\",\"frequency\"]]\n",
    "\n",
    "df_fourgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list4  = df_fourgram.iloc[:,0].values.tolist()\n",
    "\n",
    "resultlist4 = []\n",
    "manager = multiprocessing.Manager()\n",
    "resultlist4 = manager.list()\n",
    "\n",
    "def word_in_wordgroup(d_list4):\n",
    "    mergelist = []\n",
    "    try:\n",
    "        word = d_list4.split()\n",
    "    except:\n",
    "        word = []\n",
    "        #pass  disabled for non split value\n",
    "    var1 = range(len(word))\n",
    "    for j in var1:\n",
    "        if word[j] in word_list:\n",
    "            mergelist.append(word[j])\n",
    "            if len(mergelist) == len(word):\n",
    "                    resultlist4.append(d_list4)\n",
    "                        \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(word_in_wordgroup, d_list4) # string_word liste\n",
    "\n",
    "result_list4 = list(resultlist4)\n",
    "df_result4 = pd.DataFrame(result_list4, columns=[0])  # add columns parameter for empty result\n",
    "df_result4 = df_result4.rename(columns = {0: \"fourgram\"})\n",
    "df_merge4 = pd.merge(df_result4, df_fourgram, how=\"left\", on=\"fourgram\")\n",
    "df_merge_result4 = df_merge4.sort_values(by=\"frequency\", ascending=False)\n",
    "df_merge_result4.drop_duplicates(inplace=True)\n",
    "df_merge_result4.reset_index(drop=True, inplace=True)\n",
    "df_fourgram_select = df_merge_result4\n",
    "df_fourgram_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fivegram_sentence_check:\n",
    "    df_fivegram_sent = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/N Gram And Sentence/Five_Gram_And_Sentence.csv\")  # ext. sentence and ngram\n",
    "    df_fivegram_sent.rename(columns={\"five_gram\":\"fivegram\"}, inplace=True)  # ext. Not: Two_Gram_And_Sentence.csv convert to Two_Gram_And_Sentence_All.csv\n",
    "    df_fivegram = df_fivegram_sent.loc[:,[\"fivegram\",\"frequency\"]]\n",
    "else:\n",
    "    df_fivegram = pd.read_csv(f\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/{lang_folder.capitalize()}/Result/N Gram/Merge/Fivegram_Merge.csv\")  \n",
    "    df_fivegram = df_fivegram.loc[:,[\"fivegram\",\"frequency\"]]\n",
    "\n",
    "df_fivegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_list5  = df_fivegram.iloc[:,0].values.tolist()\n",
    "\n",
    "resultlist5 = []\n",
    "manager = multiprocessing.Manager()\n",
    "resultlist5 = manager.list()\n",
    "\n",
    "def word_in_wordgroup(d_list5):\n",
    "    mergelist = []\n",
    "    try:\n",
    "        word = d_list5.split()\n",
    "    except:\n",
    "        word = []\n",
    "        #pass  disabled for non split value\n",
    "    var1 = range(len(word))\n",
    "    for j in var1:\n",
    "        if word[j] in word_list:\n",
    "            mergelist.append(word[j])\n",
    "            if len(mergelist) == len(word):\n",
    "                    resultlist5.append(d_list5)\n",
    "                        \n",
    "if __name__ == '__main__':\n",
    "    # with Pool(16) as p:\n",
    "    with Pool(nprocs) as p: # Pool number CPU sayısına eşit olursa tüm CPU lar çalışır\n",
    "        p.map(word_in_wordgroup, d_list5) # string_word liste\n",
    "\n",
    "result_list5 = list(resultlist5)\n",
    "df_result5 = pd.DataFrame(result_list5, columns=[0])  # add columns parameter for empty result\n",
    "df_result5 = df_result5.rename(columns = {0: \"fivegram\"})\n",
    "df_merge5 = pd.merge(df_result5, df_fivegram, how=\"left\", on=\"fivegram\")\n",
    "df_merge_result5 = df_merge5.sort_values(by=\"frequency\", ascending=False)\n",
    "df_merge_result5.drop_duplicates(inplace=True)\n",
    "df_merge_result5.reset_index(drop=True, inplace=True)\n",
    "df_fivegram_select = df_merge_result5\n",
    "df_fivegram_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_concat = pd.concat([df_twogram_select, df_threegram_select, df_fourgram_select, df_fivegram_select], axis=1)\n",
    "df_ngram_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_three = word_in_wordgroup_simple(df_ngram_concat, \"twogram\",\"threegram\",threegram_sample)\n",
    "df_two_in_four = word_in_wordgroup_simple(df_ngram_concat, \"twogram\",\"fourgram\",fourgram_sample)\n",
    "df_two_in_five = word_in_wordgroup_simple(df_ngram_concat, \"twogram\",\"fivegram\",fivegram_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twogram_order_join_threegram = df_two_in_three.groupby([\"twogram\"])[\"threegram\"].apply(\", \".join).reset_index()\n",
    "df_twogram_order_join_fourgram = df_two_in_four.groupby([\"twogram\"])[\"fourgram\"].apply(\", \".join).reset_index()\n",
    "df_twogram_order_join_fivegram = df_two_in_five.groupby([\"twogram\"])[\"fivegram\"].apply(\", \".join).reset_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_twogram_order_join_threegram, df_twogram_order_join_fourgram, df_twogram_order_join_fivegram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_sample_join_merge = reduce(lambda  left,right: pd.merge(left,right, on=['twogram'], how='outer'), dfs)\n",
    "df_ngram_sample_join_merge.drop_duplicates(inplace=True)\n",
    "df_ngram_sample_join_merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_sample_join_merge = pd.merge(df_ngram_sample_join_merge, df_twogram_select, how=\"left\", on=\"twogram\")\n",
    "df_ngram_sample_join_merge.drop_duplicates(inplace=True)\n",
    "df_ngram_sample_join_merge.rename(columns={\"frequency\":\"two_freq\"}, inplace=True)\n",
    "df_ngram_sample_join_merge.sort_values(by=\"two_freq\", ascending=False, inplace=True)\n",
    "df_ngram_sample_join_merge.reset_index(drop=True, inplace=True)\n",
    "df_ngram_sample_join_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_sample_join_merge.to_excel(f\"{twogram_end}_Twogram_In_{threegram_sample}_Threegram_{fourgram_sample}_\\\n",
    "Fourgram_{fivegram_sample}_Fivegram_Sample_With_{word_end}_Word_Join_Result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Frequency For Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_three_freq = pd.merge(df_two_in_three, df_threegram, how=\"left\", on=\"threegram\")\n",
    "df_two_in_three_freq.drop_duplicates(inplace=True)\n",
    "df_two_in_three_freq.rename(columns={\"frequency\":\"three_freq\"}, inplace=True)\n",
    "df_two_in_three_freq.drop([\"twogram\"], axis=1, inplace=True)\n",
    "df_two_in_three_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_four_freq = pd.merge(df_two_in_four, df_fourgram, how=\"left\", on=\"fourgram\")\n",
    "df_two_in_four_freq.drop_duplicates(inplace=True)\n",
    "df_two_in_four_freq.rename(columns={\"frequency\":\"four_freq\"}, inplace=True)\n",
    "df_two_in_four_freq.drop([\"twogram\"], axis=1, inplace=True)\n",
    "df_two_in_four_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_in_five_freq = pd.merge(df_two_in_five, df_fivegram, how=\"left\", on=\"fivegram\")\n",
    "df_two_in_five_freq.drop_duplicates(inplace=True)\n",
    "df_two_in_five_freq.rename(columns={\"frequency\":\"five_freq\"}, inplace=True)\n",
    "df_two_in_five_freq.drop([\"twogram\"], axis=1, inplace=True)\n",
    "df_two_in_five_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_sample_concat = pd.concat([df_twogram_select,df_two_in_three_freq, df_two_in_four_freq, df_two_in_five_freq], axis=1)\n",
    "df_ngram_sample_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_sample_concat.to_excel(f\"{twogram_end}_Twogram_In_{threegram_sample}_Threegram_{fourgram_sample}_\\\n",
    "Fourgram_{fivegram_sample}_Fivegram_Sample_With_{word_end}_Word_Frequency_Result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Result And Select Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram_threegram_unique = df_ngram_sample_concat[[\"threegram\",\"three_freq\"]].drop_duplicates()\n",
    "df_ngram_fourgram_unique = df_ngram_sample_concat[[\"fourgram\",\"four_freq\"]].drop_duplicates()\n",
    "df_ngram_fivegram_unique = df_ngram_sample_concat[[\"fivegram\",\"five_freq\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twogram_result_freq = df_twogram_select[\"frequency\"].sum()\n",
    "threegram_result_freq = df_ngram_threegram_unique[\"three_freq\"].sum()\n",
    "fourgram_result_freq = df_ngram_fourgram_unique[\"four_freq\"].sum()\n",
    "fivegram_result_freq = df_ngram_fivegram_unique[\"five_freq\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twogram_select_freq = df_twogram.iloc[twogram_select_start:twogram_select_end,][\"frequency\"].sum()\n",
    "threegram_select_freq = df_threegram.iloc[threegram_select_start:threegram_select_end,][\"frequency\"].sum()\n",
    "fourgram_select_freq = df_fourgram.iloc[fourgram_select_start:fourgram_select_end,][\"frequency\"].sum()\n",
    "fivegram_select_freq = df_fivegram.iloc[fivegram_select_start:fivegram_select_end,][\"frequency\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(twogram_result_freq/twogram_select_freq)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(threegram_result_freq/threegram_select_freq)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fourgram_result_freq/fourgram_select_freq)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fivegram_result_freq/fivegram_select_freq)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Move And Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = glob.glob(f\"{twogram_end}_Twogram_In_{threegram_sample}_Threegram_{fourgram_sample}_\\\n",
    "Fourgram_{fivegram_sample}_Fivegram_Sample_*_Result.xlsx\")\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in output_file:\n",
    "    source = k # source directory\n",
    "    destination = path\n",
    "    shutil.copy2(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_file:\n",
    "    try:\n",
    "        os.remove(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
