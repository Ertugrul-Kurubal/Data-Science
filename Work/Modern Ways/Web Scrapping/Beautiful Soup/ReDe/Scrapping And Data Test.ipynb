{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nisanyan-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall nisanyan-cli -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from json2html import *\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "from urllib.parse import quote\n",
    "from urllib.request import Request\n",
    "from urllib import parse\n",
    "import orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# import argparse\n",
    "\n",
    "#import requests\n",
    "#from requests.adapters import HTTPAdapter\n",
    "#from urllib3.util import Retry\n",
    "#from urllib.parse import quote\n",
    "#\n",
    "#from bs4 import BeautifulSoup\n",
    "#import orjson\n",
    "\n",
    "# data dictionary\n",
    "data_dict = {}\n",
    "\n",
    "# Given words\n",
    "first_word = \"ab\"\n",
    "last_word = \"zürriyet\"\n",
    "\n",
    "# Filenames\n",
    "f_output = \"output.json\"\n",
    "f_wordlist = \"wordlist.txt\"\n",
    "\n",
    "# Request\n",
    "def req(word):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    url = \"https://www.nisanyansozluk.com/?k=\" + quote(word) + \"&lnk=1&view=annotated\"\n",
    "    session.mount(url, adapter)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "        \"Accept-Encoding\": \"*\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "        \"dnt\": \"1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "    }\n",
    "    r = session.get(url, headers=headers)\n",
    "    # print(session.cookies.get_dict())\n",
    "    return r.content\n",
    "\n",
    "\n",
    "# Sometimes distortions occur when going sequentially in the table. e.g.: add -> \"adem\"\n",
    "def back_forward():\n",
    "    try:\n",
    "        soup = BeautifulSoup(req(list(data_dict)[-2]), \"html5lib\")\n",
    "        cell = soup.find(\"tr\", {\"class\": \"yaz hghlght\"})\n",
    "        nextt_word = cell.nextSibling.nextSibling.nextSibling.nextSibling.td[\"title\"]\n",
    "\n",
    "        print(\n",
    "            \"back_forward: \"\n",
    "            + list(data_dict)[-2]\n",
    "            + \" => \\033[95m\"\n",
    "            + list(data_dict)[-1]\n",
    "            + \"\\033[0m => \"\n",
    "            + nextt_word\n",
    "        )\n",
    "    except AttributeError:\n",
    "        soup = BeautifulSoup(req(list(data_dict)[-3]), \"html5lib\")\n",
    "        cell = soup.find(\"tr\", {\"class\": \"yaz hghlght\"})\n",
    "        nextt_word = cell.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling.nextSibling.td[\n",
    "            \"title\"\n",
    "        ]\n",
    "        print(\n",
    "            \"back_back_forward: \"\n",
    "            + list(data_dict)[-3]\n",
    "            + \" => \\033[95m\"\n",
    "            + list(data_dict)[-2]\n",
    "            + \", \"\n",
    "            + list(data_dict)[-1]\n",
    "            + \"\\033[0m => \"\n",
    "            + nextt_word\n",
    "        )\n",
    "    gg(nextt_word)\n",
    "\n",
    "\n",
    "# final message\n",
    "def final_msg(msg):\n",
    "    p_green = \"\\033[92m\"\n",
    "    p_blue = \"\\033[94m\"\n",
    "    p_endc = \"\\033[0m\"\n",
    "\n",
    "    print(\"-------------------------\\n\")\n",
    "    print(p_blue + msg + p_endc + \"\\n\")\n",
    "    print(\n",
    "        p_blue\n",
    "        + \"İlk kelime: \"\n",
    "        + p_green\n",
    "        + list(data_dict)[0]\n",
    "        + p_blue\n",
    "        + \", Son kelime: \"\n",
    "        + p_green\n",
    "        + list(data_dict)[-1]\n",
    "    )\n",
    "    print(p_blue + \"Toplam kelime sayısı: \" + p_green + str(len(data_dict)) + p_endc)\n",
    "    print(p_blue + \"Çıktı dosyası: \" + p_green + f_output + p_endc)\n",
    "    print(p_blue + \"Kelime listesi dosyası: \" + p_green + f_wordlist + p_endc)\n",
    "    print(\"\\a\")\n",
    "\n",
    "\n",
    "def export():\n",
    "    # export data to .json file\n",
    "    with open(f_output, \"wb\") as f:\n",
    "        f.write(orjson.dumps(data_dict))\n",
    "\n",
    "    # export wordlist to .txt file\n",
    "    with open(f_wordlist, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(list(data_dict)))\n",
    "\n",
    "\n",
    "# scrape the cell\n",
    "def scrape_cell(cell):\n",
    "    cell = cell\n",
    "    kelime = cell.td[\"title\"]\n",
    "    tarih = cell.find(\"div\", {\"class\": \"maddetarih\"}).text\n",
    "    eskoken = cell.find_all(\"div\", {\"class\": \"eskoken\"})\n",
    "    baslik = (\n",
    "        tarihce\n",
    "    ) = koken = daha_fazla = ek_aciklama = benzer_sozcukler = maddeye_gonderenler = \"\"\n",
    "    baslik = cell.td.a.text\n",
    "\n",
    "    for i in eskoken:\n",
    "        if i.div != None:\n",
    "            title = i.div.text\n",
    "            if \"Tarihçe\" in title:\n",
    "                tarihce = str(i.p)\n",
    "            elif \"Köken\" in title:\n",
    "                koken = str(i.p)\n",
    "            elif \"Ek açıklama\" in title:\n",
    "                ek_aciklama = str(i.p)\n",
    "            elif \"Benzer sözcükler\" in title:\n",
    "                benzer_sozcukler = list(i.p.text.split(\", \"))\n",
    "            elif \"Bu maddeye gönderenler\" in title:\n",
    "                maddeye_gonderenler = list(i.p.text.strip().split(\", \"))\n",
    "        elif \"Daha fazla bilgi\" in i.p.text:\n",
    "            k = []\n",
    "            a = i.p.find_all(\"a\")\n",
    "            for i in a:\n",
    "                k.append(i.text)\n",
    "            daha_fazla = k\n",
    "\n",
    "    data = {\n",
    "        \"baslik\": baslik,\n",
    "        \"tarihce\": tarihce,\n",
    "        \"koken\": koken,\n",
    "        \"daha_fazla\": daha_fazla,\n",
    "        \"ek_aciklama\": ek_aciklama,\n",
    "        \"benzer_sozcukler\": benzer_sozcukler,\n",
    "        \"maddeye_gonderenler\": maddeye_gonderenler,\n",
    "        \"tarih\": tarih,\n",
    "    }\n",
    "    data_dict[kelime] = data\n",
    "    print(kelime)\n",
    "\n",
    "\n",
    "# Main function gg\n",
    "def gg(first, last):\n",
    "    word = first\n",
    "    lastw = last\n",
    "    print(word + \" ➔ \" + lastw + \"\\n\")\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        content = req(word)\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        cell = soup.find(\"tr\", {\"class\": \"yaz hghlght\"})\n",
    "        all_cells = soup.find_all(\"tr\", {\"class\": \"yaz hghlght\"})\n",
    "\n",
    "        # Wrong word\n",
    "        if len(all_cells) == 0:\n",
    "            print(\"Girilen kelime bulunamadı!\")\n",
    "            break\n",
    "        # Double word e.g.: ram,RAM\n",
    "        elif len(all_cells) > 1:\n",
    "            for i in all_cells:\n",
    "                scrape_cell(i)\n",
    "        else:\n",
    "            scrape_cell(cell)\n",
    "\n",
    "        kelime = cell.td[\"title\"]\n",
    "        if kelime == lastw:\n",
    "            done = True\n",
    "        else:\n",
    "\n",
    "            if (\n",
    "                cell.nextSibling.nextSibling != None\n",
    "                and cell.nextSibling.nextSibling.nextSibling.nextSibling != None\n",
    "            ):\n",
    "                if (\n",
    "                    cell.nextSibling.nextSibling.find(\"div\", {\"class\": \"etymtxt\"}).text\n",
    "                    == \"\"\n",
    "                ):\n",
    "                    word = cell.nextSibling.nextSibling.nextSibling.nextSibling.td[\n",
    "                        \"title\"\n",
    "                    ]\n",
    "                    continue\n",
    "                word = cell.nextSibling.nextSibling.td[\"title\"]\n",
    "            else:\n",
    "                back_forward()\n",
    "    else:\n",
    "        export()\n",
    "        final_msg(\"Başarıyla tamamlandı...\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "def main(first=first_word, last=last_word):\n",
    "    try:\n",
    "        gg(first, last)\n",
    "    except KeyboardInterrupt:\n",
    "        export()\n",
    "        final_msg(\"Liste tamamlanamadı!\\nKeyboardInterrupt\")\n",
    "    except Exception as e:\n",
    "        export()\n",
    "        final_msg(\"Liste tamamlanamadı!\\nBeklenmedik bir durum oluştu: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab ➔ zürriyet\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girilen kelime bulunamadı!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "from urllib.parse import quote\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def req(word):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    url = \"https://www.nisanyansozluk.com/?k=\" + quote(word) + \"&lnk=1&view=annotated\"\n",
    "    session.mount(url, adapter)\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\",\n",
    "        \"Accept-Encoding\": \"*\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "        \"cache-control\": \"max-age=0\",\n",
    "        \"content-type\": \"application/x-www-form-urlencoded\",\n",
    "        \"dnt\": \"1\",\n",
    "        \"upgrade-insecure-requests\": \"1\",\n",
    "    }\n",
    "    r = session.get(url, headers=headers)\n",
    "    # print(session.cookies.get_dict())\n",
    "    return r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req(\"elma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Language Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Web Scrapping/Data/All/etymology_cross_check2.csv\")\n",
    "#df_clean = df_clean.head(150000)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = df_clean[df_clean.lang == \"Arabic\"]\n",
    "#df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test[\"term\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_arabic = df_clean[df_clean.related_lang == \"Arabic\"]\n",
    "df_related_arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_lang = df_related_arabic[df_related_arabic.lang == \"Turkish\"]\n",
    "df_select_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var1 = pd.DataFrame(df_select_lang[\"term\"].unique(), columns=[\"term\"])\n",
    "df_var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var1.to_excel(\"Turkish_Term.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var2 = pd.DataFrame(df_select_lang[\"related_term\"].unique(), columns=[\"related_term\"])\n",
    "df_var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var2.to_excel(\"Arabic_Related_Term.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var1_translate = pd.read_excel(\"Turkish Arabic Language Pair Translate.xlsx\")\n",
    "df_var1_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var2.rename(columns={\"related_term\":\"arabic_word\"}, inplace=True)\n",
    "df_var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_var2,df_var1_translate, how=\"inner\",on=\"arabic_word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var2_translate = pd.read_excel(\"Arabic Turkish Language Pair Translate.xlsx\")\n",
    "df_var2_translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var1.rename(columns={\"term\":\"turkish_word\"}, inplace=True)\n",
    "df_var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_var1,df_var2_translate, how=\"inner\",on=\"turkish_word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "651d507d70892fab0fc6529d935cd476f6e2eb1791525b76da6cc8da34bc0503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
