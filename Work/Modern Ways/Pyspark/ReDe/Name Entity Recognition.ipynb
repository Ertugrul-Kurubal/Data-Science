{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Pyspark/ReDe/tr.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    text = file.read(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "chunks = nltk.ne_chunk(pos_tags, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        #print(chunk)\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_df.to_excel(\"NLTK_NE.xlsx\", sheet_name=\"NE\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "spacy.cli.download(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"xx_ent_wiki_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]) # \"xx_ent_wiki_sm\" multi language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Prime Minister Narendra Modi on Tuesday announced the 266 billion dollars package for the India to fight against the coronavirus pandemic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in NER.ents:\n",
    "    print(w.text,w.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  text_list = []\n",
    "  with open(\"/media/kurubal/SSD/Data Scientist/Work/Modern Ways/Project/Pyspark/ReDe/xaa.tr\", \"r\", encoding=\"utf-8\", buffering=1000000) as file:\n",
    "    for i in file:\n",
    "      text_list.append(i)\n",
    "  print(text_list)\n",
    "except:\n",
    "  print(\"There is not such a file  or path is incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Loop For List Sentence\n",
    "text_ent = []\n",
    "label_ent = []\n",
    "for i in text_list:\n",
    "    NER = nlp(i)\n",
    "    for w in NER.ents:\n",
    "        text_ent.append(w.text)\n",
    "        label_ent.append(w.label_)\n",
    "df_ne_spc = pd.DataFrame()\n",
    "df_ne_spc[\"text\"] = text_ent\n",
    "df_ne_spc[\"label\"] = label_ent\n",
    "df_ne_spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ne_spc.to_excel(\"Spacy_NE_50MB.xlsx\", sheet_name=\"NE\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
